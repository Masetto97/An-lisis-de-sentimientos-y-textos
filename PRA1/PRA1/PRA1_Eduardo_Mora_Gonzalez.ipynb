{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1894f082",
   "metadata": {
    "id": "1894f082"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.877 Análisis de sentimentos y textos</p>\n",
    "<p style=\"margin: 0; text-align:right;\">Máster Universitario en Ciencia de Datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicaciones</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# PAC 1: Procesamiento y análisis de información textual\n",
    "\n",
    "En esta práctica revisaremos y aplicaremos los conocimientos aprendidos en los módulos del 1 al 2. Concretamente trataremos 3 temas.\n",
    "\n",
    "<ul>\n",
    "<li>1. Obtención de datos a partir de información textual\n",
    "<li>2. Detección de tópicos\n",
    "<li>3. Clasificación de textos\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bf012",
   "metadata": {
    "id": "bb0bf012"
   },
   "source": [
    "El propósito de la práctica es descubrir rasgos característicos de las opiniones sobre restaurantes con las herramientas explicadas en los módulos del 1 al 2. Además, veremos si es posible clasificar automáticamente una opinión como positiva o negativa con métodos de machine learning. Utilizaremos el dataset <i>restaurants_reviews.csv</i>, extraído de una plataforma de expresión de opiniones. Este dataset contiene opiniones sobre restaurantes en inglés. El dataset se organiza en 10 columnas:\n",
    "\n",
    "<b>business_id</b>: identificador del restaurant<br>\n",
    "<b>date</b>: fecha de publicación de la opinión<br>\n",
    "<b>review_id</b>: identificador de la opinión<br>\n",
    "<b>stars</b>: calificación o valoración del restaurant en estrellas<br>\n",
    "<b>text</b>: texto de la opinión<br>\n",
    "<b>type</b>: tipo de texto<br>\n",
    "<b>user_id</b>: identificador de usuario<br>\n",
    "<b>cool, useful </b> y <b>funny</b>: Número de valoraciones que han realizado los usuarios de la plataforma para estos tres criterios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1333f6b",
   "metadata": {
    "id": "d1333f6b"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d20de",
   "metadata": {
    "id": "3d8d20de"
   },
   "source": [
    "# Preparación del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf74bac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "id": "bbf74bac",
    "outputId": "c7cb1550-c559-48ce-d776-642870aa2716"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-yxfBYGB6SEqszmxJxd97A</td>\n",
       "      <td>2007-12-13</td>\n",
       "      <td>m2CKSsepBCoRYWxiRUsxAg</td>\n",
       "      <td>4</td>\n",
       "      <td>Quiessence is, simply put, beautiful.  Full wi...</td>\n",
       "      <td>review</td>\n",
       "      <td>sqYN3lNgvPbPCTRsMFu27g</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zp713qNhx8d9KCJJnrw1xA</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>riFQ3vxNpP4rWLk_CSri2A</td>\n",
       "      <td>5</td>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "      <td>review</td>\n",
       "      <td>wFweIWhv2fREZV_dYkz_1g</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  -yxfBYGB6SEqszmxJxd97A  2007-12-13  m2CKSsepBCoRYWxiRUsxAg      4   \n",
       "4  zp713qNhx8d9KCJJnrw1xA  2010-02-12  riFQ3vxNpP4rWLk_CSri2A      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Quiessence is, simply put, beautiful.  Full wi...  review   \n",
       "4  Drop what you're doing and drive here. After I...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  sqYN3lNgvPbPCTRsMFu27g     4       3      1  \n",
       "4  wFweIWhv2fREZV_dYkz_1g     7       7      4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Obrir el fitxer de comentaris:\n",
    "df = pd.read_csv('restaurants_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c49915a",
   "metadata": {
    "id": "0c49915a"
   },
   "source": [
    "Para realizar la práctica, sólo necesitaremos los textos y las valoraciones en estrellas. Por tanto, eliminamos las columnas innecesarias y nos quedaremos solo con las columnas <b>stars</b> y <b>text</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656acd34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "656acd34",
    "outputId": "9d423ef9-2f99-4e01-e076-1f14eaaedf8f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Quiessence is, simply put, beautiful.  Full wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6906</th>\n",
       "      <td>3</td>\n",
       "      <td>First visit...Had lunch here today - used my G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6907</th>\n",
       "      <td>4</td>\n",
       "      <td>Should be called house of deliciousness!\\r\\n\\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6908</th>\n",
       "      <td>4</td>\n",
       "      <td>I recently visited Olive and Ivy for business ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6909</th>\n",
       "      <td>2</td>\n",
       "      <td>My nephew just moved to Scottsdale recently so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6910</th>\n",
       "      <td>5</td>\n",
       "      <td>4-5 locations.. all 4.5 star average.. I think...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6911 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      stars                                               text\n",
       "0         5  My wife took me here on my birthday for breakf...\n",
       "1         5  I have no idea why some people give bad review...\n",
       "2         4  love the gyro plate. Rice is so good and I als...\n",
       "3         4  Quiessence is, simply put, beautiful.  Full wi...\n",
       "4         5  Drop what you're doing and drive here. After I...\n",
       "...     ...                                                ...\n",
       "6906      3  First visit...Had lunch here today - used my G...\n",
       "6907      4  Should be called house of deliciousness!\\r\\n\\r...\n",
       "6908      4  I recently visited Olive and Ivy for business ...\n",
       "6909      2  My nephew just moved to Scottsdale recently so...\n",
       "6910      5  4-5 locations.. all 4.5 star average.. I think...\n",
       "\n",
       "[6911 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['business_id', 'review_id', 'user_id', 'date', 'type', 'funny', 'cool', 'useful'], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8aceb9",
   "metadata": {
    "id": "ef8aceb9"
   },
   "source": [
    "Trabajaremos con las opiniones que tienen las calificaciones más altas y las más bajas. Las opiniones cuya calificación sea mayor que 3 serán etiquetadas con '1', mientras que etiquetaremos con '0' las opiniones que tengan una calificación menor a 3. Las etiquetas se asignan a la columna <b>sentiment</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16d52608",
   "metadata": {
    "id": "16d52608"
   },
   "outputs": [],
   "source": [
    "df = df[(df['stars'] > 3) | (df['stars'] < 3)]\n",
    "\n",
    "df['sentiment'] = df['stars'].apply(lambda x : 1 if x > 3 else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6302df",
   "metadata": {
    "id": "ed6302df"
   },
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa6736",
   "metadata": {
    "id": "e7fa6736"
   },
   "source": [
    "Antes de trabajar con los textos de las opiniones, hay que limpiarlos de caracteres como los saltos de línea (e.g: *Should be called house of deliciousness!\\r\\n\\r*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2d9fd79",
   "metadata": {
    "id": "e2d9fd79"
   },
   "outputs": [],
   "source": [
    "class CarriageReturnReplacer(object):\n",
    "    \"\"\" Replaces \\r\\n expressions in a text.\n",
    "    >>> replacer = CarriageReturnReplacer()\n",
    "    >>> replacer.replace(\"\\r\\n\\r\\nAnyway, I can\\'t wait to go back!\")\n",
    "    'Anyway, I can\\'t wait to go back!'\n",
    "    \"\"\"\n",
    "    \n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        s = s.replace('\\r\\n', ' ')\n",
    "        s = s.replace('\\n\\n', ' ') \n",
    "        s = s.replace('\\n', ' ')\n",
    "        s = s.replace('\\r', ' ') \n",
    "        return s\n",
    "\n",
    "newline_replacer = CarriageReturnReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b890c968",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "b890c968",
    "outputId": "9e7cfcbf-eef2-4f99-aa60-eb6a9c40a10d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Quiessence is, simply put, beautiful.  Full wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6905</th>\n",
       "      <td>4</td>\n",
       "      <td>Judging by some of the reviews, maybe I went o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6907</th>\n",
       "      <td>4</td>\n",
       "      <td>Should be called house of deliciousness!  I co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6908</th>\n",
       "      <td>4</td>\n",
       "      <td>I recently visited Olive and Ivy for business ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6909</th>\n",
       "      <td>2</td>\n",
       "      <td>My nephew just moved to Scottsdale recently so...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6910</th>\n",
       "      <td>5</td>\n",
       "      <td>4-5 locations.. all 4.5 star average.. I think...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5851 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      stars                                               text  sentiment\n",
       "0         5  My wife took me here on my birthday for breakf...          1\n",
       "1         5  I have no idea why some people give bad review...          1\n",
       "2         4  love the gyro plate. Rice is so good and I als...          1\n",
       "3         4  Quiessence is, simply put, beautiful.  Full wi...          1\n",
       "4         5  Drop what you're doing and drive here. After I...          1\n",
       "...     ...                                                ...        ...\n",
       "6905      4  Judging by some of the reviews, maybe I went o...          1\n",
       "6907      4  Should be called house of deliciousness!  I co...          1\n",
       "6908      4  I recently visited Olive and Ivy for business ...          1\n",
       "6909      2  My nephew just moved to Scottsdale recently so...          0\n",
       "6910      5  4-5 locations.. all 4.5 star average.. I think...          1\n",
       "\n",
       "[5851 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(newline_replacer.replace)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6779063f",
   "metadata": {
    "id": "6779063f"
   },
   "source": [
    "También es necesario eliminar los doble espacios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "178a5e7d",
   "metadata": {
    "id": "178a5e7d"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class ExtraSpacesReplacer(object):\n",
    "    \"\"\" Replaces extra spaces in a text.\n",
    "    >>> replacer = ExtraSpacesReplacer()\n",
    "    >>> replacer.replace(\"and it was excellent.  The weather was perfect\")\n",
    "    'and it was excellent. The weather was perfect'\n",
    "    \"\"\"\n",
    "    \n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        s = re.sub('\\s\\s+', ' ', s)\n",
    "        return s\n",
    "\n",
    "spaces_replacer = ExtraSpacesReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e85adcdc",
   "metadata": {
    "id": "e85adcdc"
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(spaces_replacer.replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd14cd0",
   "metadata": {
    "id": "9dd14cd0"
   },
   "source": [
    "# 1. Obtención de datos a partir de información textual (4 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b68be",
   "metadata": {
    "id": "312b68be"
   },
   "source": [
    "## 1.1 Encontrar colocaciones (2 puntos)\n",
    "\n",
    "Recordemos que las colocaciones son términos multipalabra, es decir, secuencias de palabras que, en conjunto, tienen un significado que difiere significativamente del significado de cada palabra individual (e.g. New York tiene un significado distinto del que se puede derivar de New y de York)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de41ffd0",
   "metadata": {
    "id": "de41ffd0"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong>  Calcula los mejores bigramas y trigramas de las opiniones. (1 punto)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lk582wJzf5UM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lk582wJzf5UM",
    "outputId": "b419a309-88b8-496a-8944-e5db4806d0be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\eduar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "# Para este apartado es necesario cargar las siguientes librerías:\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.collocations import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6bfa1a6",
   "metadata": {
    "id": "f6bfa1a6"
   },
   "outputs": [],
   "source": [
    "#Importar la lista de stopwords en inglés de la libreria NLTK.\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "#Añadir stopwords\n",
    "stopwords = stopwords + ['unknown', 've', 'hadn', 'll', 'didn', 'isn', 'doesn', 'hasn' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79d8030",
   "metadata": {
    "id": "f79d8030"
   },
   "source": [
    "A partir del comando help(nltk.collocations.BigramAssocMeasures) explora la clase BigramAssocMeasures del módulo nltk.metrics.association y revisa las definiciones de las métricas de Likelihood Ratio (likelihood_ratio) y de Pointwise Mutual Information (pmi) se explica en el capítulo 5 del libro Foundations of Statistical Natural Language Processing (Manning & Schutze)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35defbdd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35defbdd",
    "outputId": "e29865b3-cdb8-42cc-8b61-0a8c1c3ae002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BigramAssocMeasures in module nltk.metrics.association:\n",
      "\n",
      "class BigramAssocMeasures(NgramAssocMeasures)\n",
      " |  A collection of bigram association measures. Each association measure\n",
      " |  is provided as a function with three arguments::\n",
      " |  \n",
      " |      bigram_score_fn(n_ii, (n_ix, n_xi), n_xx)\n",
      " |  \n",
      " |  The arguments constitute the marginals of a contingency table, counting\n",
      " |  the occurrences of particular events in a corpus. The letter i in the\n",
      " |  suffix refers to the appearance of the word in question, while x indicates\n",
      " |  the appearance of any word. Thus, for example:\n",
      " |  \n",
      " |  - n_ii counts ``(w1, w2)``, i.e. the bigram being scored\n",
      " |  - n_ix counts ``(w1, *)``\n",
      " |  - n_xi counts ``(*, w2)``\n",
      " |  - n_xx counts ``(*, *)``, i.e. any bigram\n",
      " |  \n",
      " |  This may be shown with respect to a contingency table::\n",
      " |  \n",
      " |              w1    ~w1\n",
      " |           ------ ------\n",
      " |       w2 | n_ii | n_oi | = n_xi\n",
      " |           ------ ------\n",
      " |      ~w2 | n_io | n_oo |\n",
      " |           ------ ------\n",
      " |           = n_ix        TOTAL = n_xx\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BigramAssocMeasures\n",
      " |      NgramAssocMeasures\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  chi_sq(n_ii, n_ix_xi_tuple, n_xx) from abc.ABCMeta\n",
      " |      Scores bigrams using chi-square, i.e. phi-sq multiplied by the number\n",
      " |      of bigrams, as in Manning and Schutze 5.3.3.\n",
      " |  \n",
      " |  fisher(*marginals) from abc.ABCMeta\n",
      " |      Scores bigrams using Fisher's Exact Test (Pedersen 1996).  Less\n",
      " |      sensitive to small counts than PMI or Chi Sq, but also more expensive\n",
      " |      to compute. Requires scipy.\n",
      " |  \n",
      " |  phi_sq(*marginals) from abc.ABCMeta\n",
      " |      Scores bigrams using phi-square, the square of the Pearson correlation\n",
      " |      coefficient.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  dice(n_ii, n_ix_xi_tuple, n_xx)\n",
      " |      Scores bigrams using Dice's coefficient.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  jaccard(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using the Jaccard index.\n",
      " |  \n",
      " |  likelihood_ratio(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using likelihood ratios as in Manning and Schutze 5.3.4.\n",
      " |  \n",
      " |  pmi(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams by pointwise mutual information, as in Manning and\n",
      " |      Schutze 5.4.\n",
      " |  \n",
      " |  poisson_stirling(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using the Poisson-Stirling measure.\n",
      " |  \n",
      " |  student_t(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using Student's t test with independence hypothesis\n",
      " |      for unigrams, as in Manning and Schutze 5.3.1.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  mi_like(*marginals, **kwargs)\n",
      " |      Scores ngrams using a variant of mutual information. The keyword\n",
      " |      argument power sets an exponent (default 3) for the numerator. No\n",
      " |      logarithm of the result is calculated.\n",
      " |  \n",
      " |  raw_freq(*marginals)\n",
      " |      Scores ngrams by their frequency\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.collocations.BigramAssocMeasures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f7371",
   "metadata": {
    "id": "837f7371"
   },
   "source": [
    "<i>Primer paso</i>: Obtener los tokens del texto de las opiniones. Etiqueta estos tokens por su PoS. (0.5 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "314750c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "id": "314750c1",
    "outputId": "5fcd023c-cf78-4308-e27c-931a935baa9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"my wife took me here on my birthday for breakfast and it was excellent. the weather was perfect which made sitting outside overlooking their grounds an absolute pleasure. our waitress was excellent and our food arrived quickly on the semi-busy saturday morning. it looked like the place fills up pretty quickly so the earlier you get here the better. do yourself a favor and get their bloody mary. it was phenomenal and simply the best i've ever had. i'm pretty sure they only use ingredients from th\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creamos texto en minúscula que recoja todas las opiniones\n",
    "\n",
    "opinions = \" \".join(df['text']).lower()\n",
    "\n",
    "opinions[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c27f9b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c27f9b8",
    "outputId": "1d989339-4a06-4a50-9629-dfd48ad803a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('my', 'PRP$'),\n",
       " ('wife', 'NN'),\n",
       " ('took', 'VBD'),\n",
       " ('me', 'PRP'),\n",
       " ('here', 'RB'),\n",
       " ('on', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('birthday', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('breakfast', 'NN')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                   #\n",
    "#############################################\n",
    "tokens = nltk.word_tokenize(opinions)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "pos_tags[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7287ecb",
   "metadata": {
    "id": "c7287ecb"
   },
   "source": [
    "<i>Segundo paso</i>: Calcular los 1000 mejores bigramas y los 1000 mejores trigramas a partir de los tokens etiquetados (e.g. [(Basic, JJ), ...]) del texto. Utiliza la métrica PMI o Likehood Ratio. Tienes que indicar por qué has escogido una y no otra. (0.5 puntos)\n",
    "\n",
    "<b>Atención</b>: De los 1000 bigramas y trigramas, elige a los que no comienzan ni terminan con una stopword."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2fe029",
   "metadata": {
    "id": "de2fe029"
   },
   "source": [
    "Recordemos la clasificación de etiquetas PoS.\n",
    "\n",
    "<b>Etiquetas PoS</b>\n",
    "\n",
    "<ul>\n",
    "<li>DT: Determinante</li>\n",
    "<li>JJ: Adjetivo</li>\n",
    "<li>NN: Nombre en singular</li>\n",
    "<li>NNS: Nombre en plural</li>\n",
    "<li>VBD: Verbo en pasado</li>\n",
    "<li>VBG: Verbo en gerundio</li>\n",
    "<li>MD: Verbo modal</li>\n",
    "<li>IN: Preposición o conjunción subordinada</li>\n",
    "<li>PRP: Pronombre</li>\n",
    "<li>RB: Adverbio</li>\n",
    "<li>RP: Partícula</li>    \n",
    "<li>CC: Conjunción coordinada</li>\n",
    "<li>CD: Numeral</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43dfad33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43dfad33",
    "outputId": "0521a232-af8c-4479-c1c8-bc471d69eaa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST BIGRAMS:\n",
      "[\"'08 cakebread\", \"'eggless eggrolss\", \"'how 'bout\", \"'real food'/a\", \"'visual fluffers\", '-50 gauges', '-elizabeth rozin', '-or- tinga', '............. yeppie', '/glenlivit /glenmorangie', '1/16 teaspoon', '1202 mohave', '1980s pseudo-asian', '1min 45sec', '2-4-1. woot', '4/ beach-head', '49er flapjacks', '4th-tier mid-90', '50¢ addtl', '65+ eyefucking', '=shiner bock', 'aa meeting-', 'abe frohman', 'administrative assistant', 'advent epicurean', 'afterglow punctuated', 'alain keller', 'alessandro marchesan', 'altering drugs', 'ambience=four estrellas', 'angello dorato', 'anita bryant', 'anxiously awaiting', 'aug 18.', 'authoritarian overlords', 'auto auction', 'avid youth', 'azucena tovar', 'bacon/bleu cheese/walnut', 'bahay kubo', 'band/art brut/we', 'bano squirting', 'bar/boxing ring/vintage', 'bartholomew novles', 'baton rouge', 'battling db', 'beach-head 2000', 'bellys aching', 'bengan bartha', 'bernie kantak', 'bhangra muzak', 'bifocal specs', 'big-mouth carp', 'bilbo baggins-ey', 'billy joel', 'bim bab', 'bistec encebollado', 'black-outs d+', 'bluntly honest-', 'bok choy', 'bona fides', 'borderline condemned', 'bp meds', 'bra straps', 'bubba gump', 'bucking broncho', 'buco confit', 'buon natale', 'busily hand-mixed', 'butterburgers w/fries/onion', 'buzzer beater', 'bò kho', 'cabrales fonduta', 'calcium propionate', 'canh chua', 'canolis occassionally', 'carniverous adventurer', 'cassette tapes', 'catcher mitt', 'celtic supporters-', 'cen pholks', 'cheeeeno banddddeeedo', 'cheese-red sauce-and-pepperoni', 'cheese/jewish sliders/patty', 'cheezecake crapory', 'chichen w/biscuits', 'chichi- lisous', 'chin-strap beards', 'chineness peple', 'chipote tabassco', 'chips/potato salad/macaroni', 'chua tom.i', 'chà giò', 'city-wide ordinance', 'college/recent grads', 'colonized indochine', 'competely unoticed', 'complientary champaigne', 'confrontation erupted', 'convivial father-son', 'cool-neat floral', 'corey haim', 'cozze posolipo', 'craig demarco', 'creepily stalking', 'crem burle', 'crossword puzzle', \"culinarily 'safe\", 'curling fists', 'cut-off fingertip', 'dakota fanning', 'dame edna', \"dance/top 40's/etc\", 'dank recesses', 'darcy mcgees', 'dare-devil thrills', 'dead-eyed gamblers', 'dee lish', 'deficient eyesight', 'deja vu', 'des tartes', 'dietetic arsenal', 'dimly-lit lounge-like', 'distrust fads', 'divorce papers', 'dizzie gillespie', 'dizzy dre', 'dogfishhead chicory', 'dominique ansel', 'domo arigato', 'donec obviam', 'double-stacked specialtly', 'downright-scary kool-aid-red', 'dramatic dialogue', 'dribble stain', 'durst clones', 'earthen embankment', 'edged sword', 'egg-foooo youngggg', 'egg-less eagrolls', 'elevated paradise/panera', 'ella fitzgerald', 'embassy suites', 'eminent gourmands', 'empties inbetween', 'ensalada cortada', 'ensenada michelada', 'esmechada patacon', 'euthanizing kittens', 'evoking shameless', 'fabby nom~noms', 'fashoined ny-style', 'fibber magee', 'ficticious busboy/girl', 'fifty-some-odd write-ups', 'fig/goat cheese/mint', 'fingernail clipping', 'fire-roasted tomoto', 'firestone dba', 'fleet foxes', 'floss vigorously', 'flourish deserving', 'fluorescent experiens', 'forcefully preside', 'french-fry-shaped fry-shells', 'frida kahlo', 'frutti dimare', 'ft. bragg', 'fuddy duddy', 'full-time wilderness', 'fullers esb', 'fund-raising campaign', 'furry scavengers', 'futbol kits', 'gamblers shuffling', 'ganbian sijidou', 'genetically mutated', 'germain elderflower', 'geting injured', 'giant-mega selection-awesomeness', 'glasgow celtic', 'glenfiddich /glenlivit', 'gloppy sweet/hot', 'granite reef', 'greg norman', 'grunting uggghhh', 'guo rou', 'haagen dazs', 'hagen daas', 'halcyon undergraduate', 'half-sandwich half-salad', 'hand-painted placards', 'har gow', 'hawaain special-', 'heinz 57', 'herbal infusions', 'hicima tostado', 'highs were-', 'holes-in-the-wall tacquerias', 'homer simpson', 'hopple popple', 'horizontal scroller', 'horrifically anally-retentive', 'hottee paula', 'hu tieu', 'hui guo', 'impress/first date/ex-boss/lover/landlord/twitter', 'inca kola', 'infantile gomers', 'innate intelligence', 'instagramaz honeys', 'intermittent brake', 'intestines rupture', 'ippei happhazardly', 'iridescent big-mouth', 'irish-style refreshments', 'itemizations correlated', 'itty bitty', 'ivaa summit', 'iz mir', 'jenna jameson', 'jerking hydraulics', 'joes/gift shop/paradise', 'journalists likened', 'joya lucia', 'just-ok carby', 'kang karee', 'karmic forces', 'keith galbut', 'khung nahm', 'khyber halal', 'kinki kooler', 'kore bicycles', 'kota athenian', 'krakauer wurst', 'kraut/knish/fresh chips/potato', 'kubo natin', 'kwediaw kraphrao', 'lapsang souchong', 'lead-pipe utilitarian', 'leathery palates', 'lindsey f.', 'littler ruff', 'log cabin', 'lololololololol diaf', 'longhorn cheddar-topped', 'lou malnati', 'luana joya', 'maduras fritos', 'magnum p.i', 'make-your-own cheeseboard', 'malai methi', 'manasquan inlet', \"manfest '07\", 'mapo doufu', 'marz gs/3', 'mascerated lima', 'masterful crescendo', 'mechwarrior 4/', 'mejimbo enterprises', 'mello yello', 'mellow-ambient rhythms', 'mere chapter', 'mexican/latin american-type', 'mid-century architecture', 'mille feuille', 'miranda geranios', 'mon-sat 7am-9pm', 'monosodium glutamate', 'muertos motif', 'mumbai-based hindi-language', 'munchener spaten', 'mì bò', 'nancy bartholomew', 'nectarine crustada', 'nerd talk-', 'new-recipe chile-pecan', 'newton claret', 'nic nacs', 'nightmarish openings', 'nikki buchanan', 'nikolai khabibulin', 'nj/ny natives', 'nodded enthusiastically', 'non-pizzeria bianco/cibo', 'non-smoking asthmatic', 'non-starch substitutes', \"o'pear grenache\", 'obviam redimus', 'odinian incantation', 'oiled slick', 'old-timey newspaper-menu', 'older/slightly cougarish', 'olive- however-', 'oliver twist-like', 'organic/green mocvement', 'otter creeks', 'over-fried french-fry-shaped', 'overpoweringly oniony', 'padded banko', 'pancreatic islets', 'paralyzing indecision', 'pattrick fagan', 'pavlovian stimuli', 'pay- 50¢', 'peaks/papago/sun up/san', 'pell mell', 'perrier jouet', 'phad kaprow', 'philosophical elder', 'photographic blow-ups', 'pickled/preserved veggies/meat/fruits', 'pile-o-fajita yellows', 'pinata nueva', 'pissed-off every-time', 'placard declaring', 'poca cosa', 'policia extorting', 'politically motivated/involved', 'possessive apostrophe', 'potato-chevre strudel', 'powered/laser sighted', 'preconceived notions', 'precooked-then frozen-then', 'premature ejaculation', 'prescribed mordida', 'preventative maintenance', 'primera division', 'production standpoint', 'profession commonly', 'pudgy married-guy', 'qed biatch', \"r.t. o'sullivan\", 'racial slurs', 'ragin cajuns', 'ram 2500hd', 'rapturous applause', 'ratpack ghosts', 'raven-haired maidens', 'reckless auteur', 'red-chili frybread', 'registers side-by-side', 'rehab/wet republic', 'relentless barrage', 'remote patchwork', 'removable pneumatic', 'repurposed 1950s', 'reusable milk-type', 'ring/vintage store/food', 'rugged junkyard', 'sai quai', 'sais quoi', 'sarcastic non-clean', 'sauce/ flavor/', 'savvy shopper', 'sayaka watanabe', 'scavengers vie', 'serbian lepinja', 'serptember 8,2012', 'service-small portions-pricey', 'seton h.s', 'sh1tapple wudda-wudda-waa-waa', 'shank- angello', 'shirley temple', 'shiso horny', 'shop/creperie/hole-in-the-wall bundle', 'sighted rifle', 'silk-lined zanella', 'singing/shrieking girl/guy', 'slash am/pm', 'sliders/patty melt/kosher', 'slighly scewed', 'smearing nastily', 'soupier salvadorean', 'souths sould', 'spaten pils', 'spearmint rhino', 'specified zip', 'spewed racial', 'spinto band/art', 'stealthy surcharges', 'stephen g.', 'store/food truck/soda', 'strategic placement', 'stringent regulations', 'subliminal zap', 'sushi-related inquiries', 'switch/fez/ticoz dynasty', 't.v .................', 'tagliatelle w/braised', 'taleggio caravaggio', 'taped together-double', 'teat twiddled', 'terrifying short-cut', 'thereby appeasing', 'thin-slice sauce-and-cheese', 'thrill seeker', 'ticking timebomb', 'tina turner', 'tinga poblana', 'tomato-chanterelle ragout', 'tongued restaurant-goers', 'torrential downpour', 'toto toilets', 'touchy feely', 'transferring tabs', 'transplanted midwesterner', 'très délicieux', 'tweener restaurant-perfect', 'unanswered e-mails', 'unfiltered stogies', 'unlit votive', 'unsmiling demeanor', 'upsetting philosophically', 'v95 advertises', 'vaca encebollada', 'valpolicella ripasso', 'vanguard investments', 'vaporized euphoria', 'vintage-looking house-turned-restaurant', 'vist juisit', 'vueve clicquot', 'wanton slaughter', 'water-stained panels', 'westside dewelers', 'wi fi', 'willie mays', 'willy wacky', 'woefully underrepresented', 'wowwy mowwy', 'xl three-topping', 'yekemem shai', 'youngggg burrritos', 'yuxiang qiezi', 'zha jiang', 'zippy doo', 'zur kate', '¡buen apetito', '¡muy auténtico', 'école lenôtre', 'òc châm', '-bon appetit', '06 sauv', '26/whole 12-13', '30k millionaire', '30k millionaires', '32oz thirstbuster', '40-45 mintues', '66 roadside', '730pm andrea', '90s alt', 'acid reflux', 'adventuresome palettes', 'aircraft carrier', 'annual thanksliving', 'anti pasti', 'antiseptic wipes', 'archery headquarters', 'arnold palmers', 'aural assault', 'autentica comida', 'ba tampte', 'back-story imagery', 'badger penis', 'badman shoveled', 'baggins-ey cauldron', 'balancing precariously-and', 'balsalmic vingerette', 'balsalmic vinigarette', 'baltimore md', 'banquette seating-', 'bao ji', 'barra viejo', 'becuz confucius', 'beginner golfer', 'begrudgingly concede', 'bi bim', 'bible verses', 'bing heng', 'blase housewife', 'bohemian bloodletting', 'boullion method', 'bouncers/id checkers', 'bourguignon julia', 'brake pedal', 'bred midwestern', 'burly counterparts', 'bus-guy approaching', 'cachapas jamon', 'caked-on makeup', 'capellini checca', 'carelessly drained', 'carnivorous yelpy', 'cavernous sport-bar/pool', 'cedar planked', 'celebrity bashing', 'chamange paring', 'champange paring', 'charitable donation', 'charles lafitte', 'cheerleader blush', 'chicekn tika', 'cien agaves', 'clear-heeled harmony', 'clint eastwood', 'commence slurping', 'conchita pibil', 'corwn molding', 'cotes du', 'coy toying', 'créme brûlée', 'culmination hang-out', 'curry-only spoonful', 'cust serv', 'dark-haired shirtless', 'darned tootin', 'daytime nudity', 'decorative koi', 'desolate stripmall', 'developing cca', 'dobo wat', 'dom champagnes', 'doug robson', 'efficiency bordered', 'ego stroked', 'embezzled bacardi', 'enforcement squad', 'environmentally irresponsible', 'eugene levy', 'expresses nonverbal', 'extended periods', 'famously zombie-like', 'feng shui', 'fingerprint bacteria', 'flee pell', 'flirtatious pose', 'frijoles negros', 'furrowed eyebrows', 'galze thingy', 'gen-u-ine registered', 'glossy graphics', 'granola/yogurt/fruit parfaits', 'grass-fed cows', 'gustatory flashbacks', 'h2o hydration', 'handily decanted', 'hang-over recovery', 'hard-core italians', 'herd instinct', 'hereby declare', 'holders waltzed', 'hon machi', 'horatio hernandez', \"hors d'oeurves\", \"hors d'oeuvres\", 'howell iii', 'howie mandel', 'hum vn', \"hummus/tahini 'sauce\", 'hurricane irene', 'hush puppies', 'impromptu tennis', 'inducing flavorgasm', 'infested kikkoman', 'infinite pun', 'inner-14-year-olds chuckle', 'inspires laughter', 'internal organs', 'intuitive fussing-over', 'janet reno', 'jer lees', 'jet-turbine engine', 'jhane barnes', 'kamikaze pilot', 'knick knacks', 'kokopelli swill', 'kreme doughnuts', 'krispy kreme', 'library whispers', 'lighthouse beacon', 'livestock provider', 'loaner personnel', 'lovey howell', 'magnitude shittier', 'mah jer', 'mall-made mayo-based', 'manufacturer hundreds', 'masai-esqe cage', 'mattar malai', 'mesa-gateway fliers', 'metallic p.', 'meth billboard', 'micky dees', 'mixto clasico', 'montenegro amaro', 'monthy tennis', 'mothers/ fathers', 'mrs lovey', 'mythical arcas', 'nahm prik', 'negra modelo', 'nerve wracking', 'nifty hand-written', 'non-profit organizations', 'noticable insect', 'offending microbes', 'overnight shipping', 'overripe cantaloupe', 'oy vey', 'pacing goes-i', 'pakistani leisure', 'parched winkies', 'parental units', 'patatas bravas', 'performed pourings', 'performed reiki', 'petty -my', 'phuong dong', 'piccola cucina', 'pier 83', 'pierre herme', 'pig-out stunt', 'playful mohawk', 'pomme frittes', 'post-brunch champers', 'pourer restrictors', 'preset grinders', 'presets/the rapture', 'presidential especiale', 'prince edward', 'prolonged zombie', 'rapid succession', 're-heat one-stop', 'regrettable blandness', 'regulatory agency', 'reina pepiada', 'responds clumsily', 'robotic lifeless', 'rope flanked', 'rudys lf', 'rustlers rooste', 'sa tay', 'sacred war', 'samich w/fries', 'sean connery', 'secretly despised', 'sega wat', 'selfish purposes', 'sewer vent', 'shins albums', 'shore wannabes', 'siamese kisses', 'silent wanderers', 'silvana salcido', 'simmons workout', 'snicker doodles', 'soapy suds', 'somtimes primal', 'sparkly light-lit', 'sparkly sequined', 'sparsely populated', 'spiritual awakening', 'spotlights glare', 'squished elbow-to-elbow', 'sri lanka', 'stoic gothic', 'strengthens ot', 'striking photography', 'stronghold mangus', 'substantially decreases', 'sullen disposition', 'super-acidic-1.0 ph', 'surveillance cameras', 'sustainable/organic livestock', 'susy talks', \"take-home 'fajita\", 'taxicab suspended', 'teacher misrepresent', 'techno remixes', 'terra-cotta warriors', 'textural variance', 'thankyou denise', 'thick-ass slabs', 'tieu xao', 'timeless tina', 'too- apothic', 'tood darned', 'tootsy makie', 'tortured hollow', 'tracy dempsey', 'trash- resting', 'trending analysis', 'trending artisanal', 'tum nak', 'tutti santi', 'tyrrellspass castle', 'unmanipulated masterpiece', 'unseat maxim', 'unwelcome outsiders', 'upholstered loveseats', 'vague disapproval', 'val vista', 'val vista/warner', 'veddddy goood', 'veins rugged', 'veritable torrent', 'vietri chipped', 'viking camp', 'visibly perturbed', 'voice-over actor', 'vow solemnly', 'wade moises', 'wells fargo', 'whisky slut', 'wildest princess', 'willfully ignorant', 'wishful thinker', 'yao ming', 'yer horses', 'zagat announcement', 'zanella slacks', 'bánh xèo', 'calle dieciseis', 'dac biet', 'siem reap', \"'when harry\", \"'your gum\", '10.79 sr.', '12.49 sr.', '30.00 apiece', '400s hollywood', '6.29 sr.', 'accurately convey', 'achievement trophies', 'airplane hanger', 'algoo gobi', 'alicha sega', 'american-type brunches', 'applebees locales', 'armando bros', 'artery cloggin', 'artery clogging', 'artery hardening', 'asses gossiping', 'azucar peruvian', 'bacchanalian orgy', 'bathing suits', 'better-tasting distinctive', 'betty boop', 'bhel puri', 'biscotti crumble', 'bleeding subsided', 'boisterous din', 'bollo neapolitan', 'bone-warming elixir', 'bow-tie clad', 'bowling videogame', 'brad pitt', 'brough amaretto', 'carefree grandparents-in-law', 'carribbean colada', 'carving amidst', 'cask 63', 'caveat emptor', 'celiac disease', 'checkered table-cloths', 'childlike tolerance', 'chipolte blindfolds', 'cinema suite', 'circling iridescent', 'cityscape lineup', 'colada redux', 'com tam', 'com tam-foolery', 'com tam=', 'conveyer belts', 'creates inspirational', 'crowning achievement', 'cruses/el paso', 'dahi vada', 'dashing maitre', 'decades-old economic', 'deere tractor', 'defunct azn', 'diagnosable disease', 'diamondbacks/rockies scores', 'dogfish shark', 'dragonfly combines', 'drug addicts', 'du rhone', 'dumpy outskirts', 'ehtiopian lager', 'equipment infomercials', 'eric clapton', 'exhibit /sale', 'fermier brasserie', 'fest goggles', 'fetzer chardonnay', 'financial limitations', 'flooding actions', 'frat-party gimmicks', 'fresh-grated wasabe', 'fucked-up rendition', 'ganache filling/icing', 'geriatric nonsense', 'germs grosses', 'ghetto-tastic shack', 'ghormeh sabzi', 'glove lodged', 'golfing boondoggle', 'gray skies', 'gtfo dodge', 'guero canelo', 'gulab jamuns', 'haim 2007', 'haired brunets', 'highway robbery', 'hob nob', 'hollow tubes', 'hollywood glamour', 'horseback riding', 'hubristic haze', 'hugh heffner', 'hyatt gainey', 'incessant flipping', 'incorporated guedo', 'insert surname', 'instinctively sensed', 'jalepeno quota', 'jay bogsinski', 'jetland suite', 'joan cusack', 'joes bbq/joes', 'josh hebert', 'julian wright', 'keye wott', 'kha ta-lay', 'kinneson aggressive', 'labor intensive', 'lactose intolerants', 'lager belgium', 'lazyboy recliner', 'leavong chinatown', 'lloyd wright', 'lonely loser', 'loyd wright', 'macmillan acclaimed', 'mammoth proportion', 'mans rehab/wet', 'marco polo', 'mast va', 'measly odinian', 'mel mecinas', 'midget controlling', 'musical analogue', 'musician lovergirl', 'nba finals/happy', 'near-by westward', 'neighboring thrift', 'nonverbal displeasure', 'obediently complied', 'ogfcourse adjust', 'ordered- matzoh', 'pani puri', 'panko-crusted barrumunda', 'papas fritas', 'papdi chaat', 'parasitic disease', 'paula creamer', 'phat si', 'phnom penh', 'phnum penh', 'physical exertion', 'pianist bonding', 'piña colada', 'pomodoro sauce/sausage', 'portobello sanwhich', 'potentially disastrous', 'predestroyed designer', 'president obama', 'pro-bicycle bar/restaurant', 'puffy pillow-like', 'r. kline', 'rand h.', 'rat pack-style', 'recite reservoir', 'regan-era excess', 'relations 101.', 'reluctant meat-eating', 'reputable websites', 'rick springfield', 'roach coach', 'sabudaana kheer', 'salcido esparza', 'satellite speaker', 'scantily clad', 'schweizer schnitzel', 'seamus mccaffrey', 'sewage pipes', 'shameless americana', 'shilling scottish', 'shopper rag', 'shortcoming stems', 'si iew', 'sierra bonita', 'sierra nevada', 'simplistic stunner', 'sindhi biryani', 'sizeranne 98', 'skilled kikisake-shi', 'sopa pillas', 'soup-nazi routine', 'sous vide', 'southeast asia', 'spam musubi', 'sparked inspiration', 'staunch environmental', 'strenuously object', 'superstar waitress-', 'sur dogfish', 'tam dac', 'tango eggees', 'tech companies', 'thirst quenching', \"titled 'spicy\", 'trainee dawn', 'trough slurpers', 'tullamore dew', 'undercover shoppers', 'undergraduate afternoons', 'unparalleled resource', 'unwarmed ceramic', 'usain bolt', 'usl/ul lafayette', 'va mosear', 'veggie-filled non-greasy', 'villa peru', 'waaaay over-seasoned', 'wallets intact', 'weighed-down yucky', 'wilderness gig', 'winston glaring', 'wobbled leaning', 'worthless jhane', 'woulda thunkkkk', 'yebeg alicha', 'yekik alicha', 'yummers blondie', 'aguas frescas', 'thou shalt', 'vice versa', \"'overly proactive\", '12,000 sedona', '13/person w/o', '2011 stanley', '2:30 inthe', '623 adams', '80s porn', 'accomplished cellist', 'agent sands', 'air-conditioned expansion', 'ajo als', 'arnold palmer', 'artful one-of-a-kind', 'aw aw', 'baby-making suburbs', 'bacardi flask', 'baileys nightcap', 'banh beo', 'banh xeo', 'baratin/fnb empire', 'beep-beep beep-beep', 'begun anew', 'bingsoo buyers', 'biz markie', 'blankity blank', 'bloated playdoh', 'boa bowtie', 'bonding session', 'borne illness', 'bound binder', 'brat haüs', 'brent karlicheck', 'briny spears', 'buena bella', 'bumble bee', 'bystander reported', 'candlelit passageway', 'cardstock copied', 'centurion guarding', 'chaka chaka', 'chef-owner tottie', 'cherish thee', 'chimichurri flank', 'chocolate/strawberry scones', 'cin cin', 'circa 1955', 'civiche ensalda', 'clamored five-star', 'clinic appointment']\n",
      "\n",
      "\n",
      "BEST TRIGRAMS:\n",
      "['-or- tinga poblana', '4/ beach-head 2000', 'bahay kubo natin', 'bar/boxing ring/vintage store/food', 'canh chua tom.i', 'cheese/jewish sliders/patty melt/kosher', 'dead-eyed gamblers shuffling', 'donec obviam redimus', 'egg-foooo youngggg burrritos', 'furry scavengers vie', 'glasgow celtic supporters-', 'glenfiddich /glenlivit /glenmorangie', 'hui guo rou', 'iridescent big-mouth carp', 'kraut/knish/fresh chips/potato salad/macaroni', 'luana joya lucia', 'mechwarrior 4/ beach-head', 'munchener spaten pils', 'mì bò kho', 'nancy bartholomew novles', 'over-fried french-fry-shaped fry-shells', 'pay- 50¢ addtl', 'powered/laser sighted rifle', 'ring/vintage store/food truck/soda', 'shank- angello dorato', 'spewed racial slurs', 'spinto band/art brut/we', 'bi bim bab', 'bilbo baggins-ey cauldron', 'flee pell mell', 'hu tieu xao', 'intermittent brake pedal', 'khung nahm prik', 'mattar malai methi', 'mrs lovey howell', 'silk-lined zanella slacks', 'sustainable/organic livestock provider', 'timeless tina turner', 'veins rugged junkyard', '400s hollywood glamour', 'circling iridescent big-mouth', 'corey haim 2007', 'evoking shameless americana', 'full-time wilderness gig', 'halcyon undergraduate afternoons', 'hottee paula creamer', 'mans rehab/wet republic', 'mast va mosear', 'measly odinian incantation', 'mexican/latin american-type brunches', 'phat si iew', 'piña colada redux', 'savvy shopper rag', 'sur dogfish shark', 'yebeg alicha sega', 'chef-owner tottie kaya', 'embezzled bacardi flask', 'five-star ambience=four estrellas', 'fred durst clones', 'gong bao ji', 'hateful unsmiling demeanor', 'high-pitched repeated squeals', 'impending superbowl xlv', 'krispy kreme doughnuts', 'linguine frutti dimare', 'lovey howell iii', 'low-brow fred durst', 'mah jer lees', 'miranda geranios instructed', 'nahm prik pow', 'ne sai quai', 'ne sais quoi', 'oliver twist-like methods', 'ped gang daeng', 'precious pancreatic islets', 'prolonged zombie stumble', 'rapper biz markie', 'thatch palm fronds', 'coup des tartes', 'gamberi alla pana', 'mumbai-based hindi-language film', \"'08 cakebread sauvignon\", 'alicha sega wat', 'autentica comida mexicana', 'bourguignon julia childs', 'chapoutier le sizeranne', 'cotes du rhone', 'd. black-outs d+', 'dizzie gillespie painting', 'dudes 65+ eyefucking', 'exercise equipment infomercials', 'expresses nonverbal displeasure', 'friggin cheezecake crapory', 'germain elderflower liqueur', 'hawaain special- composed', 'impress/first date/ex-boss/lover/landlord/twitter pal', 'monthy tennis membership', 'nirvana inducing flavorgasm', \"nutritional info 'tear\", 'predestroyed designer deconstructed', 'silvana salcido esparza', 'staunch environmental activism', 'thom kha ta-lay', 'tortured hollow tubes', 'unlit votive candles', 'usl/ul lafayette paraphernalia', 'worthless jhane barnes', 'beach-head 2000 unit', 'gm miranda geranios', 'porcini rubbed delmonico', 'trader joes/gift shop/paradise', 'ali vow solemnly', 'burratta forno cotto', 'ed hardy bling', 'framed publications touting', 'hunka hunka porky', 'je ne sai', 'je ne sais', 'kanye yelping phase', 'marnier galze thingy', 'osso buco confit', 'presidential primary election', 'sichuan gong bao', 'sodium infested kikkoman', 'artois lager belgium', 'brut france sparking', 'cen pholks lisa', 'com tam dac', 'midget controlling assholes', 'montpellier cabernet savignon', 'peaks/papago/sun up/san tan', 'resemble drug addicts', 'tam dac biet', \"'real food'/a substantial\", 'badman shoveled coal', 'bao ji ding', \"hors d'oeurves packages\", 'infinite pun jokes', 'korean-mom kim bop', 'reina pepiada arepa', 'removable pneumatic tire', 'reusable milk-type jars', 'unwashed morons gambling', 'billy joel song', 'precooked-then frozen-then reheated', '-tasty seaweed -masago', 'ancient sri lanka', 'anti-bacterial soap pumps', 'cask conditioned ales', 'decidedly carnivorous yelpy', 'deff replace cr', 'dutton estate chardonnay', 'orangewood cheer squad', 'pianist bonding session', 'rain sacred war', 'retired mormon doctors', 'robotic lifeless performance', 'simmons workout gear', 'kobe vaca encebollada', 'repurposed 1950s bank', 'voga pinot gris', '32oz thirstbuster beforehand', 'balsalmic vingerette reduction', 'heavily oiled slick', 'inflexible ear drums', 'largely unmanipulated masterpiece', 'mechanical bull hehehe', 'route 66 roadside', 'slighly scewed toward', 'decades-old economic relationship', 'economic relationship betweeen', 'hugh heffner spouse', 'join lindsey f.', 'lloyd wright blvd', 'relations 101. tossing', 'salcido esparza travels', 'annual thanksliving festival', 'frutti dimare margherita', 'mom-and-pop paris bake-shop', 'publicly-underwritten public transit', 'specified zip code', 'thepanko crusted tipalia', 'sensible comments concerning', 'école lenôtre trained', '06 sauv blanc', 'dearly departed wildlife', 'fabled sonic drive-in', 'le sizeranne 98', 'highs were- beet', 'massive photographic blow-ups', 'were- beet salad-', '800 degree air-fryers', 'candlelit passageway leads', 'carin s. witnesses', 'dogfishhead chicory stout', 'ho hum vn', 'kik misir watt', \"o'pear grenache omelette\", 'paul newman oreos', 'pointe hilton squaw', 'richard simmons workout', 'sarcastic non-clean district', 'sorority slasher film', 'tenth degree abou', 'everything/ conference bagged', 'haim 2007 era', 'taped together-double buck', 'there- buck hunter', 'tiki carving amidst', '66 roadside culture', 'unforgivable disaster masquerading', 'j auto auction', '/glenlivit /glenmorangie trio', '200 charitable donation', 'beau macmillan acclaimed', 'cavernous sport-bar/pool hall', 'manic drivers race', 'on-line canadian pharmacy', 'poppy seed bialy', 'propane heater involved', 'u.s. grade countrilicous', '| biscotti crumble', '~ luana joya', 'gen-u-ine registered historic', 'nifty hand-written updated', 'pre-made granola/yogurt/fruit parfaits', 'sparkly sequined shirt', '54 bánh xèo', 'congealed liver patee', 'kissing george clooney', 'named alessandro marchesan', 'yadda yadda yadda', 'brent karlicheck era', 'conjure mediterranean breezes', 'james spins sic', 'jess boneless wings-', 'recycled refer units', 'weak tongued restaurant-goers', 'st. germain elderflower', 'chuck norris jokes', 'covers stylishly displayed', 'dddinfo @ mac.com', 'following battling db', 'georgia girl- pbr', 'glutton mode chomping', 'lifetime achievement trophies', 'macadamian nut pazookie', 'near-by westward ho', 'pretentious bhangra muzak', 'sh1tapple wudda-wudda-waa-waa pretentious', 'terra cotta warriors', 'wudda-wudda-waa-waa pretentious bhangra', 'convivial father-son team', 'cin cin pronounced', 'es mucho mierda', 'fuck-you facial expression', 'israeli cous cous', 'phò phú thành', 'store/food truck/soda fountain', 'hollywood glamour meets', 'puffy pillow-like outer', 'uncomfortable padded banko', '1980s pseudo-asian theme', 'baggins-ey cauldron arrives', 'cozze posolipo mussels', 'downright-scary kool-aid-red color', 'jersey shore wannabes', 'kimchi becuz confucius', '5-mile maximum radius', '5130 w. baseline', \"holy manfest '07\", 'james bond shaken', 'marriott canyon villas', 'police baracade tape', 'sauce/ flavor/ seasoning', 'acquired 200 bruises', 'aka com tam=', 'countless hiking trails', 'crumbly sugary delicouisness', 'delicately textured phyllo', 'el bano squirting', 'etoh didnt evaporate', 'fashioned pina colada', 'm. chapoutier le', 'proven medical benefits', 'wearing chin-strap beards', 'longhorn cheddar-topped patty', 'station slash am/pm', '/ horizontal scroller', 'joes/gift shop/paradise bakery', 'sell unfiltered stogies', 'vertical / horizontal', 'ya infantile gomers', 'alamos ridge malbec', 'amarone cherries |', 'camarones culichi shrimps', 'force unsuspecting strangers', 'jenny r refers', 'statue facial expressions', 'tie dye t-shirt', '26/whole 12-13 bones', 'biet bo vien-', 'calorie/nutritional info printed', 'com tam= deliciousness', 'impromptu tennis match', 'james beard nominee', 'las cruses/el paso', 'rocker alice cooper', 'sen. ken cheuvront', 'woefully underrepresented compared', 'ye lazy masses', 'performed pourings properly', 'taleggio caravaggio cheeses', 'influential culinary grades', 'steady target jumps', 'handlebar j saloon', 'wildest princess wedding', 'cool-neat floral booths', '80 shilling scottish', 'artwork depicting scenes', 'chicekn tika masala', 'david petite petit', 'gogh alive exhibit', 'hemp-seed infused humboldt', 'hood rides digs', 'industrial-chic bungalow built', 'quarterly creme brulet', 'los muertos motif', 'september 24th 2008.', 'cantina laredo professes', 'chef-lebrity robert mcgrath', 'explaining nutrition guru', 'gambas con ajos', 'lifetime maximum instances', 'sacred war dance', 'war dance suspended', '300-degree tempura crusties', 'corkage fee waived', 'insulting ethnic caricatures', 'york thin-slice sauce-and-cheese', 'colon cancer risk', 'shortcake whoopie pies', 'texas =shiner bock', 'aa meeting- alcohol', 'cakebread sauvignon blanc', 'oaky cabernet sauvignon', 'onehope cabernet sauvignon', 'oregonian missing deschutes', 'sam kinneson aggressive', 'ivy league schools', 'jess jess boneless', 'mass organizer arranged', 'opinions undeservedly harsh', 'produces vibrant fiesta', 'toe nail clippings', 'baja sur dogfish', 'generally distrust fads', 'hagen daas vanilla', 'prince edward island', 'nodded enthusiastically throughout', 'normal glenfiddich /glenlivit', '36 inch enlargement', 'abbaye de bellocq', 'atlantic haddock provencal', 'chef/owner jeff smedstad', 'epic swirl ins', 'frank lloyd wright', 'frank loyd wright', 'fruite de mare', 'goi cuon thit', 'melt-in-your-mouth bartlett pears', 'miguel de allende', 'pakistani leisure class', 'patronizing casey moores', 'primera division de', 'rachel t. sums', 'sandwhiches ar e', 'speakeasy .the tuscan', 'text message shorthand', 'verduras de temporada', 'double edged sword', 'deja vu moment', 'troon managed properties', 'velvet rope flanked', 'critical mass organizer', 'fearless critical mass', 'cin pronounced chin', 'disposable aluminum baking', 'fund raising events', 'mexi-style osso bucco', 'thee argentine po-boy', 'penne alla campagna', '15,000 calories disappear', 'encourage automobile sales', 'mentions automatic gratuity', 'trash- resting among', '1.98 mini puddings', 'slash am/pm mini', \"pastry hors d'oeuvres\", 'budget bacchanalian orgy', 'butterburgers w/fries/onion rings', 'expecting bernie kantak', 'filing numerous growlers', '50 chà giò', 'canyon trails towne', 'earthy wood-fired imagery', 'nicole pesce plays', 'pouring skills shined', 'professed condiment whore', 's. witnesses claim', '65 cent surcharge', 'beau macmillan presents', 'bonnet wearing geese', 'classify le chalet', 'el presidential especiale', 'mandatory hospital gown', 'marigold flower garnish', 'signed baseballs jerseys', 'stella artois lager', 'supporting cast complements', 'warned tablemate ericka', 'yelp.com search reveals', 'excessively complex scenarios', 'lactose intolerants beware', 'manhandling non threatening', 'otter creeks =', 'tom jetland suite', 'carry antiseptic wipes', 'dozen krispy kreme', 'atmosphere-very industrial meets', 'finest vehicles backed', 'total pudgy married-guy', '/ mixto clasico', 'basically just-ok carby', 'escabeche / mixto', 'giant meth billboard', 'indy resturant failure', 'johnny chu snorting', 'mixto clasico /', 'brown dribble stain', 'embassy suites hotel', 'just-ok carby kid', 'bhangra muzak playing', 'e. lodge stimulus', 'leather bound binder', 'reported aloo tikki', 'then-thirty minute absence', '50¢ addtl charge', 'oh-la-la wee wee', 'creepily stalking folks', '-greeted immediately -lots', 'foraged mushroom rissotto-correctally', 'lead-pipe utilitarian interior', 'poor service-small portions-pricey', 'alice cooper posters', 'dolce della casa', 'fogo e brasa', 'girl- pbr tall', 'gps tracking unit', 'hemp cargo shorts', 'john deere tractor', 'overworked waitresses franticly', 're-opened september 10th', 'da vang devotee', 'easter island statue', '90 sec hbo', 'flirtatious pose hanging', 'player dan marjerle', 'upgraded irish pub/bar', 'zagat announcement earlier', 'bosnian/jugoslavian cuisine catagory', 'teaks touch ém', 'employees busily hand-mixed', 'ficus tree growing', 'framed magazine covers', 'india.the term bollywood', \"kool aid 'drank\", 'la homer simpson', 'la marz gs/3', 'la pinata nueva', 'la primera division', 'magazine covers stylishly', 'profession commonly says', 'shilling scottish ale', 'terra-cotta warriors charming', 'turf accountant fails', 'raise costs/prices huh', 'yelping soul leaped', 'bahn mi sandwiches-', 'jon ric fashion', 'alterna/artsy coronado hood', 'anglaise | chantilly', 'brief activity occurred', 'community fund raising', 'designer deconstructed jeans', 'jason silberschlag pulls', 'reverse racism theory', 'super-acidic-1.0 ph version', 'fig/goat cheese/mint bruschetta', 'bare minimum coverup', 'estilo del rey', 'arroz con gandules', 'boys woop woop', 'high-school hostess wagged', 'orecchiette alla puttanesca', 'strategically placed bunkers', 'unfortunate non-distinct animal', 'cruses/el paso foodie', 'matzoh ball soup-', 'ordered- matzoh ball', '..................................................................................................................... heated toilet', 'ave midday madness', 'johnston concept yields', 'tex mex restuarant', \"um 'overly proactive\", 'bucca di beppo', 'flashy stainless steel', 'hindi-language film industry', 'nutrition guru dr.', \"typical dance/top 40's/etc\", 'typical pile-o-fajita yellows', 'lamb shank- angello', 'mascerated lima bean', 'apathetic hipster pj', 'coq au vin', 'deconstructed ny taxicab', 'el guero canelo', 'freshwater eel nigiri', 'grand marnier galze', 'leaving susy talks', 'lotus lamp shades', 'ny taxicab suspended', 'resembles congealed liver', 'tin foil wrapping', 'mine thereby appeasing', 'bouncer aggressively pushing', 'phones and/or ipods', '[ insert surname', 'tapas papas fritas', 'surfing monkey statues', 'teal cacti pictures', 'thit nuong cha', 'dyson hand dryers', 'carne esmechada patacon', 'half-sandwich half-salad combo', 'roach infested 24', 'teenagers carry guns', '7 mì bò', 'caldo 7 mares', 'chua tom.i hope', 'jimmy kimmel hired', 'purchasing alcoholic beverages', \"craving 'real food'/a\", 'gone competely unoticed', 'island gingered yam', 'clasico / ruby', 'evaluative writing topic', 'heated cat turds', 'wall nic nacs', \"'ol standby village\", 'acoustic guitar wrapping', 'bamboo plant centerpiece', 'bananas amarone cherries', 'carburetor cleaning strength', 'convince bri d.', 'de frijoles negros', 'dos molinos abogado', 'fritoras de frijoles', 'revisit tryst café', 'richard simmons gear', 'ba tampte pickles', 'fame career plastered', 'dac biet bo', 'nowhere mentions automatic', 'rubbery mat beneath', 'trending analysis showed', 'blart mall cop', 'fluorescent experiens filled', 'roasted tomato-chanterelle ragout', 'rugged junkyard dog', 'sliders/patty melt/kosher dog', 'hairs celebrating secretary', 'pronounced chin chin', 'ripped jeans notwithstanding', 'chicharron quesadilla estilo', 'gtfo dodge attitude', 'offered complientary champaigne', 'raw veins rugged', '//www.yelp.com/biz/daily-buffet-phoenix # hrid:3-lmrtz9ydgpxf6i2wgfha', '//www.yelp.com/biz/la-grande-orange-grocery-phoenix # hrid:4cxbhzxxtmexf9krjmfviq/src', 'cafe poca cosa', 'select=m1wrcux3oouhhrynl8kjqg # m1wrcux3oouhhrynl8kjqg', 'select=mmwkpmltbkcwnnlrai-tgw # mmwkpmltbkcwnnlrai-tgw', 'select=o9jiksb-mobhcxoi_msekq # o9jiksb-mobhcxoi_msekq', 'select=rk8m1rxl50pb69_c2yqqtg # rk8m1rxl50pb69_c2yqqtg', 'select=xg8u6fz2e8hu0xq3hf7czg # xg8u6fz2e8hu0xq3hf7czg', 'golfing and/or glued', 'handed polestar pilsner', 'urban multi-use warehouse', 'delicous nordique crepe', 'wear alice cooper-like', 'bourbon barreled porter', 'hopefully air-conditioned expansion', 'popped collar frat', '90s alt rock', 'bald headed caucasian', 'black olive- however-', 'button tufted leather', 'lack-luster curb appeal', 'roastburger kicks butt', 'snobby las cruses/el', 'mango tango eggees', '-fresh tuna -soft', 'appetizer- soft pretsels', 'bonfire celebrating richardson', 'dot bomb misogynist', 'lush foliage present', 'mall-made mayo-based california', 'monumental fucking embarrassment', 'post tourney repast', 'products liberally displayed', 'riding dirt bikes', 'seperate employess thanked', 'sterile designer package', 'tagliatelle w/braised short', 'wearing swim trunks', 'yelping phase ends', 'luncheon carts.tthe stir', 'problem transferring tabs', 'words highway robbery', 'cooper-like eye makeup', 'manzana lift rocks', 'newer waitors tend', 'british coal miners', 'compass via opentable', 'county health inspector', 'discussed environmental concerns', 'failing health codes', 'failing health inspections', 'lee oriental supermarket', 'multi-purpose entertainment facility', 'pastelitos con papas', 'tart granny smith', 'zha jiang noodles', 'mind altering drugs', 'mind creepily stalking', '-shaped balls floating', '108 east pierce', 'total whisky slut', 'incapable decision maker', 'velvet wallpaper adorn', 'art exhibit /sale', 'comfortably out-of control', 'converted loft warehouse', 'ladies re-living prom', 'nw indiana originally', 'registered historic landmark', 'ruby tuesdays hangs', 'shazzam lighting bolt', 'creaky wooden floors', 'exclusively senior citizens', 'legitimate rotating spit', 'outright d-bagly points', 'pseudo social activism', 'qdoba fails miserably', 'swirled marbled rye', 'uber-douchy clientele explains', 'joes bbq/joes farm', 'named philip chiang', 'pickled/preserved veggies/meat/fruits etc', 'unreadable chalk board', 'seriously pissed-off every-time', 'perceived cost imbalance', 'eclectic drawing inspiration', 'certainly strengthens ot', 'busboy randomly swipe', 'dear communist neighbors', 'guru dr. andrew', 'lime daiquiri masquerading', 'pussy john leguizamo', 'vine ripened heirloom', 'costa rican mahi', 'lenôtre trained almond', 'unseasoned burnt 80/20', 'well-known ethnic groceries', '10/12 year anejo', 'card holders waltzed', 'freshman year undergrad', 'stephen g. hit', 'brownie upside downy', 'insert lesbian joke', 'oreganos fav jr', 'recite reservoir dogs', 'brussels sprout sauerkraut', 'quintessential gastronomic delight', 'asu instructor david', 'festive décor charmed', 'mr. drifty ii', 'mr. drifty iii', 'non-party-bro asu population', 'serious healing properties', 'speciality texan jalapeno', 'tasteful western inflections', 'frat-party gimmicks throughout', 'auditioned vintage 95', 'butter-marinated brussels sprouts', 'chrissy p invited', 'creating rootbeer floats', 'du rhone caught', 'la piccola cucina', 'pediatric boards yesterday', 'piazza al forno', 'roller skates tho', 'secluded rooftop @', '1202 mohave street', 'required 130 degrees', 'nasty chemical aftertaste', 'cheese-red sauce-and-pepperoni type', 'dried smeared feces', 'official ribbon cutting', 'yummers blondie cookie', 'bon vivant fashion', 'captain crunch cereal', 'choux easily withstanding', 'expect cheese-red sauce-and-pepperoni', 'forever cherish thee', 'former tenant salon', 'grad school graduation', 'gusto es mucho', 'hole-in-the-wall family-run operations', 'menu- lobster potpie', 'mucho gusto es', 'quart-sized mason jar', 'televised sporting events', 'van gogh alive', 'edison light bulbs', 'mixing softdrink machine', 'shoveled coal onto', 'somewhat reluctant meat-eating', 'terra cotta colored', 'aviation pictures printed', 'bi cha gio', 'nuong cha gio', 'strawberry shortcake whoopie', 'thomas james spins', 'approximately 15,000 calories', 'four peaks/papago/sun up/san', 'iphone e-mail responses', 'latest endeavor speaks', 'schnitzel cordon bleu', 'sideways baseball hats', 'urban dwellers alike', 'itty bitty end', 'aji massage flew', 'dividers thoughtfully placed', 'fru fru pineapple', 'kimmel hired chris', 'newcastle sits untouched', 'particularly high-pitched repeated', '22.53 + 4.00', 'gulab jamun ball', 'riesling soaked pears', 'seven-figure foodie fantasy', 'tissue paper plug', 'vaguely insulting ethnic', 'padded banko seating', 'conspire art collective', '.the tuscan colors', 'bro daren c', 'kiss bison witches', 'skepticism largely assuming', 'mims serves legitimately', 'owner alain keller', 'owner craig demarco', 'tyrranical owner slinging', 'au-gratin brussel sprouts', 'bravo thirsty lion', 'celebrated lead actor', 'drifty iii appears', 'ipad driven pos', 'microscopic clam bits', 'wed .40 cent', 'zupas officially opens', 'bred midwestern girl', 'and/or erik t.', 'del giorno featuring', 'stanley cup finals', 'sweaty airless mess', 'grey goose gimlets', 'proper food-safe temperature', '[ chorus x2', 'phnom penh noodle', 'beppo throws junk', 'doug robson set', 'incredibly hateful.so hateful', 'mexicali flavour houses', 'snicker doodle cookies', 'weigh 1000 lbs', 'buca di bepo-ish', 'current economic crisis', 'king carlos iv', 'man-held computer system', 'tom ka gai', '1 singing/shrieking girl/guy', 'cheddar-topped patty crumbled', 'chorus x2 ]', 'featured aguas frescas', 'grilled cheese/jewish sliders/patty', 'pacing goes-i hope', 'tood darned dark', 'audrey w reportedly', 'island statue facial', 'mia ........ j/k', \"'when harry met\", 'highlighting local in-season', 'july chargrilled hotdogs', 'local journalists likened', 'local policia extorting', 'pholks lisa g', 'frequent bingsoo buyers', 'handiwork shines throughout', \"'no stealing kimchi\", 'high-priced eateries b/c', 'itemized receipt attached', 'lesbian cousin rita', 'lodge stimulus package', 'textured phyllo dough', 'tommy bahama clothing', 'possessive apostrophe several', 'busily hand-mixed tea', 'stainless steel industrial', 'compare forearm tats', 'poor mans rehab/wet', 'blueberry hill girdle', 'old-fashioned southern breakfasts', 'sinigang na baboy', 'soapy hands goddamit', 'anticipation level spiked', 'aqua e sale', 'clamored five-star rating', 'clearly illegal behinds', 'dunlap corporate freeway', 'sous vide duck', 'cars parked afront', 'known accomplished cellist', 'lame charlie sheen', 'puu puu platter', 're-heat one-stop shop', 'pablo picasso mexico', 'gorgeous jw marriott', 'medlock historic district', 'otb results screens', 'sat willie mays', 'serbian lepinja roll', 'gray skies face', 'high powered/laser sighted', 'buca di beppo', 'whole organic/green mocvement', '2005 napa cab', 'balsalmic vinigarette dressing', 'coffehouse style old-looking', 'down-home country cookin', 'il fornaio entered', 'tammie co pastries', 'vault-of-the-heavens style crenelations', 'tai bo vien', 'better-tasting distinctive touch', 'free wi fi', 'joined @ smunchphoenix', 'skills merely resulted', 'terra verde farms', 'viva la revolucion', 'landscaping trucks parked', 'mothers/ fathers days', 'public transit arrives', 'regretting life afterwords', 'restored ....... praise', 'axis n radius', 'field sobriety test', 'metallic p. anyway', '11:00 till 3:00', 'denver ski lodge', 'read mothers/ fathers', 'addictive fett alfredo', 'clear panel refrigerators', 'tex mex spin', 'semi-health conscious diner', 'coffee shop/creperie/hole-in-the-wall bundle', 'dark-haired shirtless man', 'jose garces personality', 'marshmallow creme flambeed', 'wearing ed hardy', 'north african chili-based', 'noticable insect problem', 'sewer vent problem', 'coconut cloud confection', 'embedded grated coconut', 'newer loves nee', 'decor evoking shameless', 'barrio queen aspires', 'fire-roasted tomoto salsa', 'open mon-sat 7am-9pm', 'granny smith apple', 'nantucket nectar apple', '1. moscow mule', 'mayo clinic appointment', 'approached the-dimly lit', 'sommelier named alessandro', 'three unanswered e-mails', 'bon apetite rate', 'creeks = yummmm', 'cuon thit tom', 'focaccia del giorno', 'thou shalt provide', 'pudgy married-guy look', 'bannana pecan caramel', 'outer parts nearing', 'recurring event function', 'hilton squaw peak', 'adam richman devoured', 'backlit colorful liquor', 'brother ali vow', 'cracked-out junkie brother', 'killer moscow mule', 'met sophia lauren', 'mustart / tomatillo', 'sierra nevada mustard', 'patronize farmers markets', 'spotted donkey cantina', \"typical 'when harry\", 'wasabi crème fraîche', 'az stronghold mangus', 'cedar planked salmon', 'instructor david young', 'kid freindly atmoshpere', 'mileage apparently differed', 'guava juice chaser', \"divine 'tator wedges\", 'yummy sauce/ flavor/', 'early 90s alt', 'jimmy buffett paraphernalia', 'princess wedding dreams', 'drunken animals munchies', 'fruity adult beverages/margaritas', 'inedible science experiment', 'overwhelming- cool cave-like', 'applebees meets outback', 'guera blonde ale', 'hang-over recovery joint', 'purse hooks underneath', 'speedy freaking gonzales', 'tango eggees cup', 'weight watchers dream', 'shitake mushroom rames', 'wild boar pappardelle', '6-8 amuse bouche', 'charlie sheen pop', 'corn-starchy orange extract', 'dr. andrew weil', 'hip 20-30 somethings', 'lasagna della casa', 'sexy speakeasy .the', 'soup-nazi routine set', 'swiss chard medley', 'camarones al ajillo', 'forno cotto mozzarella', 'sausage samich w/fries', 'reluctant meat-eating boyfriend', 'vintage clothing/ furniture', 'bingsoo buyers card', 'dark haired brunets', 'fest goggles head', 'digging women seeking', 'found ippei happhazardly', 'exudes southern hospitality', 'granular texture equivalent', 'harry met sally', 'catcher mitt dessert', 'w/ embedded grated', 'corporately consistent signage', 'remained pervasive throughout', 'televisions throughout broadcasting', '12 pasteles uncooked', 'feng shui touch', 'upscale 8th arrondissement', 'usually ella fitzgerald', 'check ratpack ghosts', '7th street-coronado district', 'ama ebi nigiri', 'de cassis liqueur', 'discount coupons differ', 'handy google mobile', 'mangoes coz personally', 'mutton hominy stew', 'salon de venus', 'spiedini al forno', 'swirled strawberry/mango margarita', 'tenant salon de', 'zesty cabbage-carrot-cilanto slaw', 'beefy goodness oozed']\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "#Cargamos las métricas para el cálculo de bigramas y trigramas:\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# Cálculo de los bigramas y trigramas con la métrica PMI\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "finder.apply_word_filter(lambda w: w.lower() in stopwords)  # Filtrar stopwords\n",
    "finder.apply_ngram_filter(lambda w1, w2: w1.lower() in stopwords or w2.lower() in stopwords)  # Filtrar bigramas con stopwords\n",
    "bigrams = finder.nbest(bigram_measures.pmi, 1000)\n",
    "\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_words(tokens)\n",
    "finder.apply_word_filter(lambda w: w.lower() in stopwords)  # Filtrar stopwords\n",
    "finder.apply_ngram_filter(lambda w1, w2, w3: w1.lower() in stopwords or w3.lower() in stopwords)  # Filtrar trigramas con stopwords\n",
    "trigrams = finder.nbest(trigram_measures.pmi, 1000)\n",
    "\n",
    "# Filtrar bigramas y trigramas que comienzan o terminan con stopwords\n",
    "filtered_bigrams = [f\"{w1} {w2}\" for (w1, w2) in bigrams if not w1.lower() in stopwords and not w2.lower() in stopwords]\n",
    "filtered_trigrams = [f\"{w1} {w2} {w3}\" for (w1, w2, w3) in trigrams if not w1.lower() in stopwords and not w3.lower() in stopwords]\n",
    "\n",
    "# Imprimir los mejores bigramas y trigramas en formato de lista\n",
    "print(\"BEST BIGRAMS:\")\n",
    "print(str(filtered_bigrams[:1000]))\n",
    "\n",
    "print(\"\\n\\nBEST TRIGRAMS:\")\n",
    "print(str(filtered_trigrams[:1000]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3883d1ca",
   "metadata": {
    "id": "3883d1ca"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong>  Detectar ngramas que cumplen el patrón sintáctico de un sintagma nominal (e.g: adjetivo + nombre en singular/plural y nombre + nombre) (0.5 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50c96d2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50c96d2e",
    "outputId": "eb3364ca-3cbd-4e5c-cee0-13740827e5c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wife', 'birthday', 'breakfast', 'weather', 'grounds', 'absolute pleasure', 'waitress', 'food', 'semi-busy saturday morning', 'place', 'favor', 'bloody mary', 'i', 'pretty sure', 'ingredients', 'garden', 'order', 'everything', 'menu', 'i', 'white truffle', 'eggs', 'vegetable skillet', 'pieces', 'bread', 'toast', 'i', 'i', 'i', 'idea', 'people', 'bad reviews', 'place', 'everyone', 'something', 'own fault', 'many people', 'case', 'friend', 'i', 'pm', 'past sunday', 'i', 'sunday evening', 'thought', 'seat', 'girl', 'someone', 'waiter', 'drink orders', 'everyone', 'host', 'waiter', 'server', 'prices', 'orders', 'baked spaghetti calzone', 'beef', 'pizza', 'calzone', 'one', 'pizza', 'friend', 'pizza', 'calzone', 'calzone', 'sweetish sauce', 'sauce', 'part', 'pizza', 'door', 'everything', 'bad reviewers', 'things', 'bad reviewers', 'serious issues', 'gyro plate', 'rice', 'i', 'candy selection', 'quiessence', 'beautiful', 'full windows', 'earthy wooden walls', 'feeling', 'warmth', 'restaurant', 'middle', 'farm', 'restaurant', 'tuesday evening', 'reservations', 'couple days', 'friend', 'i', 'sandwiches', 'farm kitchen', 'week', 'restaurant', 'crisp']\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Definir la expresión regular que busca patrones de sintagma nominal\n",
    "noun_pattern = r\"\"\"\n",
    "    NP: {<JJ>*<NN.*>+}\n",
    "        {<NN.*>+<NN.*>+}\n",
    "\"\"\"\n",
    "\n",
    "# Crear un analizador de sintaxis que use la expresión regular definida anteriormente\n",
    "chunk_parser = nltk.RegexpParser(noun_pattern)\n",
    "\n",
    "# Analizar la lista de etiquetas de POS y extraer los sintagmas nominales\n",
    "\n",
    "tree = chunk_parser.parse(pos_tags)\n",
    "result = []\n",
    "for subtree in tree.subtrees():\n",
    "    if subtree.label() == 'NP':\n",
    "        # Convertir el subárbol de sintagma nominal en una cadena de texto y agregarlo a la lista de resultados\n",
    "        noun_phrase = \" \".join(word for word, pos in subtree.leaves())\n",
    "        result.append(noun_phrase)\n",
    "\n",
    "print(result[:100]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c0a10",
   "metadata": {
    "id": "535c0a10"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Detectar colocaciones con un modelo de detección de frases, con el módulo Phraser de Gensim. Entrena el modelo con todas las opiniones (0,5 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7edb87",
   "metadata": {
    "id": "3a7edb87"
   },
   "source": [
    "<i>Primer paso</i>: Convertir las opiniones en una lista de phrases. Las phrases no deben ser stopwords. Tampoco deben empezar ni acabar con una stopword. (0.5 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3eba6bcc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eba6bcc",
    "outputId": "9c939aa8-9501-4368-905e-1818776a4c7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My wife took me here on my birthday for breakfast and it was excellent',\n",
       " 'The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure',\n",
       " 'Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning',\n",
       " 'It looked like the place fills up pretty quickly so the earlier you get here the better',\n",
       " 'Do yourself a favor and get their Bloody Mary',\n",
       " \"It was phenomenal and simply the best I've ever had\",\n",
       " \"I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it\",\n",
       " 'It was amazing',\n",
       " 'While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious',\n",
       " 'It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinions_string = \" \".join(df['text'])\n",
    "\n",
    "opinion_sentences = opinions_string.split('. ')\n",
    "\n",
    "opinion_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97f2bd0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97f2bd0b",
    "outputId": "e1ff1c0a-530e-4a71-c8f5-2d65632a8f98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wife took birthday breakfast excellent\n",
      "weather perfect made sitting outside overlooking grounds absolute pleasure\n",
      "waitress excellent food arrived quickly semi-busy saturday morning\n",
      "looked like place fills pretty quickly earlier get better\n",
      "favor get bloody mary\n",
      "phenomenal simply best i've ever\n",
      "i'm pretty sure use ingredients garden blend fresh order\n",
      "everything menu looks excellent, white truffle scrambled eggs vegetable skillet tasty delicious\n",
      "came 2 pieces griddled bread amazing absolutely made meal complete\n",
      "best \"toast\" i've ever\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                 #\n",
    "#############################################\n",
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# Preprocesar las oraciones eliminando stopwords y aplicando el modelo de detección de frases\n",
    "preprocessed_sentences = []\n",
    "for sentence in opinion_sentences:\n",
    "    words = sentence.split()\n",
    "    words = [word.lower() for word in words if word.lower() not in stopwords]\n",
    "    if len(words) > 1:\n",
    "        if words[0] not in stopwords and words[-1] not in stopwords:\n",
    "            preprocessed_sentences.append(words)\n",
    "            \n",
    "\n",
    "# Entrenar el modelo de detección de frases utilizando las oraciones preprocesadas\n",
    "phrases = Phrases(preprocessed_sentences, min_count=5, threshold=10)\n",
    "phraser = Phraser(phrases)\n",
    "\n",
    "# Aplicar el modelo de detección de frases a las oraciones para obtener las colocaciones detectadas\n",
    "collocations = phraser[preprocessed_sentences]\n",
    "\n",
    "# Mostrar las primeras 10 colocaciones detectadas por pantalla\n",
    "for i in range(10):\n",
    "    print(\" \".join(preprocessed_sentences[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e13a71",
   "metadata": {
    "id": "d4e13a71"
   },
   "source": [
    "## 1.2 Vectorizar palabras y términos (2 puntos)\n",
    "\n",
    "Exploraremos la vectorización de palabras y términos con el método Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391acd67",
   "metadata": {
    "id": "391acd67"
   },
   "source": [
    "Recordemos que el paquete gensim implementa un método para entrenar modelos Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c3a3166",
   "metadata": {
    "id": "9c3a3166"
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4OGwpBv8fOtp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4OGwpBv8fOtp",
    "outputId": "28c2a902-4a7d-46c8-8309-4021ed74338d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wife',\n",
       " 'took',\n",
       " 'birthday',\n",
       " 'breakfast',\n",
       " 'excellent',\n",
       " '.',\n",
       " 'weather',\n",
       " 'perfect',\n",
       " 'made',\n",
       " 'sitting']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quitar espacios del texto:\n",
    "opinion_phrases_no_stopwords =  [palabra for palabra in tokens if palabra.lower() not in stopwords]\n",
    "opinion_phrases_stripped_no_stopwords = [c.strip() for c in opinion_phrases_no_stopwords]\n",
    "opinion_phrases_stripped_no_stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aaa498",
   "metadata": {
    "id": "20aaa498"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "Ejercicio:</strong> Obtener targets de las opiniones y sus aspectos utilizando el modelo word2vec (2 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d68931",
   "metadata": {
    "id": "a9d68931"
   },
   "source": [
    "<i>Primer paso</i>: Convertir las phrases de cada oración en un token. Lo haremos concatenando los tokens de la phrase con el caracter '_' (e.g: 'scrambled eggs' -> 'scrambled_eggs'). Entonces, en cada oración sustituimos los bigramas que son phrases por la forma tokenizada (e.g: I made scrambled eggs -> I made scrambled_eggs). De esta forma, las colocaciones formarán parte del vocabulario del modelo word2vec que generaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dee08237",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dee08237",
    "outputId": "a0b73667-e6f2-4bb9-8da6-f6c62ac375e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My wife took me here on my birthday for breakfast and it was excellent',\n",
       " 'The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure',\n",
       " 'Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning',\n",
       " 'It looked like the place fills up pretty quickly so the earlier you get here the better',\n",
       " 'Do yourself a favor and get their Bloody Mary',\n",
       " \"It was phenomenal and simply the best I've ever had\",\n",
       " \"I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it\",\n",
       " 'It was amazing',\n",
       " 'While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious',\n",
       " 'It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "collocation_phrases = [phrase for phrase in list(set(opinion_phrases_stripped_no_stopwords)) if ' ' in phrase]\n",
    "\n",
    "def transform_sentence(sentence):\n",
    "    transformed_sentence = sentence\n",
    "    n_grams = list(ngrams(nltk.word_tokenize(sentence), 2))\n",
    "    ngrams_t = [' '.join(gram) for gram in n_grams]\n",
    "    for ngram in ngrams_t:\n",
    "        if ngram in collocation_phrases:\n",
    "            opt = ngram.replace(' ', '_')\n",
    "            transformed_sentence = transformed_sentence.replace(ngram,opt)\n",
    "    return transformed_sentence\n",
    "\n",
    "opinion_sentences_transformed = [transform_sentence(os) for os in opinion_sentences]\n",
    "\n",
    "opinion_sentences_transformed[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53930c4",
   "metadata": {
    "id": "b53930c4"
   },
   "source": [
    "<i>Segundo paso</i>: crear una sentence stream donde todos los tokens de las oraciones están lematizados. Los tokens no pueden ser stopwords ni tener un stopword al inicio o al final. Para simplificar la tarea, consideramos que el lema de una colocación no cambia y su PoS es 'col'. (e.g: ['The guests like scrambled eggs', 'The rooms were dirty'] -> [['the', 'guest', 'like', 'scrambled_eggs], ['the', 'room', 'be ', 'dirty']]). (1 punto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cfcef92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9cfcef92",
    "outputId": "8a95c70d-0bd6-4368-e8c4-dd61ffe8bd45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wife', 'took', 'birthday', 'breakfast', 'excellent']\n",
      "['weather', 'perfect', 'made', 'sitting', 'outside', 'overlooking', 'ground', 'absolute', 'pleasure']\n",
      "['waitress', 'excellent', 'food', 'arrived', 'quickly', 'Saturday', 'morning']\n",
      "['looked', 'like', 'place', 'fill', 'pretty', 'quickly', 'earlier', 'get', 'better']\n",
      "['favor', 'get', 'Bloody', 'Mary']\n",
      "['phenomenal', 'simply', 'best', 'ever']\n",
      "['pretty', 'sure', 'use', 'ingredient', 'garden', 'blend', 'fresh', 'order']\n",
      "['amazing']\n",
      "['EVERYTHING', 'menu', 'look', 'excellent', 'white', 'truffle', 'scrambled', 'egg', 'vegetable', 'skillet', 'tasty', 'delicious']\n",
      "['came', 'piece', 'griddled', 'bread', 'amazing', 'absolutely', 'made', 'meal', 'complete']\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Crear objeto para lematizar tokens\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def transform_sentence(sentence):\n",
    "    transformed_tokens = []\n",
    "    for token in word_tokenize(sentence):\n",
    "        if token.lower() not in stopwords and token.isalpha() and len(token) > 1:\n",
    "            # apply lemmatization\n",
    "            transformed_token = lemmatizer.lemmatize(token)\n",
    "            transformed_tokens.append(transformed_token)\n",
    "    return transformed_tokens\n",
    "\n",
    "def transform_sentences(sentences):\n",
    "    transformed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        transformed_sentence = transform_sentence(sentence)\n",
    "        transformed_sentences.append(transformed_sentence)\n",
    "    return transformed_sentences\n",
    "\n",
    "transformed_sentences = transform_sentences(opinion_sentences_transformed)\n",
    "\n",
    "for i in range(10):\n",
    "    print(transformed_sentences[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b807880",
   "metadata": {
    "id": "7b807880"
   },
   "source": [
    "<i>Tercer paso</i>: Crear un modelo word2vec de las opiniones lematizadas. El modelo debe llamarse w2v_opinions (0.5 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b033b7ad",
   "metadata": {
    "id": "b033b7ad"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓ                                   #\n",
    "#############################################\n",
    "\n",
    "# Creacion Modelo\n",
    "w2v_opinions  =  gensim.models.Word2Vec(transformed_sentences, vector_size=100,\n",
    "                 window=5, min_count=1, workers=4)\n",
    "# guardar modelo\n",
    "w2v_opinions.save('w2v_opinions.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e07b3c",
   "metadata": {
    "id": "00e07b3c"
   },
   "source": [
    "<i>Cuarto paso</i>: A partir del vocabulario del modelo word2vec, selecciona posibles aspectos de la opinión (e.g: food) y lista términos semánticamente relacionados con estos aspectos según este modelo. (0.5 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc470830",
   "metadata": {
    "id": "cc470830"
   },
   "source": [
    "Obtener el vocabulario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d10af9ee",
   "metadata": {
    "id": "d10af9ee"
   },
   "outputs": [],
   "source": [
    "vocabulary = list(w2v_opinions.wv.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "568d1445",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "568d1445",
    "outputId": "cb7c3b6b-f658-4cdd-c3a1-b87f35d855ac"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                   #\n",
    "#############################################\n",
    "\n",
    "# Seleccionamos posibles aspectos\n",
    "aspect_food = 'food'\n",
    "aspect_clean = 'clean'\n",
    "aspect_service = 'service'\n",
    "aspect_price = 'price'\n",
    "\n",
    "# Obtener términos semánticamente relacionados con los aspectos\n",
    "related_terms_food = w2v_opinions.wv.most_similar(aspect_food, topn=10)\n",
    "related_terms_clean = w2v_opinions.wv.most_similar(aspect_clean, topn=10)\n",
    "related_terms_service = w2v_opinions.wv.most_similar(aspect_service, topn=10)\n",
    "related_terms_price = w2v_opinions.wv.most_similar(aspect_price, topn=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "764f6cdb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "764f6cdb",
    "outputId": "45809f36-53ab-433d-8965-a854b953f0c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Términos relacionados con 'food':\n",
      "service (0.95)\n",
      "Service (0.94)\n",
      "consistently (0.94)\n",
      "Food (0.93)\n",
      "ambiance (0.93)\n",
      "Always (0.92)\n",
      "atmosphere (0.92)\n",
      "Overall (0.92)\n",
      "always (0.92)\n",
      "pretty (0.92)\n",
      "\n",
      "Términos relacionados con 'clean':\n",
      "competent (0.99)\n",
      "polite (0.98)\n",
      "welcoming (0.98)\n",
      "courteous (0.98)\n",
      "helpful (0.98)\n",
      "prompt (0.98)\n",
      "efficient (0.98)\n",
      "comfortable (0.98)\n",
      "smiling (0.98)\n",
      "pleasant (0.98)\n",
      "\n",
      "Términos relacionados con 'service':\n",
      "Service (0.97)\n",
      "atmosphere (0.96)\n",
      "food (0.95)\n",
      "ambiance (0.94)\n",
      "consistently (0.94)\n",
      "always (0.93)\n",
      "efficient (0.93)\n",
      "staff (0.92)\n",
      "fast (0.92)\n",
      "quick (0.92)\n",
      "\n",
      "Términos relacionados con 'price':\n",
      "reasonable (0.97)\n",
      "Good (0.94)\n",
      "decent (0.94)\n",
      "reasonably (0.93)\n",
      "pretty (0.93)\n",
      "priced (0.93)\n",
      "cheap (0.93)\n",
      "selection (0.93)\n",
      "value (0.93)\n",
      "Great (0.93)\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                   #\n",
    "#############################################\n",
    "\n",
    "#Imprimimos ejemplos de aspectos cercanos semánticamente al target según el modelo Word2Vec que el alumno puede \n",
    "#encontrar en el dataframe anterior. Entre estos ejemplos está, por ejemplo, 'atmosphere' que tiene un valor de\n",
    "#similitud semántica alto. El ejercicio consiste en que el alumno elija los ejemplos que considere pertinentes.\n",
    "\n",
    "# Imprimir términos relacionados\n",
    "print(f\"\\nTérminos relacionados con '{aspect_food}':\")\n",
    "for term, similarity in related_terms_food:\n",
    "    print(f\"{term} ({similarity:.2f})\")\n",
    "    \n",
    "# Imprimir términos relacionados\n",
    "print(f\"\\nTérminos relacionados con '{aspect_clean}':\")\n",
    "for term, similarity in related_terms_clean:\n",
    "    print(f\"{term} ({similarity:.2f})\")\n",
    "    \n",
    "# Imprimir términos relacionados\n",
    "print(f\"\\nTérminos relacionados con '{aspect_service}':\")\n",
    "for term, similarity in related_terms_service:\n",
    "    print(f\"{term} ({similarity:.2f})\")\n",
    "    \n",
    "# Imprimir términos relacionados\n",
    "print(f\"\\nTérminos relacionados con '{aspect_price}':\")\n",
    "for term, similarity in related_terms_price:\n",
    "    print(f\"{term} ({similarity:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80090d4b",
   "metadata": {
    "id": "80090d4b"
   },
   "source": [
    "# 2. Detección de temas (4 puntos)\n",
    "\n",
    "En estos apartados exploraremos cúales son los tópicos tratados en las opiniones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb679a",
   "metadata": {
    "id": "00bb679a"
   },
   "source": [
    "## 2.1 Exploración de los temas con WordNet (2 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7a2020",
   "metadata": {
    "id": "9e7a2020"
   },
   "source": [
    "En este apartado accederemos a Wordnet a través de la librería nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "320fee0c",
   "metadata": {
    "id": "320fee0c"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd9034",
   "metadata": {
    "id": "7ccd9034"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Comprueba si, según Wordnet, existen aspectos que están alejados semánticamente del sentido del target, aunque en el modelo word2vec sean similares. Compruébalo calculando la similitud de Wu and Palmer entre el sentido de wordnet 'restaurant.n.01' y algunos de sus aspectos. (1 punto)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "30cfcb0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30cfcb0f",
    "outputId": "edb9f0a4-45f5-4e27-f11b-64c656af9677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud WUP entre restaurant.n.01 y food.n.01: 0.1\n",
      "Similitud WUP entre restaurant.n.01 y clean_and_jerk.n.01: 0.05\n",
      "Similitud WUP entre restaurant.n.01 y service.n.01: 0.06666666666666667\n",
      "Similitud WUP entre restaurant.n.01 y monetary_value.n.01: 0.07142857142857142\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                   #\n",
    "#############################################\n",
    "\n",
    "# Definir el synset de destino para restaurant.n.01\n",
    "target_synset = wn.synset('restaurant.n.01')\n",
    "\n",
    "# Definir los synsets de aspecto\n",
    "aspect_synsets = [\n",
    "    wn.synset('food.n.01'),\n",
    "    wn.synset('clean.n.01'),\n",
    "    wn.synset('service.n.01'),\n",
    "    wn.synset('price.n.01')\n",
    "]\n",
    "\n",
    "# Calcule la similitud de Wu-Palmer entre el objetivo y cada synset de aspecto\n",
    "for synset in aspect_synsets:\n",
    "    similarity = target_synset.path_similarity(synset)\n",
    "    print(f'Similitud WUP entre {target_synset.name()} y {synset.name()}: {similarity}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3209e5cc",
   "metadata": {
    "id": "3209e5cc"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Lista los términos monopalabra del vocabulario de word2vec que no están en Wordnet. Los términos deben ser nombres o adjetivos. ¿Creeis que estos términos son importantes para poder realizar un buen análisis de sentimientos? (1 punto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "70e8e4bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70e8e4bd",
    "outputId": "4c566845-82d9-457e-878c-eafb80b60a49",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros 10 términos monopalabras no presentes en WordNet: ['listed', 'lopsided', 'borrowed', 'Highly', 'replacement', 'proposition', 'Oddity', 'Europa', 'strongbow', 'invisible']\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Cargue el modelo word2vec\n",
    "model = gensim.models.Word2Vec.load('w2v_opinions.model')\n",
    "\n",
    "# Obtener el vocabulario del modelo.\n",
    "vocab = set(model.wv.index_to_key)\n",
    "\n",
    "# Obtenga un conjunto de sustantivos y adjetivos de WordNet\n",
    "wordnet_nouns = set(list(wn.all_synsets('n')))\n",
    "wordnet_adjectives = set(list(wn.all_synsets('a')))\n",
    "wordnet_nouns_and_adjectives = wordnet_nouns.union(wordnet_adjectives)\n",
    "\n",
    "#Encuentra términos de monopalabras en el vocabulario del modelo que no están en WordNet\n",
    "missing_terms = []\n",
    "for term in vocab:\n",
    "    if (len(term.split('_')) == 1) and (term not in wordnet_nouns_and_adjectives):\n",
    "        missing_terms.append(term)\n",
    "\n",
    "# Imprime los términos que faltan\n",
    "print(f\"Primeros 10 términos monopalabras no presentes en WordNet: {missing_terms[:10]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f788a4",
   "metadata": {
    "id": "f5f788a4"
   },
   "source": [
    "## 2.2 LDA (2 puntos)\n",
    "\n",
    "Recordad que en el notebook del módulo 1 hemos visto la aplicación del método LDA para extraer temas de documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6825ff",
   "metadata": {
    "id": "1c6825ff"
   },
   "source": [
    "<i>Primer paso</i>: Convertir las opiniones transformadas (opinion_sentences_transformed) en listas de nombres y colocaciones. Esto es necesario ya que los nombres y las colocaciones expresan los temas de las opiniones (e.g: [['wife have breakfast sitting_outside'], [' 'waitress', 'served', 'ceviche']...] -> [['wife', 'breakfast', 'sitting_outside'], ['waitress', 'ceviche']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "76ad1379",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76ad1379",
    "outputId": "ed92b22a-f32b-416d-f783-58539ad3a8d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wife', 'birthday', 'breakfast'],\n",
       " ['weather', 'grounds', 'pleasure'],\n",
       " ['waitress', 'food', 'morning'],\n",
       " ['place'],\n",
       " ['favor']]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_noun_and_collocation(sentence):\n",
    "    nouns_and_collocations = []\n",
    "    noun_tags = ['NN', 'NNS']\n",
    "    tokens_pos_tagged = pos_tag(word_tokenize(sentence))\n",
    "    for tpos in tokens_pos_tagged:\n",
    "        if '_' in tpos[0]:\n",
    "            nouns_and_collocations.append(tpos[0])\n",
    "        elif tpos[1] in noun_tags:\n",
    "            nouns_and_collocations.append(tpos[0])\n",
    "    return nouns_and_collocations\n",
    "            \n",
    "noun_and_collocation_stream = [get_noun_and_collocation(opinion) for opinion in opinion_sentences_transformed]\n",
    "\n",
    "noun_and_collocation_stream[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef21d0c",
   "metadata": {
    "id": "eef21d0c"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Extrae temas a partir de las listas de nombres y colocaciones de cada oración transformada. Experimenta con el parámetro num_topics hasta encontrar un conjunto de temas informativos, asignando nombres a los temas encontrados. (2 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "71e72f43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3284e474",
    "outputId": "b4dda9d2-acf2-4d04-9bc3-2d598f261dfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de LDA con 5 temas:\n",
      "Tema 0: ['night', 'bar', 'pizza', 'hour', 'table', 'places', 'drink', 'way', 'beer', 'years']\n",
      "Tema 1: ['food', 'place', 'service', 'lunch', 'burger', 'location', 'prices', 'restaurant', 'quality', 'lot']\n",
      "Tema 2: ['order', 'sauce', 'chicken', 'cheese', 'side', 'salad', 'rice', 'plate', 'flavor', 'beef']\n",
      "Tema 3: ['time', 'experience', 'people', 'drinks', 'staff', 'things', 'wine', 'family', 'home', 'reviews']\n",
      "Tema 4: ['menu', 'service', '..', 'restaurant', 'meal', 'times', 'i', 'bread', 'anything', 'stars']\n",
      "=============================================================================================================================\n",
      "Modelo de LDA con 10 temas:\n",
      "Tema 0: ['chicken', 'burger', 'sauce', 'fries', 'meat', 'lot', 'bread', 'beef', 'nothing', 'steak']\n",
      "Tema 1: ['restaurant', 'night', 'dinner', 'experience', 'i', 'course', 'reviews', 'house', 'wife', 'room']\n",
      "Tema 2: ['food', 'time', 'people', 'location', 'minutes', 'wait', 'family', 'order', 'couple', 'door']\n",
      "Tema 3: ['staff', 'bar', 'hour', 'drinks', 'table', 'bit', 'side', 'items', 'dishes', 'rice']\n",
      "Tema 4: ['order', 'times', 'something', 'place', 'anything', 'drink', 'friends', 'friend', 'decor', 'dessert']\n",
      "Tema 5: ['place', 'menu', 'area', 'thing', 'way', 'sushi', 'places', 'breakfast', 'things', 'quality']\n",
      "Tema 6: ['place', 'everything', 'dish', 'tacos', 'flavor', 'patio', 'pork', 'shrimp', 'rolls', 'coffee']\n",
      "Tema 7: ['pizza', 'salad', 'cheese', 'sandwich', 'sauce', 'chips', 'meal', 'bacon', 'chicken', 'part']\n",
      "Tema 8: ['service', '..', 'food', 'server', 'atmosphere', 'wine', 'beer', 'restaurants', 'selection', 'waiter']\n",
      "Tema 9: ['lunch', 'food', 'prices', 'home', 'taste', 'husband', 'portions', 'choice', 'sandwiches', 'style']\n",
      "=============================================================================================================================\n",
      "Modelo de LDA con 15 temas:\n",
      "Tema 0: ['menu', 'staff', 'meal', 'bit', 'location', 'price', 'side', 'dish', 'meat', 'beer']\n",
      "Tema 1: ['..', 'night', 'something', 'husband', 'name', 'appetizer', 'tea', 'fun', 'meals', 'crispy']\n",
      "Tema 2: ['cheese', 'cream', 'shrimp', 'bacon', 'waitress', 'sauce', 'plates', 'oil', 'tomato', 'counter']\n",
      "Tema 3: ['restaurant', 'way', 'anything', 'stars', 'day', 'taste', 'options', 'soup', 'bowl', 'amount']\n",
      "Tema 4: ['lunch', 'salad', 'hour', 'sauce', 'tacos', 'wait', 'course', 'spot', 'choice', 'kitchen']\n",
      "Tema 5: ['fries', 'bread', 'decor', 'fan', 'sandwiches', 'coffee', 'potato', 'trip', 'fish', 'onions']\n",
      "Tema 6: ['places', 'home', 'dessert', 'music', 'someone', 'burgers', 'dog', 'problem', 'others', 'line']\n",
      "Tema 7: ['thing', 'flavor', 'burger', 'beef', 'rice', 'rolls', 'beans', 'fact', 'style', 'Everything']\n",
      "Tema 8: ['drinks', 'minutes', 'spicy', 'chips', 'roll', 'steak', 'couple', 'salsa', 'appetizers', 'guy']\n",
      "Tema 9: ['service', 'bar', 'server', 'area', 'wine', 'sandwich', 'friends', 'things', 'town', 'part']\n",
      "Tema 10: ['food', 'pizza', 'service', 'i', 'prices', 'quality', 'place', 'kind', 'selection', 'review']\n",
      "Tema 11: ['place', 'time', 'table', 'years', 'dishes', 'friend', 'patio', 'family', 'reviews', 'water']\n",
      "Tema 12: ['chicken', 'plate', 'drink', 'nothing', 'one', 'star', 'bite', 'half', 'bartender', 'choices']\n",
      "Tema 13: ['dinner', 'order', 'times', 'experience', 'breakfast', 'restaurants', 'house', 'everyone', 'waiter', 'visit']\n",
      "Tema 14: ['people', 'sushi', 'everything', 'atmosphere', 'pork', 'lot', 'wife', 'door', 'party', 'tender']\n",
      "=============================================================================================================================\n",
      "Modelo de LDA con 20 temas:\n",
      "Tema 0: ['staff', 'places', 'breakfast', 'house', 'everyone', 'coffee', 'bowl', 'stuff', 'burgers', 'customer']\n",
      "Tema 1: ['server', 'drink', 'tables', 'Everything', 'size', 'ingredients', 'plates', 'man', 'rest', 'seating']\n",
      "Tema 2: ['something', 'anything', 'beer', 'steak', 'specials', 'kitchen', 'group', 'evening', 'type', 'crowd']\n",
      "Tema 3: ['table', 'area', 'atmosphere', 'tacos', 'spicy', 'soup', 'shrimp', 'part', 'problem', 'days']\n",
      "Tema 4: ['menu', 'people', 'friend', 'bread', 'items', 'reviews', 'options', 'appetizer', 'portion', 'meals']\n",
      "Tema 5: ['sauce', 'side', 'hour', 'beef', 'restaurants', 'kind', 'dog', 'world', 'oil', 'wings']\n",
      "Tema 6: ['place', 'lunch', 'bit', 'day', 'lot', 'fact', 'choice', 'ambiance', 'today', 'party']\n",
      "Tema 7: ['time', 'bar', 'i', 'drinks', 'location', 'quality', 'years', 'spot', 'review', 'list']\n",
      "Tema 8: ['fries', 'dishes', 'water', 'style', 'potatoes', 'amount', 'hours', 'owner', 'curry', 'hand']\n",
      "Tema 9: ['friends', 'course', 'town', 'bacon', 'door', 'kids', 'fun', 'money', 'corn', 'cake']\n",
      "Tema 10: ['salad', 'cheese', 'rice', 'cream', 'roll', 'wife', 'beans', 'sandwiches', 'reason', 'ice']\n",
      "Tema 11: ['price', 'prices', 'wait', 'selection', 'chips', 'salsa', 'anyone', 'lettuce', 'tomatoes', 'guacamole']\n",
      "Tema 12: ['food', 'service', 'way', 'times', 'sausage', 'end', 'slice', 'life', 'spring', 'vibe']\n",
      "Tema 13: ['night', 'flavor', 'everything', 'patio', 'portions', 'waitress', 'star', 'trip', 'date', 'bite']\n",
      "Tema 14: ['chicken', '..', 'pork', 'chocolate', 'fish', 'tender', 'crispy', 'mouth', 'sides', 'burrito']\n",
      "Tema 15: ['dinner', 'home', 'sandwich', 'husband', 'nothing', 'decor', 'rolls', 'room', 'dining', 'guy']\n",
      "Tema 16: ['experience', 'thing', 'sushi', 'meat', 'things', 'family', 'fan', 'tea', 'top', 'plenty']\n",
      "Tema 17: ['order', 'pizza', 'burger', 'minutes', 'taste', 'dessert', 'visit', 'variety', 'deal', 'counter']\n",
      "Tema 18: ['restaurant', 'meal', 'wine', 'plate', 'stars', 'couple', 'one', 'glass', 'music', 'entrees']\n",
      "Tema 19: ['dish', 'waiter', 'week', 'someone', 'street', 'half', 'manager', 'favorites', 'share', 'morning']\n",
      "=============================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                 #\n",
    "#############################################\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Crear el diccionario y el corpus de palabras\n",
    "dictionary = Dictionary(noun_and_collocation_stream)\n",
    "corpus = [dictionary.doc2bow(opinion) for opinion in noun_and_collocation_stream]\n",
    "\n",
    "# Entrenar el modelo de LDA con diferentes valores de num_topics\n",
    "for num_topics in [5, 10, 15, 20]:\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "    print(f\"Modelo de LDA con {num_topics} temas:\")\n",
    "    for topic in lda_model.show_topics(num_topics=num_topics, formatted=False):\n",
    "        print(f\"Tema {topic[0]}: {[t[0] for t in topic[1]]}\")\n",
    "    print(\"=\"*125)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d5da8",
   "metadata": {},
   "source": [
    "En el primer modelo de LDA con 5 temas, se podría asignar los siguientes nombres a cada tema:\n",
    "\n",
    "* Tema 0: Vida nocturna y bares\n",
    "* Tema 1: Comida y precios\n",
    "* Tema 2: Comida para llevar y pedidos\n",
    "* Tema 3: Experiencia y servicio\n",
    "* Tema 4: Menú y restaurantes\n",
    "\n",
    "Para el segundo modelo de LDA con 10 temas, se podría asignar los siguientes nombres a cada tema:\n",
    "\n",
    "* Tema 0: Comida rápida y hamburguesas\n",
    "* Tema 1: Cena y experiencias en el restaurante\n",
    "* Tema 2: Espera y tiempo de servicio\n",
    "* Tema 3: Personal y bebidas\n",
    "* Tema 4: Pedidos y decoración\n",
    "* Tema 5: Comida y calidad\n",
    "* Tema 6: Platos principales y especialidades\n",
    "* Tema 7: Pizza y sándwiches\n",
    "* Tema 8: Servicio y bebidas\n",
    "* Tema 9: Almuerzo y precios\n",
    "\n",
    "Para el tercer modelo de LDA con 15 temas, se podría asignar los siguientes nombres a cada tema:\n",
    "\n",
    "* Tema 0: Menú y precios\n",
    "* Tema 1: Algo diferente y divertido\n",
    "* Tema 2: Mariscos y platos especiales\n",
    "* Tema 3: Comida y opciones\n",
    "* Tema 4: Almuerzo y tacos\n",
    "* Tema 5: Papas fritas y café\n",
    "* Tema 6: Ambiente y otros detalles\n",
    "* Tema 7: Sabor y comida rápida\n",
    "* Tema 8: Bebidas y aperitivos\n",
    "* Tema 9: Servicio y vino\n",
    "* Tema 10: Comida y calidad\n",
    "* Tema 11: Tiempo y lugar\n",
    "* Tema 12: Pollo y bebidas\n",
    "* Tema 13: Cena y experiencia\n",
    "* Tema 14: Sushi y carne de cerdo\n",
    "\n",
    "Para el ultimo modelo de LDA con 20 temas, se podría asignar los siguientes nombres a cada tema:\n",
    "\n",
    "* Tema 0: Personal y ambiente del establecimiento\n",
    "* Tema 1: Servicio y calidad de los alimentos\n",
    "* Tema 2: Variedad y opciones de comida\n",
    "* Tema 3: Ambiente y problemas en el establecimiento\n",
    "* Tema 4: Menú y opciones de comida\n",
    "* Tema 5: Salsas y guarniciones\n",
    "* Tema 6: Comida y ambiente del establecimiento\n",
    "* Tema 7: Tiempo de espera y calidad del establecimiento\n",
    "* Tema 8: Guarniciones y propietario del establecimiento\n",
    "* Tema 9: Ambiente y público del establecimiento\n",
    "* Tema 10: Ensaladas y opciones vegetarianas\n",
    "* Tema 11: Precios y opciones de comida\n",
    "* Tema 12: Comida y servicio\n",
    "* Tema 13: Ambiente y calidad del establecimiento\n",
    "* Tema 14: Carnes y opciones de comida\n",
    "* Tema 15: Comida y ambiente del establecimiento\n",
    "* Tema 16: Experiencia culinaria y variedad de opciones\n",
    "* Tema 17: Ordenar y calidad de la comida\n",
    "* Tema 18: Comida y bebida\n",
    "* Tema 19: Servicio y popularidad del establecimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3868f4a0",
   "metadata": {
    "id": "3868f4a0"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong> Ejercicio opcional:</strong> Si consideramos también los verbos, ¿mejoraríamos los resultados? Justifica la respuesta mostrando LDA si consideramos los verbos, además de los nombres y colocaciones.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "00bb8946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\eduar\\miniconda3\\envs\\edu\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\eduar\\miniconda3\\envs\\edu\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\eduar\\miniconda3\\envs\\edu\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\eduar\\miniconda3\\envs\\edu\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\eduar\\miniconda3\\envs\\edu\\lib\\site-packages (from scikit-learn) (1.21.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1a1c9e63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a1c9e63",
    "outputId": "ed6fdc3c-a83e-465c-ab5c-15be577cd159"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                 #\n",
    "#############################################\n",
    "from gensim import corpora\n",
    "\n",
    "# función para extraer nombres, verbos y colocaciones de una oración\n",
    "def get_noun_verb_and_collocation(sentence):\n",
    "    noun_verb_and_collocations = []\n",
    "    noun_tags = ['NN', 'NNS']\n",
    "    verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    tokens_pos_tagged = pos_tag(word_tokenize(sentence))\n",
    "    for i in range(len(tokens_pos_tagged)-1):\n",
    "        if '_' in tokens_pos_tagged[i][0]:\n",
    "            noun_verb_and_collocations.append(tokens_pos_tagged[i][0])\n",
    "        elif tokens_pos_tagged[i][1] in noun_tags and tokens_pos_tagged[i+1][1] in verb_tags:\n",
    "            noun_verb_and_collocations.append(tokens_pos_tagged[i][0])\n",
    "        elif tokens_pos_tagged[i][1] in verb_tags and tokens_pos_tagged[i+1][1] in noun_tags:\n",
    "            noun_verb_and_collocations.append(tokens_pos_tagged[i][0])\n",
    "        elif '_' in tokens_pos_tagged[i][0] and tokens_pos_tagged[i+1][1] in verb_tags:\n",
    "            noun_verb_and_collocations.append(tokens_pos_tagged[i][0])\n",
    "        elif '_' in tokens_pos_tagged[i][0] and tokens_pos_tagged[i+1][1] in noun_tags:\n",
    "            noun_verb_and_collocations.append(tokens_pos_tagged[i][0])\n",
    "    return noun_verb_and_collocations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "194d03f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraer nombres, verbos y colocaciones de cada oración transformada\n",
    "noun_verb_and_collocation_stream = [get_noun_verb_and_collocation(opinion) for opinion in opinion_sentences_transformed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3e5c4610",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7c78b26",
    "outputId": "3a0d2831-3329-437d-faec-7c0f9ccb6a6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('place', 0.1756753),\n",
       " ('service', 0.09894681),\n",
       " ('menu', 0.04671423),\n",
       " ('restaurant', 0.03467527),\n",
       " ('pizza', 0.022971252),\n",
       " ('atmosphere', 0.022602575),\n",
       " ('everything', 0.020159384),\n",
       " ('waitress', 0.019082388),\n",
       " ('take', 0.017288648),\n",
       " ('meal', 0.013537677)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topic(1, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a29ddeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.031*\"is\" + 0.022*\"atmosphere\" + 0.021*\"friend\" + 0.020*\"everything\" + 0.014*\"decor\" + 0.013*\"bar\" + 0.013*\"friends\" + 0.013*\"tacos\" + 0.012*\"ordered\" + 0.012*\"dining\"')\n",
      "(1, '0.173*\"food\" + 0.095*\"service\" + 0.053*\"was\" + 0.035*\"people\" + 0.022*\"pizza\" + 0.018*\"drinks\" + 0.018*\"portions\" + 0.015*\"get\" + 0.013*\"everyone\" + 0.013*\"meal\"')\n",
      "(2, '0.022*\"time\" + 0.022*\"fries\" + 0.019*\"waitress\" + 0.019*\"are\" + 0.017*\"dishes\" + 0.017*\"Food\" + 0.015*\"were\" + 0.015*\"sandwich\" + 0.014*\"dish\" + 0.013*\"has\"')\n",
      "(3, '0.057*\"staff\" + 0.050*\"i\" + 0.034*\"restaurant\" + 0.033*\"server\" + 0.027*\"sauce\" + 0.026*\"prices\" + 0.020*\"husband\" + 0.020*\"bread\" + 0.019*\"location\" + 0.018*\"Everything\"')\n",
      "(4, '0.185*\"place\" + 0.050*\"had\" + 0.049*\"menu\" + 0.031*\"have\" + 0.026*\"chicken\" + 0.022*\"\\'s\" + 0.021*\"salad\" + 0.018*\"take\" + 0.018*\"waiter\" + 0.016*\"fried\"')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el936026003483589844171017681\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el936026003483589844171017681_data = {\"mdsDat\": {\"x\": [-0.056541750720121076, 0.354817718264317, -0.07218485234473786, -0.1001813761087704, -0.12590973909068776], \"y\": [-0.00765956780048067, 0.014272604865236422, 0.020497429461671955, -0.2888301256564491, 0.2617196591300215], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [19.83037528543657, 20.433552164738607, 20.150863754422506, 19.837469838650655, 19.74773895675166]}, \"tinfo\": {\"Term\": [\"place\", \"food\", \"service\", \"staff\", \"was\", \"i\", \"had\", \"menu\", \"people\", \"restaurant\", \"server\", \"have\", \"is\", \"sauce\", \"chicken\", \"prices\", \"atmosphere\", \"pizza\", \"'s\", \"time\", \"fries\", \"salad\", \"friend\", \"husband\", \"everything\", \"bread\", \"waitress\", \"are\", \"location\", \"drinks\", \"is\", \"atmosphere\", \"friend\", \"everything\", \"decor\", \"bar\", \"friends\", \"tacos\", \"ordered\", \"dining\", \"specials\", \"rolls\", \"thing\", \"kids\", \"eat\", \"crust\", \"burgers\", \"area\", \"items\", \"beef\", \"kitchen\", \"stars\", \"wait\", \"night\", \"said\", \"guy\", \"favorites\", \"want\", \"chef\", \"cake\", \"food\", \"service\", \"was\", \"people\", \"pizza\", \"drinks\", \"portions\", \"get\", \"everyone\", \"meal\", \"drink\", \"meat\", \"cheese\", \"grilled\", \"family\", \"be\", \"rice\", \"bacon\", \"someone\", \"dinner\", \"selection\", \"plates\", \"reviews\", \"seating\", \"wanted\", \"ambiance\", \"room\", \"Everyone\", \"taking\", \"know\", \"time\", \"fries\", \"waitress\", \"are\", \"Food\", \"dishes\", \"were\", \"sandwich\", \"dish\", \"burger\", \"has\", \"table\", \"soup\", \"patio\", \"servers\", \"make\", \"made\", \"manager\", \"go\", \"plate\", \"got\", \"say\", \"music\", \"others\", \"chips\", \"owners\", \"bartender\", \"took\", \"experience\", \"taste\", \"staff\", \"i\", \"restaurant\", \"server\", \"sauce\", \"prices\", \"husband\", \"bread\", \"location\", \"Everything\", \"wife\", \"try\", \"owner\", \"beans\", \"things\", \"one\", \"price\", \"mashed\", \"beer\", \"ingredients\", \"sushi\", \"places\", \"man\", \"hour\", \"pork\", \"ordering\", \"add\", \"shredded\", \"hostess\", \"parking\", \"place\", \"had\", \"menu\", \"have\", \"chicken\", \"'s\", \"salad\", \"take\", \"waiter\", \"fried\", \"love\", \"order\", \"salsa\", \"roasted\", \"like\", \"roll\", \"home\", \"guys\", \"party\", \"sandwiches\", \"eating\", \"visit\", \"Prices\", \"appetizer\", \"cooked\", \"lot\", \"coffee\", \"cream\", \"having\", \"mom\"], \"Freq\": [1152.0, 1113.0, 613.0, 354.0, 344.0, 314.0, 310.0, 306.0, 224.0, 215.0, 204.0, 194.0, 192.0, 171.0, 163.0, 162.0, 141.0, 142.0, 138.0, 139.0, 137.0, 133.0, 132.0, 128.0, 125.0, 125.0, 121.0, 120.0, 117.0, 118.0, 191.5103457170874, 140.31208869348762, 131.53493498037224, 125.14618711257907, 89.28499584682656, 83.31550039053054, 81.57062342373041, 78.49995892692894, 77.64913205263466, 76.36271455333394, 73.28822597164626, 61.771322066197634, 55.6384502953339, 54.42476607569539, 53.08111535465026, 50.413714287333114, 49.577985854735665, 49.34281918554617, 48.71177824328489, 47.71135223695812, 47.089173043919445, 46.20507313880017, 43.61139775064766, 39.90327227257503, 37.64017068818172, 36.123039221271355, 35.722511438509265, 35.19212774431443, 33.35185819425695, 32.84708772006479, 1112.4140039399997, 612.6752447176929, 344.1443161254563, 224.28124075815794, 142.23665037375494, 117.30943138456152, 114.61336918111421, 99.00420984089295, 86.21792525607306, 83.8254119023939, 83.22152290108383, 79.46668624404904, 75.9539233773079, 62.703319167668774, 62.1473867103156, 59.47776178122572, 58.384819189703975, 54.76859829604232, 49.44691231217193, 43.67625147808384, 43.468348824507856, 40.7374317617302, 38.74738812547213, 36.75438469000911, 36.5168030772322, 36.06885980850031, 34.57682712843687, 33.81347782041673, 33.485266976057446, 33.215829287339176, 138.77198548670293, 136.98060895151565, 120.89777693490976, 119.96997834021795, 105.2978596331522, 105.3259539513906, 96.45349026581562, 96.22270874932205, 86.41615925636316, 81.46815244065485, 81.74227018235533, 79.12661226464645, 71.81281544034503, 69.25817838184999, 66.9586909642995, 66.60375621956544, 65.83401921657956, 63.93623914768513, 63.60676081257171, 57.52015338189051, 55.74969676325254, 50.22782061672323, 49.51585440392549, 48.46009062560271, 47.78081437004786, 47.49049359996546, 44.02066194512935, 43.42059096593021, 42.83774032516288, 42.38254435187757, 353.6626056466174, 314.0977328269359, 215.19019721836005, 203.3233741780854, 170.7193352276127, 161.31222606169882, 127.89567719771422, 124.93929492039406, 116.74858664618296, 111.46472190635674, 89.93779741284597, 67.59459577889814, 60.66869755350828, 56.02519151966881, 53.647815135182, 53.5416282457943, 52.023732747933984, 49.62260594825759, 47.27576913907585, 44.88861950823069, 44.23477273242619, 44.04519446124266, 41.01949695528205, 40.12554332211721, 39.45387645666139, 38.49113454081704, 36.35394361888917, 34.645060778877955, 34.03753866677738, 31.747946821296637, 1151.75592277838, 309.3488018886212, 306.266020971322, 193.97692472731208, 162.43553676846713, 137.72522676016632, 132.8610666257945, 113.34700823880135, 109.62741433403941, 101.05077811432751, 72.11658133224049, 66.13794920151479, 58.319134695707504, 56.20302089263032, 50.662337525796296, 39.42236456106099, 39.2848567523843, 38.544357191038564, 34.47956933126005, 32.69515576690129, 31.893349058955202, 30.677911837430546, 29.074524476063996, 27.320906493617642, 26.919146740768717, 26.794610984100313, 26.700430292832873, 26.645548311402848, 26.56078945683758, 26.07070034717267], \"Total\": [1152.0, 1113.0, 613.0, 354.0, 344.0, 314.0, 310.0, 306.0, 224.0, 215.0, 204.0, 194.0, 192.0, 171.0, 163.0, 162.0, 141.0, 142.0, 138.0, 139.0, 137.0, 133.0, 132.0, 128.0, 125.0, 125.0, 121.0, 120.0, 117.0, 118.0, 192.20838617830952, 141.0100647162025, 132.23374904173798, 125.84427097876367, 89.98268605302977, 84.01188438803301, 82.2679503794102, 79.19633627989685, 78.34576920149365, 77.05860696039404, 73.98657709772367, 62.467316859220574, 56.33582102415517, 55.12247470053459, 53.77798347302871, 51.11104967384648, 50.275030780343975, 50.039498856998776, 49.40951037343262, 48.40827326061125, 47.78693654352841, 46.90272841317842, 44.307430035762884, 40.60037609784689, 38.33714442555407, 36.8200359429463, 36.42349739999621, 35.88934219886705, 34.0501640642284, 33.54467359546444, 1113.1113314329978, 613.3730970147216, 344.8423964286363, 224.97964481894226, 142.93391063945788, 118.00669863002994, 115.31067149540311, 99.70166306968208, 86.91473809088755, 84.5223882491617, 83.92014654321893, 80.16488965226046, 76.65133270356968, 63.401979331011965, 62.84529218449444, 60.17559688564285, 59.08251540954383, 55.46803751166299, 50.14524689978844, 44.375123243128115, 44.166925348946755, 41.436630684432934, 39.44430655269207, 37.45389819740089, 37.215446285211485, 36.76468465083053, 35.2757974820755, 34.51071493010622, 34.18520768338845, 33.917513925490596, 139.46561759579978, 137.67331094701672, 121.59088557997507, 120.66602283773125, 105.99098238549186, 106.02121566088077, 97.1481639792935, 96.91604849574342, 87.10987881660867, 82.16035449016167, 82.4396632963757, 79.81986617495849, 72.50681382457924, 69.95427988390732, 67.65259455874171, 67.29741849995959, 66.52728527204113, 64.62890364569063, 64.30108717156293, 58.21444656463026, 56.4440346488494, 50.922003443021026, 50.208195766020225, 49.15477538142167, 48.47407993163428, 48.18450875582087, 44.714181919742, 44.11490544313743, 43.5315204366014, 43.076700479568196, 354.36024956731444, 314.7943695645995, 215.88724655865718, 204.02122339376425, 171.41658974545962, 162.00984111295466, 128.59264025976992, 125.63633129720446, 117.44674321757537, 112.16081331122619, 90.63482904627087, 68.29274830620459, 61.36545569685938, 56.72356516476255, 54.344066677159745, 54.23810346538197, 52.72252975524058, 50.32232083787315, 47.974738580675165, 45.585671271695894, 44.931326312285485, 44.74206283960904, 41.717684906097986, 40.82246236692271, 40.152566332133894, 39.19145858675218, 37.0508567565945, 35.34244621646996, 34.73510230348802, 32.44448405812247, 1152.4431256619823, 310.0373061405678, 306.9540983990276, 194.6645060851008, 163.12238756745748, 138.41224007146704, 133.54943653601416, 114.03504950000115, 110.31477343690848, 101.73788982479671, 72.80370950888195, 66.825670798314, 59.00649739605548, 56.892018472322924, 51.3489728882753, 40.10838164507294, 39.97444259733188, 39.23252870723254, 35.16781435878623, 33.381747600781274, 32.582750937729, 31.364395229694058, 29.760644443315506, 28.007493962924933, 27.60911772085226, 27.48149360443354, 27.386895714712143, 27.332248142235642, 27.24722408942125, 26.757889935185666], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.4854, -3.7964, -3.861, -3.9108, -4.2485, -4.3177, -4.3388, -4.3772, -4.3881, -4.4048, -4.4459, -4.6169, -4.7214, -4.7435, -4.7685, -4.82, -4.8367, -4.8415, -4.8544, -4.8751, -4.8883, -4.9072, -4.965, -5.0538, -5.1122, -5.1534, -5.1645, -5.1795, -5.2332, -5.2484, -1.756, -2.3524, -2.9292, -3.3574, -3.8128, -4.0054, -4.0287, -4.1751, -4.3134, -4.3415, -4.3488, -4.3949, -4.4401, -4.6318, -4.6407, -4.6847, -4.7032, -4.7671, -4.8694, -4.9935, -4.9982, -5.0631, -5.1132, -5.166, -5.1725, -5.1848, -5.2271, -5.2494, -5.2592, -5.2672, -3.8235, -3.8365, -3.9614, -3.9691, -4.0995, -4.0993, -4.1873, -4.1897, -4.2972, -4.3561, -4.3528, -4.3853, -4.4823, -4.5185, -4.5523, -4.5576, -4.5692, -4.5984, -4.6036, -4.7042, -4.7355, -4.8398, -4.854, -4.8756, -4.8897, -4.8958, -4.9717, -4.9854, -4.9989, -5.0096, -2.8723, -2.9909, -3.3691, -3.4259, -3.6006, -3.6573, -3.8894, -3.9128, -3.9806, -4.0269, -4.2415, -4.5271, -4.6352, -4.7149, -4.7582, -4.7602, -4.789, -4.8362, -4.8847, -4.9365, -4.9511, -4.9554, -5.0266, -5.0486, -5.0655, -5.0902, -5.1474, -5.1955, -5.2132, -5.2828, -1.6871, -3.0016, -3.0117, -3.4684, -3.6458, -3.8109, -3.8468, -4.0057, -4.039, -4.1205, -4.4578, -4.5444, -4.6702, -4.7071, -4.8109, -5.0618, -5.0653, -5.0843, -5.1958, -5.2489, -5.2737, -5.3126, -5.3663, -5.4285, -5.4433, -5.4479, -5.4514, -5.4535, -5.4567, -5.4753], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.6143, 1.613, 1.6127, 1.6124, 1.6102, 1.6096, 1.6094, 1.6091, 1.609, 1.6089, 1.6085, 1.6068, 1.6055, 1.6052, 1.6049, 1.6042, 1.604, 1.6039, 1.6037, 1.6035, 1.6032, 1.603, 1.6021, 1.6006, 1.5996, 1.5988, 1.5985, 1.5983, 1.5972, 1.5969, 1.5874, 1.5869, 1.586, 1.5849, 1.5831, 1.5821, 1.5819, 1.581, 1.5799, 1.5797, 1.5796, 1.5792, 1.5789, 1.5769, 1.5768, 1.5763, 1.5761, 1.5753, 1.574, 1.5721, 1.572, 1.571, 1.5702, 1.5691, 1.569, 1.5689, 1.568, 1.5676, 1.5673, 1.5671, 1.5969, 1.5969, 1.5962, 1.5961, 1.5954, 1.5953, 1.5947, 1.5947, 1.5939, 1.5935, 1.5934, 1.5932, 1.5923, 1.5919, 1.5916, 1.5916, 1.5914, 1.5911, 1.5911, 1.5899, 1.5895, 1.5882, 1.588, 1.5877, 1.5875, 1.5874, 1.5863, 1.5861, 1.5859, 1.5857, 1.6156, 1.6154, 1.6144, 1.6142, 1.6135, 1.6133, 1.6122, 1.612, 1.6116, 1.6114, 1.6099, 1.6073, 1.6062, 1.6052, 1.6047, 1.6047, 1.6043, 1.6036, 1.6029, 1.6022, 1.602, 1.6019, 1.6007, 1.6004, 1.6, 1.5996, 1.5986, 1.5977, 1.5973, 1.5959, 1.6215, 1.6199, 1.6199, 1.6186, 1.6179, 1.6172, 1.617, 1.6161, 1.6159, 1.6154, 1.6126, 1.6118, 1.6104, 1.6099, 1.6087, 1.6049, 1.6047, 1.6044, 1.6024, 1.6013, 1.6007, 1.6, 1.5988, 1.5973, 1.5968, 1.5968, 1.5967, 1.5967, 1.5966, 1.5961]}, \"token.table\": {\"Topic\": [5, 2, 4, 3, 5, 4, 2, 5, 3, 1, 1, 2, 1, 3, 2, 4, 1, 4, 4, 3, 1, 1, 2, 1, 5, 3, 5, 5, 5, 1, 1, 1, 2, 3, 3, 2, 2, 1, 5, 2, 1, 3, 2, 1, 2, 5, 1, 1, 3, 2, 3, 3, 2, 1, 5, 5, 3, 5, 5, 5, 4, 4, 4, 4, 4, 1, 1, 1, 1, 2, 5, 4, 5, 5, 3, 3, 4, 3, 4, 2, 2, 5, 5, 3, 1, 4, 5, 1, 4, 3, 4, 3, 4, 5, 3, 2, 2, 5, 4, 3, 2, 4, 2, 4, 4, 4, 2, 2, 5, 5, 1, 2, 1, 5, 5, 3, 5, 4, 3, 2, 2, 4, 3, 2, 4, 2, 3, 1, 4, 1, 4, 3, 1, 5, 2, 3, 1, 4, 3, 3, 4, 5, 1, 5, 3, 1, 2, 2, 3, 4], \"Freq\": [0.9970216501715875, 0.9852012648494661, 0.9896504556541941, 0.9906503141759019, 0.9744412643763716, 0.9716374505588873, 0.97920056548579, 0.9640277004340835, 0.9944804442703237, 0.9792264335026731, 0.992836931759195, 0.9915620322502923, 0.9879554613564037, 0.9840278433132491, 0.9804638932310561, 0.9872440111502012, 0.9915660437129565, 0.979682253421016, 0.9949351330890174, 0.9858769537039836, 0.9945294756447667, 0.9837627397412482, 0.9915026565018961, 0.9691583258674618, 0.9931193529950428, 0.9902199292425374, 0.9858729620639588, 0.9779378056549698, 0.9878440975470939, 0.9782620454689076, 0.9890791651579434, 0.9862623138134571, 0.9915465419425915, 0.9872588639579526, 0.9903678178512193, 0.989035451186384, 0.9914691399580111, 0.9855334205043428, 0.9821147410529352, 0.9894754547849985, 0.9932911449031625, 0.9877899868584766, 0.9865496339485077, 0.9883729616806032, 0.9990015990300206, 0.9927471483233293, 0.9982323042080263, 0.9967429554501548, 0.9951093574899507, 0.992962373464205, 0.9953175415096857, 0.9921331872958438, 0.9936598299413762, 0.977728540400749, 0.9940730634783255, 0.9966542537945497, 0.9946668475003946, 0.9965864034565689, 0.9909266320631454, 0.9756233599765837, 0.9788369040325469, 0.9798527007133915, 0.9953913360937863, 0.9974765445592365, 0.9871522946716036, 0.9989158320172554, 0.9917119119307685, 0.9796367143051417, 0.9835323918951866, 0.9729486681274407, 0.9932038973976248, 0.9961962059965531, 0.9824793509638116, 0.9889605967291557, 0.9920741501793592, 0.9955805362733228, 0.9827966267132652, 0.990269003337293, 0.993594873358254, 0.9938195280566167, 0.9854688298416736, 0.9968917228862431, 0.971676020156243, 0.995853350974999, 0.9852125483665475, 0.9956100333498213, 0.9876444068806142, 0.995586625735917, 0.9695990241313722, 0.9765073612388405, 0.9940446022487847, 0.9754172287648822, 0.9863001656205659, 0.9667930924887157, 0.9863585203722918, 0.9956456290979984, 0.9934661366551873, 0.999615490211955, 0.9834146484870583, 0.9963162655099336, 0.9894626885144653, 0.9712953258678387, 0.9973057871281628, 0.9862956167203119, 0.993766791535518, 0.9958902317167859, 0.9887358508356957, 0.9816779058569167, 0.9843208503358537, 0.9723653361314543, 0.9925190182207803, 0.9921816797418787, 0.9912058023463702, 0.9958858940159887, 0.982942600552956, 0.9905480205810944, 0.9885641816795613, 0.9975697232917874, 0.9818938105203834, 0.9878811493797357, 0.9735792034485692, 0.9949945237226949, 0.9903537393798685, 0.9993917290853847, 0.9903106249529956, 0.9771614067016734, 0.9930101214238236, 0.9866654582976508, 0.9989833804221712, 0.9807531791066385, 0.9792722274474495, 0.9897285448567226, 0.9848940451529381, 0.9909234090348588, 0.9653298088937932, 0.9750050382786656, 0.9940389432859924, 0.9936687351867918, 0.9966614165998301, 0.9747272394230901, 0.9957133324772348, 0.9883818824809009, 0.993061433815621, 0.9971465885565317, 0.9951403793371797, 0.9752198802101443, 0.9942108369852575, 0.9975571552762637, 0.9881813105645699, 0.9929957495043459], \"Term\": [\"'s\", \"Everyone\", \"Everything\", \"Food\", \"Prices\", \"add\", \"ambiance\", \"appetizer\", \"are\", \"area\", \"atmosphere\", \"bacon\", \"bar\", \"bartender\", \"be\", \"beans\", \"beef\", \"beer\", \"bread\", \"burger\", \"burgers\", \"cake\", \"cheese\", \"chef\", \"chicken\", \"chips\", \"coffee\", \"cooked\", \"cream\", \"crust\", \"decor\", \"dining\", \"dinner\", \"dish\", \"dishes\", \"drink\", \"drinks\", \"eat\", \"eating\", \"everyone\", \"everything\", \"experience\", \"family\", \"favorites\", \"food\", \"fried\", \"friend\", \"friends\", \"fries\", \"get\", \"go\", \"got\", \"grilled\", \"guy\", \"guys\", \"had\", \"has\", \"have\", \"having\", \"home\", \"hostess\", \"hour\", \"husband\", \"i\", \"ingredients\", \"is\", \"items\", \"kids\", \"kitchen\", \"know\", \"like\", \"location\", \"lot\", \"love\", \"made\", \"make\", \"man\", \"manager\", \"mashed\", \"meal\", \"meat\", \"menu\", \"mom\", \"music\", \"night\", \"one\", \"order\", \"ordered\", \"ordering\", \"others\", \"owner\", \"owners\", \"parking\", \"party\", \"patio\", \"people\", \"pizza\", \"place\", \"places\", \"plate\", \"plates\", \"pork\", \"portions\", \"price\", \"prices\", \"restaurant\", \"reviews\", \"rice\", \"roasted\", \"roll\", \"rolls\", \"room\", \"said\", \"salad\", \"salsa\", \"sandwich\", \"sandwiches\", \"sauce\", \"say\", \"seating\", \"selection\", \"server\", \"servers\", \"service\", \"shredded\", \"someone\", \"soup\", \"specials\", \"staff\", \"stars\", \"sushi\", \"table\", \"tacos\", \"take\", \"taking\", \"taste\", \"thing\", \"things\", \"time\", \"took\", \"try\", \"visit\", \"wait\", \"waiter\", \"waitress\", \"want\", \"wanted\", \"was\", \"were\", \"wife\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el936026003483589844171017681\", ldavis_el936026003483589844171017681_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el936026003483589844171017681\", ldavis_el936026003483589844171017681_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el936026003483589844171017681\", ldavis_el936026003483589844171017681_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora\n",
    "\n",
    "# crear diccionario y corpus\n",
    "dictionary = corpora.Dictionary(noun_verb_and_collocation_stream)\n",
    "corpus = [dictionary.doc2bow(opinion) for opinion in noun_verb_and_collocation_stream]\n",
    "\n",
    "vocab = list(dictionary.values())\n",
    "term_frequency = list(dictionary.dfs.values())\n",
    "\n",
    "# entrenar el modelo LDA\n",
    "num_topics = 5\n",
    "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# mostrar los temas encontrados\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# visualizar el modelo LDA\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(vis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "82bacd7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82bacd7c",
    "outputId": "0db96537-9dee-449a-ad6a-b173e37ff48a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La incorporación de verbos mejora o no los resultados?\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "print(\"La incorporación de verbos mejora o no los resultados?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ebe819",
   "metadata": {},
   "source": [
    "En general, la incorporación de verbos puede ayudar a mejorar los resultados de LDA, ya que los verbos pueden proporcionar información adicional sobre las acciones y las relaciones entre los elementos en el texto. En este caso, parece que la incorporación de verbos ha mejorado la separación de los temas, y algunos de los temas parecen más coherentes y relevantes que en la versión anterior sin verbos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ed8c2",
   "metadata": {
    "id": "1f6ed8c2"
   },
   "source": [
    "# 3. Clasificación (2 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32290b6e",
   "metadata": {
    "id": "32290b6e"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Crea un clasificador automático de opiniones positivas y negativas. (0.75 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45ae89",
   "metadata": {
    "id": "3a45ae89"
   },
   "source": [
    "<i>Primer paso</i>: realizamos dos listas. Una con los textos y otra con las etiquetas de valoración (0 y 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ec874492",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec874492",
    "outputId": "4bc940e8-e684-487d-e27a-7d328955ae95"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "opinions = df['text'].to_list()\n",
    "labels = df['sentiment'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9720bc32",
   "metadata": {
    "id": "9720bc32"
   },
   "source": [
    "<i>Segundo paso</i>: Vectorizamos las opiniones con un vectorizador tf.idf. Usad 'word' como analyzer (0.25 punts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9fbcfdd4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fbcfdd4",
    "outputId": "255e55ec-a18c-494d-e593-7e3bb3429f82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                 #\n",
    "#############################################\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word')\n",
    "opinions_vectorized = vectorizer.fit_transform(opinions)\n",
    "print(opinions_vectorized.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3122e026",
   "metadata": {
    "id": "3122e026"
   },
   "source": [
    "<i>Tercer paso</i>: Preparamos el corpus de entrenamiento y evaluación (0.25 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0b251acc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b251acc",
    "outputId": "f99f0b5a-b4f0-4d29-d5e6-b18542f8f011"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(opinions_vectorized, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaecbf4",
   "metadata": {
    "id": "0aaecbf4"
   },
   "source": [
    "<i>Cuarto paso</i>: Entrenar al clasificador con Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "39791e56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39791e56",
    "outputId": "d149d93c-6023-4eb1-e076-75788b888d34"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "log_model = classifier.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78a34d",
   "metadata": {
    "id": "8c78a34d"
   },
   "source": [
    "Quinto paso<i>: Utilizar el modelo entrenado para predecir la categoría 1 (positivo) o 0 (negativo) de las opiniones del conjunto de test y mostrar las palabras más informativas para cada categoría. (0.25 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "160fc014",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "160fc014",
    "outputId": "41a3aea5-f3d5-4872-a3a1-2c2e78e8f696"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras más informativas para la categoría positiva: ['definitely', 'awesome', 'and', 'excellent', 'best', 'amazing', 'love', 'good', 'delicious', 'great']\n",
      "Palabras más informativas para la categoría negativa: ['not', 'no', 'wasn', 'ok', 'bland', 'just', 'horrible', 'was', 'better', 'bad']\n",
      "Accuracy: 0.8949615713065756\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "coef = classifier.coef_[0]\n",
    "top_positive_words = [feature_names[i] for i in coef.argsort()[-10:]]\n",
    "top_negative_words = [feature_names[i] for i in coef.argsort()[:10]]\n",
    "\n",
    "print(\"Palabras más informativas para la categoría positiva:\", top_positive_words)\n",
    "print(\"Palabras más informativas para la categoría negativa:\", top_negative_words)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af23001",
   "metadata": {
    "id": "4af23001"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Exercicio:</strong> Muestra sobre qué aspectos se hacen valoraciones negativas. (0.75 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb0943",
   "metadata": {
    "id": "d8eb0943"
   },
   "source": [
    "<i>Primer paso<i>: Elige dos palabras más informativas de la categoría 0 y toma un conjunto de opiniones en las que aparezcan estas palabras. Preprocesa las opiniones quitando los caracteres de salto de línea (0.25 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7eb6ab69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7eb6ab69",
    "outputId": "28be75bc-8aa9-4df8-b160-fe52457a2f96"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  # \n",
    "#############################################\n",
    "# Elegimos dos palabras más informativas para la categoría negativa, por ejemplo \"terrible\" y \"horrible\"\n",
    "selected_words = [\"horrible\", \"bad\"]\n",
    "\n",
    "# Tomamos un conjunto de opiniones en las que aparezcan estas palabras\n",
    "selected_opinions = []\n",
    "for i in range(len(opinions)):\n",
    "    opinion = opinions[i].replace('\\n', '')\n",
    "    if all(word in opinion for word in selected_words):\n",
    "        selected_opinions.append(opinion)\n",
    "\n",
    "# Preprocesamos las opiniones quitando los caracteres de salto de línea\n",
    "selected_opinions = [opinion.replace('\\n', '') for opinion in selected_opinions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c24dbb",
   "metadata": {
    "id": "b5c24dbb"
   },
   "source": [
    "<i>Segundo paso<i>: Utiliza el diccionario de opiniones (archivo AFINN-111) para extraer la polaridad de cada opinión como la media de los valores de las opinion words del texto. (0.25 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "895f6b09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "895f6b09",
    "outputId": "ff7b7873-2168-491e-d699-4405de10895f"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "#Creamos una lista de opinion words y pesos a partir del diccionario AFINN\n",
    "opinion_words_file = 'AFINN-111.txt'\n",
    "\n",
    "opinion_words = {}\n",
    "with open(opinion_words_file, 'r') as f:\n",
    "    for line in f:\n",
    "        word, score = line.split('\\t')\n",
    "        opinion_words[word] = int(score)\n",
    "\n",
    "opinions_polarity = []\n",
    "for opinion in selected_opinions:\n",
    "    opinion_score = sum(opinion_words.get(word.lower(), 0) for word in opinion.split())\n",
    "    opinions_polarity.append(opinion_score)\n",
    "\n",
    "# Normalizamos la polaridad entre -1 y 1\n",
    "max_score = max(abs(score) for score in opinions_polarity)\n",
    "opinions_polarity = [score / max_score for score in opinions_polarity]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561cfe1",
   "metadata": {
    "id": "1561cfe1"
   },
   "source": [
    "Tercer paso: Selecciona opiniones con polaridad negativa que ejemplifiquen los aspectos peor valorados. Comenta cuáles son estos aspectos (0.25 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "412f2bf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "412f2bf5",
    "outputId": "24deaf9b-edbc-4d29-e9f5-944a5a4746e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opiniones seleccionadas con polaridad negativa:\n",
      "\n",
      "-  I've eaten here many times, but none as bad as last night. Service was excellent, and highly attentive. Food, absolutely horrible. My expectation was they would serve a steak on par with their seafood. After all, they were charging 39 bucks for a ribeye. What I was hoping for was a 1- 1-1/2' thick steak, cooked Pittsburgh style as I had ordered. What I got a a 3/4 in thick piece of meat that was mostly fat, gristle, and in no way resembled Pittsburgh Style. Salad, similar to something you could get at Chick Filet Veggies, blah. Bread basket, ample, but day old, and if not, it certainly wasn't fresh. In addition to bad food, we were crammed into a small room where we were nuts to butts with 6 other tables, listening to conversations ranging from someone's recent bout with pinkeye, and another couple who elected to speak entirely in French, until the waiter showed up, then it was like they turned off the French switch and suddenly began speaking English. I've had it with this place. If I'm going to pay 150 bucks for dinner, it'll be at Mortons, or Maestro where the steaks are 1-1/2 in thick, cooked to perfection, and half of it doesnt wind up on the plate as fat and gristle\n",
      "\n",
      "-  I read all the good reviews of this restaurant , and wanted to try it, as it's right up the street from where I live. I don't think I hit a bad night, it was slow, the service was horrible, they confused our order with the table next to us. He put plates of food down and walked off, the order had nothing to do with ours. We tried to get his attention,gave up and took a bite. OMG, it was AWFUL,fried fish that tasted so fishy that I couldn't eat it. At that point, the table next to us realized it was their meal , and got up and walked out . Finally I got the waiters attention, he took the plates ,gave us the correct meal, and didn't offer to do anything about all of our time he had wasted, let alone the mistake. Our real meal was O.K., I tried the tamales, wasn't blown away,my boyfriend said his steak was so-so. I don't think I would come back again.\n",
      "\n",
      "-  I want to like this place as it is in the neighborhood and there aren't many other options around here, but this is probably the worst restaurant/bar I've ever had the misfortune of going to. My last trip I sat down to order a beer at the bar. The bartender takes about five minutes to get me a beer, despite the bar being nearly empty, as he was too busy for me with his time and focus instead spent flirting with one of the servers. I drink my beer and want another. The glass sits empty for about five minutes while he paces around the bar. I try on three occasions to get his attention and ask for another. The dude is just ignoring me at this point. I guess they don't want my business and I just leave. He can have the 8 cents he owes me in change for a tip... I just want out of here. I feel bad when I leave that he even got the 8 cents from me. I hope he spends it well. On other trips here my wife and I have had the food. It's not any better than the service sadly. I have an English Bulldog at home with a poop eating problem and I'm not sure he would even find this food acceptable. Overall just the worst restaurant around and it's no surprise why the place always looks so dead out front. Nobody's there because it's simply horrible. Avoid at all cost. I wouldn't eat here again, but would give the bar another shot if I heard the replaced all the help. Whoever owns this place needs to clean house. Looking at other reviews on here, I see I'm not alone in my feelings about the service. Get rid of these people already... just sad!!\n",
      "\n",
      "-  Treachery! Seek it out: Let me begin this review by saying that mine is not an indictment on this particular location but on every single P.F. Chang's I've had the misfortune of being dragged to. P.F. Chang's is effectively the Cheesecake Factory of bad Chinese food, with the disadvantage of not being as capable of delivering passable food. They bill themselves as a \"gourmet\" Asian experience, which is a bit of a laugh when they turn around and sell you their equally horrible frozen entrées right from the convenience of your local Wal-Mart. Everything is bland, overcooked, and ridiculously priced. Once you've paid nearly $14 for their \"orange peel chicken\", you may privately hope to yourself that they might have the audacity to include some fried rice. This is your second mistake. Sadly, the bistro calls you on your bullshit. You get white or brown--fried rice is several dollars extra and not at all worth it. Everything seems to be cooked just wrong--things that shouldn't be are too crispy, things that should be sauced are too dry, things that should taste good make you question the will of the gods, all the while you find there's no way to escape the treachery. Things get more and more frustrating as you dig deeper: P.F. Changs, while sounding vaguely Asian, is a wholly American creation. It was founded by Paul Fleming (in Scottsdale, actually) with SOMEone with some Asian influence to spice it up a bit, named Philip Chiang. (They literally cut out the poor bastard's \"i\" so that it would \"fit on the signs better,\" or, as I like to believe, not scare away the flyover-states-white-folk so.) It's highly Americanized (and yes, I've had Chinese food in China, but I've also enjoyed great \"American Chinese\" food here) to the point of regrettable blandness and it is SO VERY EXPENSIVE you may find your credit score taking a ding after eating a plate of \"wok charred beef\", listed as one of the saltiest foods in America. The decoration is clumsy and their famous inclusion of terra cotta warriors are literally symbols of death. From Jennifer 8 Lee's Fortune Cookie Chronicles: 'But not everyone finds the terra-cotta warriors charming. \"Chinese people would never put that in a restuarant,\" Jim told me, pointing at the statues. \"It's not lucky. It's something you put at a burial site! But in America, they think it's a Chinese thing.\" From a Chinese perspective, P.F. Changs is decorated with death.' Which is just fine, because everything there tastes like death anyway.\n",
      "\n",
      "-  WOW! I have never been to a place with service this horrible. I may have caught them on a bad night, but I'm not giving them a second chance to find out! I came in with a group of 4. We stood up front for 5-10 minutes before waving someone down to be seated. That was a mistake.. As we're waiting for a server to take our drink order, we noticed that EVERY single table had empty cups waiting to be refilled, dirty plates that needed bussing, and people with impatient faces. There was only 1-2 dedicated servers, and they completely ignored us. We stuck around just to see how long it would take before someone took our drink order, it was like being in the Twilight Zone. After 30 minutes and flagging down 3 employees, we just walked out laughing, no one had even apologized for the wait, I'm not sure they even saw us leave! It was the strangest restaurant experience of my life. What a joke... **EDIT** To their credit, right after I posted this review, I received a message explaining that the Director of Operations would like to amend the situation, even going so far as to include their cell number. I admit that I feel strange posting such a negative statement amid all the 4 and 5 star reviews, I'm assuming I caught them on a really bad night. I'll edit this if I ever return to the restaurant, but the fact that someone actually reached out to try to rectify things says something\n",
      "\n",
      "-  The worse food and service. I have never given a bad review. I ordered a shrimp dish with brown rice. The waiter brought out white rice. The sauce was horrible and bland. The calamari salad was not fresh. The dressing was bland too. The waitress checked on us and we asked for check immediately. She never asked if anything was wrong or if the food was good. I would never recommend or eat here again. The service and food was horrible. I was visiting from out of town and was hoping to get a new experience. This was not it...\n",
      "\n",
      "-  One star for the most terrible service in recent memory. Our server \"DK\" was outright rude and horrible, and outright ignored us when she wasn't making us uncomfortable. As a former waitress of 14 years, I have the utmost respect, patience and tolerance for my former bretheren - but this woman needs to go. BAD DK! Otherwise, our Teppiniyaki (sp?) chef was wonderful, the food was delicious and fresh and a solid four stars. I have been going to Kyoto since 1995 and it remains a fun and festive place with very reasonable prices. Too bad it was ruined this time by DK! DK needs a different job, more suited to her personallity, like euthanizing kittens or something.\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "selected_opinions_neg = []\n",
    "for i in range(len(opinions_polarity)):\n",
    "    if opinions_polarity[i] < 0:\n",
    "        selected_opinions_neg.append(selected_opinions[i])\n",
    "\n",
    "# Mostrar las opiniones seleccionadas\n",
    "print(\"Opiniones seleccionadas con polaridad negativa:\")\n",
    "for opinion in selected_opinions_neg:\n",
    "    print(\"\\n- \", opinion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69d8d2f",
   "metadata": {
    "id": "b69d8d2f"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Obtén los resultados de las métricas de evaluación del clasificador basado en regresión logística. Obtén también los resultados de las métricas de evaluación cuando el modelo del clasificador es diferente. Por ejemplo, un modelo SVM (0.25 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "afc6ad8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afc6ad8f",
    "outputId": "51da2179-7a5e-4216-e99b-36610b4a5276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      " [[ 98 115]\n",
      " [  8 950]]\n",
      "Reporte de clasificación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.46      0.61       213\n",
      "           1       0.89      0.99      0.94       958\n",
      "\n",
      "    accuracy                           0.89      1171\n",
      "   macro avg       0.91      0.73      0.78      1171\n",
      "weighted avg       0.90      0.89      0.88      1171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN MODELO REGRESIÓN LOGÍSTICA              #\n",
    "#############################################\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Obtenemos las predicciones del modelo en el conjunto de test\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Mostramos la matriz de confusión\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Mostramos el reporte de clasificación con las métricas\n",
    "print(\"Reporte de clasificación:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "210a7df8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "210a7df8",
    "outputId": "9c34cd1b-9fb3-4f44-da5c-43f672110ecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      " [[143  70]\n",
      " [ 23 935]]\n",
      "Reporte de clasificación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.67      0.75       213\n",
      "           1       0.93      0.98      0.95       958\n",
      "\n",
      "    accuracy                           0.92      1171\n",
      "   macro avg       0.90      0.82      0.85      1171\n",
      "weighted avg       0.92      0.92      0.92      1171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN MODELO SVM                       #\n",
    "#############################################\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Creamos un modelo SVM\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Entrenamos el modelo con los datos de entrenamiento\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Obtenemos las predicciones del modelo en el conjunto de test\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "# Mostramos la matriz de confusión\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred_svm))\n",
    "\n",
    "# Mostramos el reporte de clasificación con las métricas\n",
    "print(\"Reporte de clasificación:\\n\", classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e4564",
   "metadata": {
    "id": "ce2e4564"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Compara los dos modelos en función de estas métricas de evaluación. (0.25 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0466babb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0466babb",
    "outputId": "856d7009-8fca-4fc7-d7f0-00e3649d41d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decir cuál de los dos modelos tiene mejores valores de precision recall y f1.\n",
      "Comentar si las diferencias son significativas y apuntar las causas por las cuales un modelo da mejores resultados que el otro.\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "print(\"Decir cuál de los dos modelos tiene mejores valores de precision recall y f1.\\nComentar si las diferencias son significativas y apuntar las causas por las cuales un modelo da mejores resultados que el otro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c745374f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe240b14",
    "outputId": "158f66c0-26c7-4501-9bad-dc113e25e83b"
   },
   "source": [
    "Ambos modelos presentan una precisión alta para la clase 1 (comentarios positivos), con valores superiores al 0.89. Sin embargo, en cuanto a la precisión para la clase 0 (comentarios negativos), el modelo de Regresión Logística presenta un valor de 0.92, mientras que el modelo SVM presenta un valor de 0.86.\n",
    "\n",
    "En cuanto a la medida de recall, el modelo SVM presenta un valor ligeramente superior para ambas clases (0.67 para la clase 0 y 0.98 para la clase 1) en comparación con el modelo de Regresión Logística (0.46 para la clase 0 y 0.99 para la clase 1).\n",
    "\n",
    "En cuanto al F1-score, el modelo SVM presenta un valor superior en general (0.75 para la clase 0 y 0.95 para la clase 1) en comparación con el modelo de Regresión Logística (0.61 para la clase 0 y 0.94 para la clase 1).\n",
    "\n",
    "En conclusión, el modelo SVM presenta un rendimiento ligeramente superior en general en comparación con el modelo de Regresión Logística en este conjunto de datos y métricas de evaluación."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
