{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkBKubyZ7na7"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.877 · Análisis de sentimientos y textos</p>\n",
    "<p style=\"margin: 0; text-align:right;\">Máster Universitario de Ciencia de Datos(Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicaciones</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjWsCAwv7na-"
   },
   "source": [
    "# PRA 3: Deep Learning para el análisis de textos \n",
    "\n",
    "En esta práctica revisaremos y aplicaremos los conocimientos aprendidos durante el curso y, en más detalle, en los últimos módulos del mismo. En concreto trataremos los siguientes temas:\n",
    "\n",
    "1. **Traducción automatica**: con custom embeddings y con embeddings preentrenados.\n",
    "2. **Classificación de frases**: Aplicación de los conceptos ya trabajados para la reutilización de la arquitectura de dos modelos.\n",
    "\n",
    "3. **Detección de NER y NEL**: detección y clasificación de entidades nombradas (NER) y entity linking basandonos en los temas ya trabajados en los notebooks de NER y NEL y añadiendo un ejemplo sencillo de transformers. \n",
    "\n",
    "También incluimos algunos otros temas transversales trabajados a lo largo de la asignatura.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Nombre y apellidos: EDUARDO MORA GONZÁLEZ</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3Opj4kn7nbA"
   },
   "source": [
    "# PARTE 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8DAwGy17nbB"
   },
   "source": [
    "En esta primera parte de la práctica se pide resolver los ejercicios usando la libreria **KERAS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyW13h9Z7nbB"
   },
   "source": [
    "# 1. Traducción Automática (4 puntos)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuKkXmmS7nbD"
   },
   "source": [
    "## 1.1 TA con Custom Embeddings (2 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T19:53:13.101876Z",
     "iopub.status.busy": "2023-06-03T19:53:13.100807Z",
     "iopub.status.idle": "2023-06-03T19:53:13.108491Z",
     "shell.execute_reply": "2023-06-03T19:53:13.107259Z",
     "shell.execute_reply.started": "2023-06-03T19:53:13.101833Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector, Flatten, Dense, Dropout, Activation\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import load_model\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuhhJrUk7nbD"
   },
   "source": [
    "\n",
    "El objetivo de este apartado es entrenar un modelo de traducción automática entre inglés y holandes, siguento los mismos pasos que en el notebook de Machine Translation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdz0sUSb7nbD"
   },
   "source": [
    "\n",
    "<strong>Implementación:</strong> Siguiendo los pasos trabajados en el notebook de traducción automática, implementar y entrenar un modelo de traducción automática, del inglés al holandés.  <br>\n",
    "    - La capa embedding debe de tener una dimensión igual a 300 <br>\n",
    "    - Se recomienda una longitud màxima de secuencia de 12 <br>\n",
    " <br>\n",
    "    \n",
    "Mostrad la aplicación del modelo entrenado con algún ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L28axKt4kIwO"
   },
   "source": [
    "Primero deberéis cargar los datos proporcionados, que encontraréis en el fichero mt/nld.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T11:42:32.261185Z",
     "iopub.status.busy": "2023-06-03T11:42:32.260466Z",
     "iopub.status.idle": "2023-06-03T11:42:32.637251Z",
     "shell.execute_reply": "2023-06-03T11:42:32.636274Z",
     "shell.execute_reply.started": "2023-06-03T11:42:32.261151Z"
    },
    "executionInfo": {
     "elapsed": 732,
     "status": "ok",
     "timestamp": 1684230280227,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "oVYldy9c7nbG",
    "outputId": "09272a62-f59d-423c-d911-2557400a1263"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Go.' 'Lopen!']\n",
      " ['Go.' 'Vooruit.']\n",
      " ['Hi.' 'Hoi.']\n",
      " ...\n",
      " [\"If you translate from your second language into your own native language, rather than the other way around, you're less likely to make mistakes.\"\n",
      "  'Als je vanuit je tweede taal naar je eigen moedertaal vertaalt, in plaats van andersom, maak je minder snel fouten.']\n",
      " ['The Tatoeba Project, which can be found online at tatoeba.org, is working on creating a large database of example sentences translated into many languages.'\n",
      "  'Het Tatoeba-project, dat je online kan vinden op tatoeba.org, werkt aan een grote gegevensbank met in vele talen vertaalde voorbeeldzinnen.']\n",
      " [\"If someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.\"\n",
      "  'Als iemand die je achtergrond niet kent zegt dat je klinkt als een moedertaalspreker betekent dat dat diegene waarschijnlijk iets in je spreken opgemerkt heeft dat hem deed realiseren dat je geen moedertaalspreker bent. Met andere woorden, je klinkt niet echt als een moedertaalspreker.']]\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Definir una función para leer fichero\n",
    "def read_text(filename):\n",
    "        file = open(filename, mode='rt', encoding='utf-8')\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "        return text\n",
    "\n",
    "# Definir una función para procesar los datos\n",
    "def to_lines(text):\n",
    "    # Elimina espacios en blanco al principio y al final del texto y luego lo divide en líneas\n",
    "    sents = text.strip().split('\\n')\n",
    "    \n",
    "    # Divide cada línea en tabulaciones y toma solo los dos primeros elementos\n",
    "    sents = [i.split('\\t')[:2] for i in sents]\n",
    "    return sents\n",
    "\n",
    "# Lee el texto del archivo especificado\n",
    "data = read_text(\"/kaggle/input/data-pra2/mt/nld.txt\")\n",
    "\n",
    "# Convierte el texto en una lista de líneas\n",
    "nld = to_lines(data)\n",
    "nld = array(nld)\n",
    "\n",
    "print(nld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fmFddVqkYU_"
   },
   "source": [
    "Preprocesar los datos, para eliminar puntuaciones y poner en minúscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T11:42:32.639187Z",
     "iopub.status.busy": "2023-06-03T11:42:32.638550Z",
     "iopub.status.idle": "2023-06-03T11:42:33.662485Z",
     "shell.execute_reply": "2023-06-03T11:42:33.661545Z",
     "shell.execute_reply.started": "2023-06-03T11:42:32.639152Z"
    },
    "executionInfo": {
     "elapsed": 1489,
     "status": "ok",
     "timestamp": 1684230347567,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "JgIsoQqS7nbH"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Elimina puntuaciones de cada elemento \n",
    "nld[:, 0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in nld[:, 0]]\n",
    "nld[:, 1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in nld[:, 1]]\n",
    "\n",
    "# Convierte a minúsculas cada elemento\n",
    "for i in range(len(nld)):\n",
    "    nld[i,0] = nld[i,0].lower()\n",
    "    nld[i,1] = nld[i,1].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRInE02Skhbv"
   },
   "source": [
    "Visualizar los datos resultantes, para tener una idea de como van a ser los datos con los que vamos a trabajar, en concreto ver el tamaño del corpus tanto los vectores del inglés como las del holandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T11:42:33.665547Z",
     "iopub.status.busy": "2023-06-03T11:42:33.665016Z",
     "iopub.status.idle": "2023-06-03T11:42:33.672219Z",
     "shell.execute_reply": "2023-06-03T11:42:33.671334Z",
     "shell.execute_reply.started": "2023-06-03T11:42:33.665512Z"
    },
    "executionInfo": {
     "elapsed": 732,
     "status": "ok",
     "timestamp": 1684230409512,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "d8LfflHB7nbI",
    "outputId": "9591eeed-caa9-4051-a48e-8a69f8ea344d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go' 'lopen']\n",
      " ['go' 'vooruit']\n",
      " ['hi' 'hoi']\n",
      " ...\n",
      " ['if you translate from your second language into your own native language rather than the other way around youre less likely to make mistakes'\n",
      "  'als je vanuit je tweede taal naar je eigen moedertaal vertaalt in plaats van andersom maak je minder snel fouten']\n",
      " ['the tatoeba project which can be found online at tatoebaorg is working on creating a large database of example sentences translated into many languages'\n",
      "  'het tatoebaproject dat je online kan vinden op tatoebaorg werkt aan een grote gegevensbank met in vele talen vertaalde voorbeeldzinnen']\n",
      " ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker'\n",
      "  'als iemand die je achtergrond niet kent zegt dat je klinkt als een moedertaalspreker betekent dat dat diegene waarschijnlijk iets in je spreken opgemerkt heeft dat hem deed realiseren dat je geen moedertaalspreker bent met andere woorden je klinkt niet echt als een moedertaalspreker']]\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "#Visualizar los datos\n",
    "print(nld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXqDyZiEks-S"
   },
   "source": [
    "Calcular el vocabulario tanto en holandés como en inglés, e imprimir su tamaño. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T11:42:33.674214Z",
     "iopub.status.busy": "2023-06-03T11:42:33.673553Z",
     "iopub.status.idle": "2023-06-03T11:42:35.872041Z",
     "shell.execute_reply": "2023-06-03T11:42:35.871018Z",
     "shell.execute_reply.started": "2023-06-03T11:42:33.674181Z"
    },
    "executionInfo": {
     "elapsed": 12765,
     "status": "ok",
     "timestamp": 1684230669813,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "ofwfcS6f7nbL",
    "outputId": "3504a281-cc5b-4c46-c0d6-911ed1059e5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario en inglés: 10101\n",
      "Tamaño del vocabulario en holandés: 14586\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Definir una función para tokenizar las oraciones\n",
    "def tokenization(sentences):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    return tokenizer\n",
    "\n",
    "# Tokenizar las oraciones en inglés y obtener el tamaño del vocabulario en inglés\n",
    "eng_tokenizer = tokenization(nld[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "print('Tamaño del vocabulario en inglés: %s' % str(eng_vocab_size))\n",
    "\n",
    "# Tokenizar las oraciones en holandés y obtener el tamaño del vocabulario en holandés\n",
    "dut_tokenizer = tokenization(nld[:, 1])\n",
    "dut_vocab_size = len(dut_tokenizer.word_index) + 1\n",
    "print('Tamaño del vocabulario en holandés: %s' % str(dut_vocab_size))\n",
    "\n",
    "# Definir la longitud máxima de las secuencias en inglés y holandés\n",
    "eng_length = 8\n",
    "dut_length = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPXEAlbGoSuL"
   },
   "source": [
    "Separamos los conjuntos de entrenamiento por idioma y los codificamos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T11:42:35.873831Z",
     "iopub.status.busy": "2023-06-03T11:42:35.873476Z",
     "iopub.status.idle": "2023-06-03T11:42:38.787592Z",
     "shell.execute_reply": "2023-06-03T11:42:38.786669Z",
     "shell.execute_reply.started": "2023-06-03T11:42:35.873798Z"
    },
    "executionInfo": {
     "elapsed": 3910,
     "status": "ok",
     "timestamp": 1684230740023,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "mX7rtG4Q7nbM"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Definir una función para codificar las secuencias\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # Codificar las secuencias con los índices de las palabras\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    # Hacer el padding\n",
    "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
    "    return seq\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "train, test = train_test_split(nld, test_size=0.001, random_state=12) # División en conjuntos de entrenamiento y prueba\n",
    "\n",
    "# Codificar las secuencias en el conjunto de entrenamiento\n",
    "trainX = encode_sequences(dut_tokenizer, dut_length, train[:, 1])  # Codificar las secuencias en holandés\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])  # Codificar las secuencias en inglés\n",
    "\n",
    "# Codificar las secuencias en el conjunto de prueba\n",
    "testX = encode_sequences(dut_tokenizer, dut_length, test[:, 1])  # Codificar las secuencias en holandés\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])  # Codificar las secuencias en inglés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fYxSWIwoYSB"
   },
   "source": [
    "Definimos el modelo encoder-decoder basandonos en el notebook visto en la asignatura, e instanciamos el modelo con una capa de embedding para las frases de la lengua origen (inglés) y la dimensión de la última capa como el vocabulario de la lengua destino (holandés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T11:42:38.789360Z",
     "iopub.status.busy": "2023-06-03T11:42:38.789022Z",
     "iopub.status.idle": "2023-06-03T11:42:40.407097Z",
     "shell.execute_reply": "2023-06-03T11:42:40.406172Z",
     "shell.execute_reply.started": "2023-06-03T11:42:38.789327Z"
    },
    "executionInfo": {
     "elapsed": 4898,
     "status": "ok",
     "timestamp": 1684230860990,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "bGLbPLtE7nbN"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Ajustamos los parametros\n",
    "max_text_length = 8\n",
    "embedding_vec_length = 300\n",
    "\n",
    "# Definimos el modelo\n",
    "model = Sequential()\n",
    "\n",
    "# Capa de embedding para las frases en inglés\n",
    "model.add(Embedding(eng_vocab_size, embedding_vec_length, input_length=max_text_length, mask_zero=True))\n",
    "\n",
    "# Capa LSTM como encoder\n",
    "model.add(LSTM(embedding_vec_length))\n",
    "\n",
    "# Capa de repetición del vector de salida del encoder\n",
    "model.add(RepeatVector(eng_length))\n",
    "\n",
    "# Capa LSTM como decoder\n",
    "model.add(LSTM(embedding_vec_length, return_sequences=True))\n",
    "\n",
    "# Capa densa de salida con activación softmax\n",
    "model.add(Dense(len(eng_tokenizer.word_index) + 1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuufosmqoxcA"
   },
   "source": [
    "Compilamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T10:57:51.816977Z",
     "iopub.status.busy": "2023-06-03T10:57:51.816591Z",
     "iopub.status.idle": "2023-06-03T10:57:51.844976Z",
     "shell.execute_reply": "2023-06-03T10:57:51.843942Z",
     "shell.execute_reply.started": "2023-06-03T10:57:51.816945Z"
    },
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1684230867221,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "wNwpJh4D7nbO"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Definir el optimizador RMSprop con una tasa de aprendizaje de 0.001\n",
    "rms = optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "# Compilar el modelo con el optimizador RMSprop y la función de pérdida 'sparse_categorical_crossentropy'\n",
    "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuVDwSApo0VT"
   },
   "source": [
    "Entrenamos y guardamos el modelo. \n",
    "El modelo puede tardar horas si se hace en CPU, mucho menos si se puede realizar en GPU. Colab permite el uso de GPU en general, si no se hace un uso extensivo, y se va deshabilitando la opción y habilitando segun necesidades. Si se tiene activada siempre penaliza y la desactiva. Para probar si funciona, recomendamos probar de lanzar el entrenamiento solo con una época y ver que funciona, y una vez tenemos claro que el flujo esta funcionando, ya lanzar con muchas más. \n",
    "\n",
    "Hemos visto que en Colab, a pesar de que pedimos que el tamaño de sentencia máxima sea 12, no puede cargar el modelo en memoria y recomendamos bajarlo a 4 y el número de \"units\" a 200, de esta manera si que es capaz de trabajar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T10:57:51.846994Z",
     "iopub.status.busy": "2023-06-03T10:57:51.846490Z",
     "iopub.status.idle": "2023-06-03T11:11:30.232395Z",
     "shell.execute_reply": "2023-06-03T11:11:30.231377Z",
     "shell.execute_reply.started": "2023-06-03T10:57:51.846960Z"
    },
    "executionInfo": {
     "elapsed": 470376,
     "status": "ok",
     "timestamp": 1684231435973,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "_1muJyWc7nbO",
    "outputId": "8bbfb3a3-69c3-4939-f161-437aa696e031",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "917/919 [============================>.] - ETA: 0s - loss: 4.2259\n",
      "Epoch 1: val_loss improved from inf to 3.85890, saving model to dut_en\n",
      "919/919 [==============================] - 38s 31ms/step - loss: 4.2250 - val_loss: 3.8589\n",
      "Epoch 2/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 3.5461\n",
      "Epoch 2: val_loss improved from 3.85890 to 3.39739, saving model to dut_en\n",
      "919/919 [==============================] - 27s 29ms/step - loss: 3.5459 - val_loss: 3.3974\n",
      "Epoch 3/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 3.1599\n",
      "Epoch 3: val_loss improved from 3.39739 to 3.07479, saving model to dut_en\n",
      "919/919 [==============================] - 26s 28ms/step - loss: 3.1596 - val_loss: 3.0748\n",
      "Epoch 4/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 2.8637\n",
      "Epoch 4: val_loss improved from 3.07479 to 2.85852, saving model to dut_en\n",
      "919/919 [==============================] - 26s 29ms/step - loss: 2.8637 - val_loss: 2.8585\n",
      "Epoch 5/50\n",
      "917/919 [============================>.] - ETA: 0s - loss: 2.6398\n",
      "Epoch 5: val_loss improved from 2.85852 to 2.72263, saving model to dut_en\n",
      "919/919 [==============================] - 26s 28ms/step - loss: 2.6399 - val_loss: 2.7226\n",
      "Epoch 6/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 2.4634\n",
      "Epoch 6: val_loss improved from 2.72263 to 2.61215, saving model to dut_en\n",
      "919/919 [==============================] - 26s 29ms/step - loss: 2.4632 - val_loss: 2.6122\n",
      "Epoch 7/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 2.3234\n",
      "Epoch 7: val_loss improved from 2.61215 to 2.54555, saving model to dut_en\n",
      "919/919 [==============================] - 26s 28ms/step - loss: 2.3236 - val_loss: 2.5455\n",
      "Epoch 8/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 2.2038\n",
      "Epoch 8: val_loss improved from 2.54555 to 2.51320, saving model to dut_en\n",
      "919/919 [==============================] - 26s 29ms/step - loss: 2.2037 - val_loss: 2.5132\n",
      "Epoch 9/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 2.0978\n",
      "Epoch 9: val_loss improved from 2.51320 to 2.47586, saving model to dut_en\n",
      "919/919 [==============================] - 26s 28ms/step - loss: 2.0977 - val_loss: 2.4759\n",
      "Epoch 10/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 2.0019\n",
      "Epoch 10: val_loss improved from 2.47586 to 2.44687, saving model to dut_en\n",
      "919/919 [==============================] - 27s 29ms/step - loss: 2.0029 - val_loss: 2.4469\n",
      "Epoch 11/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 1.9110\n",
      "Epoch 11: val_loss did not improve from 2.44687\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 1.9117 - val_loss: 2.4476\n",
      "Epoch 12/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 1.8441\n",
      "Epoch 12: val_loss did not improve from 2.44687\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 1.8441 - val_loss: 2.4597\n",
      "Epoch 13/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 1.7609\n",
      "Epoch 13: val_loss improved from 2.44687 to 2.44020, saving model to dut_en\n",
      "919/919 [==============================] - 28s 31ms/step - loss: 1.7609 - val_loss: 2.4402\n",
      "Epoch 14/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 1.6839\n",
      "Epoch 14: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 14s 15ms/step - loss: 1.6841 - val_loss: 2.4517\n",
      "Epoch 15/50\n",
      "915/919 [============================>.] - ETA: 0s - loss: 1.6217\n",
      "Epoch 15: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 1.6218 - val_loss: 2.4738\n",
      "Epoch 16/50\n",
      "915/919 [============================>.] - ETA: 0s - loss: 1.5622\n",
      "Epoch 16: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 15ms/step - loss: 1.5631 - val_loss: 2.4938\n",
      "Epoch 17/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 1.5134\n",
      "Epoch 17: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 15ms/step - loss: 1.5134 - val_loss: 2.5001\n",
      "Epoch 18/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 1.4638\n",
      "Epoch 18: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 14s 15ms/step - loss: 1.4635 - val_loss: 2.5240\n",
      "Epoch 19/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 1.4174\n",
      "Epoch 19: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 14s 15ms/step - loss: 1.4173 - val_loss: 2.5467\n",
      "Epoch 20/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 1.3706\n",
      "Epoch 20: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 1.3706 - val_loss: 2.5972\n",
      "Epoch 21/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 1.3205\n",
      "Epoch 21: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 15ms/step - loss: 1.3205 - val_loss: 2.5935\n",
      "Epoch 22/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 1.2752\n",
      "Epoch 22: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 1.2750 - val_loss: 2.6110\n",
      "Epoch 23/50\n",
      "917/919 [============================>.] - ETA: 0s - loss: 1.2365\n",
      "Epoch 23: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.2365 - val_loss: 2.6355\n",
      "Epoch 24/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 1.2014\n",
      "Epoch 24: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 1.2020 - val_loss: 2.6498\n",
      "Epoch 25/50\n",
      "917/919 [============================>.] - ETA: 0s - loss: 1.1700\n",
      "Epoch 25: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 1.1702 - val_loss: 2.6613\n",
      "Epoch 26/50\n",
      "915/919 [============================>.] - ETA: 0s - loss: 1.1408\n",
      "Epoch 26: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 14s 15ms/step - loss: 1.1409 - val_loss: 2.6699\n",
      "Epoch 27/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 1.1180\n",
      "Epoch 27: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 1.1180 - val_loss: 2.6892\n",
      "Epoch 28/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 1.0942\n",
      "Epoch 28: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 15ms/step - loss: 1.0942 - val_loss: 2.6971\n",
      "Epoch 29/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 1.0705\n",
      "Epoch 29: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 1.0705 - val_loss: 2.7069\n",
      "Epoch 30/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 1.0440\n",
      "Epoch 30: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 1.0440 - val_loss: 2.7432\n",
      "Epoch 31/50\n",
      "917/919 [============================>.] - ETA: 0s - loss: 1.0176\n",
      "Epoch 31: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 14s 15ms/step - loss: 1.0175 - val_loss: 2.7579\n",
      "Epoch 32/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 0.9919\n",
      "Epoch 32: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 0.9921 - val_loss: 2.7747\n",
      "Epoch 33/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 0.9705\n",
      "Epoch 33: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 15ms/step - loss: 0.9708 - val_loss: 2.7774\n",
      "Epoch 34/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 0.9518\n",
      "Epoch 34: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 0.9520 - val_loss: 2.8064\n",
      "Epoch 35/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 0.9341\n",
      "Epoch 35: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 14s 15ms/step - loss: 0.9341 - val_loss: 2.8149\n",
      "Epoch 36/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 0.9156\n",
      "Epoch 36: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 0.9156 - val_loss: 2.8338\n",
      "Epoch 37/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 0.8977\n",
      "Epoch 37: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 0.8979 - val_loss: 2.8323\n",
      "Epoch 38/50\n",
      "917/919 [============================>.] - ETA: 0s - loss: 0.8828\n",
      "Epoch 38: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 14s 15ms/step - loss: 0.8830 - val_loss: 2.8564\n",
      "Epoch 39/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 0.8689\n",
      "Epoch 39: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 0.8690 - val_loss: 2.8827\n",
      "Epoch 40/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 0.8516\n",
      "Epoch 40: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 14s 15ms/step - loss: 0.8518 - val_loss: 2.8757\n",
      "Epoch 41/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 0.8334\n",
      "Epoch 41: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 0.8334 - val_loss: 2.9020\n",
      "Epoch 42/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 0.8194\n",
      "Epoch 42: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 0.8197 - val_loss: 2.9152\n",
      "Epoch 43/50\n",
      "917/919 [============================>.] - ETA: 0s - loss: 0.8054\n",
      "Epoch 43: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 14s 15ms/step - loss: 0.8054 - val_loss: 2.9331\n",
      "Epoch 44/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 0.7935\n",
      "Epoch 44: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 0.7936 - val_loss: 2.9413\n",
      "Epoch 45/50\n",
      "915/919 [============================>.] - ETA: 0s - loss: 0.7797\n",
      "Epoch 45: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 14s 15ms/step - loss: 0.7799 - val_loss: 2.9565\n",
      "Epoch 46/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 0.7672\n",
      "Epoch 46: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 0.7675 - val_loss: 2.9855\n",
      "Epoch 47/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 0.7552\n",
      "Epoch 47: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 0.7552 - val_loss: 3.0127\n",
      "Epoch 48/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 0.7416\n",
      "Epoch 48: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 0.7416 - val_loss: 3.0269\n",
      "Epoch 49/50\n",
      "917/919 [============================>.] - ETA: 0s - loss: 0.7318\n",
      "Epoch 49: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 14ms/step - loss: 0.7320 - val_loss: 3.0459\n",
      "Epoch 50/50\n",
      "915/919 [============================>.] - ETA: 0s - loss: 0.7238\n",
      "Epoch 50: val_loss did not improve from 2.44020\n",
      "919/919 [==============================] - 13s 15ms/step - loss: 0.7239 - val_loss: 3.0582\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Crear un punto de control para guardar los mejores pesos durante el entrenamiento\n",
    "checkpoint = ModelCheckpoint('dut_en', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Entrenar el modelo y guardar los mejores pesos durante el entrenamiento\n",
    "history = model.fit(trainX, \n",
    "                    trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
    "                    epochs=50, \n",
    "                    batch_size=64, \n",
    "                    validation_split=0.2, \n",
    "                    callbacks=[checkpoint],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Rnmfh3GpdJ0"
   },
   "source": [
    "Una vez entrenado el modelo, se aplica con el conjunto de test para obtener unas prediciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T11:11:30.236778Z",
     "iopub.status.busy": "2023-06-03T11:11:30.236451Z",
     "iopub.status.idle": "2023-06-03T11:11:32.545383Z",
     "shell.execute_reply": "2023-06-03T11:11:32.544348Z",
     "shell.execute_reply.started": "2023-06-03T11:11:30.236749Z"
    },
    "executionInfo": {
     "elapsed": 8849,
     "status": "ok",
     "timestamp": 1684231533639,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "QX74wIEu7nbP",
    "outputId": "62dbb6ae-596d-49f1-f013-c52126eed6e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Función para buscar la palabra correspondiente a un índice en el tokenizer\n",
    "def get_word(n, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == n:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# Obtener las predicciones del modelo\n",
    "preds = np.argmax(model.predict(testX.reshape((testX.shape[0], testX.shape[1]))), axis=-1)\n",
    "\n",
    "# Resultado de las predicciones\n",
    "preds_text = []\n",
    "\n",
    "# Buscamos las predicciones\n",
    "for i in preds:\n",
    "    temp = []\n",
    "    for j in range(len(i)):\n",
    "        t = get_word(i[j], eng_tokenizer)\n",
    "        if j > 0:\n",
    "            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)\n",
    "        else:\n",
    "            if(t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t) \n",
    "\n",
    "    preds_text.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Drwik5f5pllA"
   },
   "source": [
    "Visualizamos los resultados de las predicciones con los valores esperados. Los resultados son curiosos, no podríamos usar este modelo para un entorno real como vais a poder ver. \n",
    "\n",
    "Pregunta: ¿Porque creéis que no son buenos, y como creéis que podrían obtenerse mejores resultados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T11:11:54.034120Z",
     "iopub.status.busy": "2023-06-03T11:11:54.033770Z",
     "iopub.status.idle": "2023-06-03T11:11:54.055246Z",
     "shell.execute_reply": "2023-06-03T11:11:54.054419Z",
     "shell.execute_reply.started": "2023-06-03T11:11:54.034093Z"
    },
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1684231545536,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "Se_zdVHz7nbP"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel lousy</td>\n",
       "      <td>im feel  love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do you want red or white wine with your meal</td>\n",
       "      <td>if your black and name or the white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you look satisfied</td>\n",
       "      <td>you look to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is that all right with you</td>\n",
       "      <td>are you ok good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you cant live on that island</td>\n",
       "      <td>you cant live in that glass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>was i wrong</td>\n",
       "      <td>was i wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>its been fun</td>\n",
       "      <td>its been fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>did that happen recently</td>\n",
       "      <td>did the happened</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>you were late werent you</td>\n",
       "      <td>you youre guys late</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>ive hired an assistant</td>\n",
       "      <td>i have an the</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          actual  \\\n",
       "0                                   i feel lousy   \n",
       "1   do you want red or white wine with your meal   \n",
       "2                             you look satisfied   \n",
       "3                     is that all right with you   \n",
       "4                   you cant live on that island   \n",
       "..                                           ...   \n",
       "69                                   was i wrong   \n",
       "70                                  its been fun   \n",
       "71                      did that happen recently   \n",
       "72                      you were late werent you   \n",
       "73                        ive hired an assistant   \n",
       "\n",
       "                              predicted  \n",
       "0                     im feel  love      \n",
       "1   if your black and name or the white  \n",
       "2                      you look to       \n",
       "3                   are you ok good      \n",
       "4         you cant live in that glass    \n",
       "..                                  ...  \n",
       "69                     was i wrong       \n",
       "70                    its been fun       \n",
       "71                did the happened       \n",
       "72              you youre guys late      \n",
       "73                    i have an the      \n",
       "\n",
       "[74 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Crear un DataFrame con las columnas 'actual' y 'predicted'\n",
    "results = pd.DataFrame({'actual': test[:, 0], 'predicted': preds_text})\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados que se muestran indican que el modelo no está generando predicciones precisas y coherentes con los valores esperados. Hay varias posibles razones por las que esto podría estar sucediendo y cómo se podrían obtener mejores resultados:\n",
    "\n",
    "1. **Tamaño de los datos de entrenamiento:** El tamaño del conjunto de datos de entrenamiento puede no ser suficiente para capturar la complejidad y variabilidad del problema de traducción automática. Obtener un conjunto de datos más grande y diverso podría ayudar a mejorar el rendimiento del modelo.\n",
    "\n",
    "2. **Longitud máxima de secuencia:** En el código, se establece una longitud máxima de secuencia de 8 palabras tanto para el inglés como para el holandés. Esta longitud podría ser demasiado limitada para capturar el contexto completo de las oraciones y dificultar la generación de traducciones precisas. Aumentar la longitud máxima de secuencia podría permitir que el modelo capture mejor las relaciones y estructuras más largas en los textos.\n",
    "\n",
    "3. **Hiperparámetros del modelo:** Los hiperparámetros del modelo, como la dimensión de embedding, el número de capas LSTM y el tamaño del lote, pueden tener un impacto significativo en el rendimiento del modelo. Ajustar y optimizar adecuadamente estos hiperparámetros podría mejorar la capacidad del modelo para capturar y aprender las características del idioma.\n",
    "\n",
    "4. **Entrenamiento y tiempo de entrenamiento:** El número de épocas de entrenamiento y el tamaño del lote utilizado en el código proporcionado podrían no ser óptimos para este problema específico. Ajustar estos parámetros y aumentar el número de épocas podría permitir un entrenamiento más exhaustivo del modelo y una mejora en las predicciones.\n",
    "\n",
    "5. **Arquitectura del modelo:** El modelo utilizado en el código es una arquitectura básica de encoder-decoder con capas LSTM. Esta arquitectura puede no ser lo suficientemente compleja o adecuada para el problema de traducción automática en cuestión. Se podrían explorar arquitecturas más avanzadas, como el uso de mecanismos de atención, para capturar mejor las relaciones entre las palabras y mejorar la calidad de las traducciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Re2z6jLm7nbP"
   },
   "source": [
    "## 1.2 TA con Embeddings preentrenados (2 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zzbdDJq7nbP"
   },
   "source": [
    "En este apartado repetiremos el ejercicio anterior cargando a la capa de embedding los pesos d'un modelo GloVe entrenado para el inglés. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vncpo5D7nbP"
   },
   "source": [
    "Empezamos cargando el modelo GloVe para el inglés. Podéis usar 'glove.42B.300d.txt'. Se puede bajar de aquí: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T11:43:16.419070Z",
     "iopub.status.busy": "2023-06-03T11:43:16.418258Z",
     "iopub.status.idle": "2023-06-03T11:45:51.944376Z",
     "shell.execute_reply": "2023-06-03T11:45:51.943305Z",
     "shell.execute_reply.started": "2023-06-03T11:43:16.419035Z"
    },
    "executionInfo": {
     "elapsed": 140289,
     "status": "ok",
     "timestamp": 1684232895770,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "Oops18MJ7nbR",
    "outputId": "b2fe4e3d-9f4d-4d51-da2d-1af927b5fb60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1917494\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('/kaggle/input/dataTA/glove.42B.300d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kajSHKSH7nbR"
   },
   "source": [
    "A continuación, tenemos que construir la matriz de embeddings. \n",
    "Para no cargar todo el vocabulario del modelo, podemos filtrar solo aquellas entradas presentes en el vocabulario del tokenizador que usaremos. Además, tenemos de incluir en la matriz de vectores correspondientes los índices de las entradas (palabras) que no encontremos en el modelo glove cargado. Estos vectores se suelen inicializar con 0s o con el resultado de una distribución N(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snHfP3Cb7nbR"
   },
   "source": [
    "Por ejemplo, si nuestro tokenizador se llamara `eng_tokenizer` podríamos hacer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T11:46:05.947345Z",
     "iopub.status.busy": "2023-06-03T11:46:05.946782Z",
     "iopub.status.idle": "2023-06-03T11:46:05.991149Z",
     "shell.execute_reply": "2023-06-03T11:46:05.990237Z",
     "shell.execute_reply.started": "2023-06-03T11:46:05.947311Z"
    },
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1684233309037,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "MiQ797NU7nbR"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(eng_tokenizer.word_index) + 1, 300))\n",
    "for word, i in eng_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMMsDFOy7nbR"
   },
   "source": [
    "Para inicializar una capa de embeddings con pesos predefinidos se utiliza el argumento `weights`. Además, como no queremos que se modifiquen los pesos, marcamos el argumento `trainable` como `False`. \n",
    "\n",
    "Siguiendo con nuestro ejemplo, haríamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T11:46:08.680155Z",
     "iopub.status.busy": "2023-06-03T11:46:08.679570Z",
     "iopub.status.idle": "2023-06-03T11:46:08.688532Z",
     "shell.execute_reply": "2023-06-03T11:46:08.687533Z",
     "shell.execute_reply.started": "2023-06-03T11:46:08.680123Z"
    },
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1684233326288,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "QLjfzHAq7nbR"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(eng_tokenizer.word_index) + 1,\n",
    "                            embedding_vec_length,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_text_length,\n",
    "                            trainable=False,\n",
    "                            mask_zero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TAaf6I77nbR"
   },
   "source": [
    "Implementa y entrena de nuevo un modelo de traducción automática del inglés al holandés de forma similar, esta vez cargando los pesos de la capa embedding a partir del modelo Glove preentrenado en inglés y disponible en 'glove.42B.300d.txt'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T11:46:17.189779Z",
     "iopub.status.busy": "2023-06-03T11:46:17.189386Z",
     "iopub.status.idle": "2023-06-03T11:46:17.237289Z",
     "shell.execute_reply": "2023-06-03T11:46:17.236527Z",
     "shell.execute_reply.started": "2023-06-03T11:46:17.189748Z"
    },
    "executionInfo": {
     "elapsed": 1312,
     "status": "ok",
     "timestamp": 1684233339425,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "9hKTQCdp7nbS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 8, 300)            3030300   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 300)               721200    \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 8, 300)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 8, 300)            721200    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8, 10101)          3040401   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,513,101\n",
      "Trainable params: 4,482,801\n",
      "Non-trainable params: 3,030,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Se establecen los pesos de la capa de embeddings del modelo utilizando la matriz de embeddings predefinida\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "\n",
    "# Se marca la capa de embeddings como no entrenable, lo que significa que los pesos no se actualizarán durante el entrenamiento\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "# Se muestra un resumen del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqyHV6m3rIUn"
   },
   "source": [
    "Entrenamos y guardamos el modelo. Otra vez, aunque este entrenamiento es quizá un \"poco\" más liviano que el anterior, recomendamos el uso de GPU si es viable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T11:46:22.222438Z",
     "iopub.status.busy": "2023-06-03T11:46:22.221588Z",
     "iopub.status.idle": "2023-06-03T12:01:49.252168Z",
     "shell.execute_reply": "2023-06-03T12:01:49.251077Z",
     "shell.execute_reply.started": "2023-06-03T11:46:22.222396Z"
    },
    "executionInfo": {
     "elapsed": 161926,
     "status": "ok",
     "timestamp": 1684233520086,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "l58sRZaY7nbS",
    "outputId": "8c4b6602-faca-488b-93c0-3d62ccd46cac",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 4.1412\n",
      "Epoch 1: val_loss improved from inf to 3.72254, saving model to dut_en\n",
      "919/919 [==============================] - 39s 32ms/step - loss: 4.1409 - val_loss: 3.7225\n",
      "Epoch 2/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 3.4276\n",
      "Epoch 2: val_loss improved from 3.72254 to 3.28532, saving model to dut_en\n",
      "919/919 [==============================] - 29s 32ms/step - loss: 3.4271 - val_loss: 3.2853\n",
      "Epoch 3/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 3.0593\n",
      "Epoch 3: val_loss improved from 3.28532 to 3.02064, saving model to dut_en\n",
      "919/919 [==============================] - 29s 31ms/step - loss: 3.0590 - val_loss: 3.0206\n",
      "Epoch 4/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 2.8001\n",
      "Epoch 4: val_loss improved from 3.02064 to 2.88180, saving model to dut_en\n",
      "919/919 [==============================] - 31s 34ms/step - loss: 2.7998 - val_loss: 2.8818\n",
      "Epoch 5/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 2.6018\n",
      "Epoch 5: val_loss improved from 2.88180 to 2.73826, saving model to dut_en\n",
      "919/919 [==============================] - 28s 30ms/step - loss: 2.6021 - val_loss: 2.7383\n",
      "Epoch 6/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 2.4454\n",
      "Epoch 6: val_loss improved from 2.73826 to 2.67229, saving model to dut_en\n",
      "919/919 [==============================] - 28s 30ms/step - loss: 2.4453 - val_loss: 2.6723\n",
      "Epoch 7/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 2.3133\n",
      "Epoch 7: val_loss improved from 2.67229 to 2.64364, saving model to dut_en\n",
      "919/919 [==============================] - 28s 30ms/step - loss: 2.3135 - val_loss: 2.6436\n",
      "Epoch 8/50\n",
      "917/919 [============================>.] - ETA: 0s - loss: 2.2082\n",
      "Epoch 8: val_loss improved from 2.64364 to 2.64094, saving model to dut_en\n",
      "919/919 [==============================] - 28s 30ms/step - loss: 2.2083 - val_loss: 2.6409\n",
      "Epoch 9/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 2.1129\n",
      "Epoch 9: val_loss improved from 2.64094 to 2.61920, saving model to dut_en\n",
      "919/919 [==============================] - 28s 31ms/step - loss: 2.1128 - val_loss: 2.6192\n",
      "Epoch 10/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 2.0265\n",
      "Epoch 10: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 16ms/step - loss: 2.0264 - val_loss: 2.6233\n",
      "Epoch 11/50\n",
      "917/919 [============================>.] - ETA: 0s - loss: 1.9485\n",
      "Epoch 11: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 16ms/step - loss: 1.9485 - val_loss: 2.6240\n",
      "Epoch 12/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 1.8564\n",
      "Epoch 12: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 17ms/step - loss: 1.8566 - val_loss: 2.6314\n",
      "Epoch 13/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 1.7900\n",
      "Epoch 13: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 16ms/step - loss: 1.7906 - val_loss: 2.6729\n",
      "Epoch 14/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 1.7408\n",
      "Epoch 14: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.7408 - val_loss: 2.7209\n",
      "Epoch 15/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 1.6968\n",
      "Epoch 15: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.6968 - val_loss: 2.7446\n",
      "Epoch 16/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 1.6571\n",
      "Epoch 16: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.6569 - val_loss: 2.7611\n",
      "Epoch 17/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 1.6197\n",
      "Epoch 17: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 16ms/step - loss: 1.6199 - val_loss: 2.7785\n",
      "Epoch 18/50\n",
      "917/919 [============================>.] - ETA: 0s - loss: 1.5833\n",
      "Epoch 18: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.5836 - val_loss: 2.8013\n",
      "Epoch 19/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 1.5476\n",
      "Epoch 19: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 16ms/step - loss: 1.5476 - val_loss: 2.8289\n",
      "Epoch 20/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 1.5082\n",
      "Epoch 20: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.5082 - val_loss: 2.8441\n",
      "Epoch 21/50\n",
      "917/919 [============================>.] - ETA: 0s - loss: 1.4846\n",
      "Epoch 21: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 16ms/step - loss: 1.4847 - val_loss: 2.8889\n",
      "Epoch 22/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 1.4670\n",
      "Epoch 22: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.4668 - val_loss: 2.9020\n",
      "Epoch 23/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 1.4476\n",
      "Epoch 23: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 16ms/step - loss: 1.4474 - val_loss: 2.8971\n",
      "Epoch 24/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 1.4177\n",
      "Epoch 24: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 16s 17ms/step - loss: 1.4179 - val_loss: 2.9167\n",
      "Epoch 25/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 1.3966\n",
      "Epoch 25: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.3966 - val_loss: 2.9225\n",
      "Epoch 26/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 1.3799\n",
      "Epoch 26: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.3799 - val_loss: 2.9550\n",
      "Epoch 27/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 1.3561\n",
      "Epoch 27: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.3563 - val_loss: 2.9638\n",
      "Epoch 28/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 1.3360\n",
      "Epoch 28: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.3360 - val_loss: 2.9827\n",
      "Epoch 29/50\n",
      "917/919 [============================>.] - ETA: 0s - loss: 1.3172\n",
      "Epoch 29: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.3178 - val_loss: 2.9954\n",
      "Epoch 30/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 1.3034\n",
      "Epoch 30: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 16ms/step - loss: 1.3034 - val_loss: 3.0118\n",
      "Epoch 31/50\n",
      "917/919 [============================>.] - ETA: 0s - loss: 1.2877\n",
      "Epoch 31: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.2881 - val_loss: 3.0219\n",
      "Epoch 32/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 1.2756\n",
      "Epoch 32: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 16ms/step - loss: 1.2761 - val_loss: 3.0145\n",
      "Epoch 33/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 1.2633\n",
      "Epoch 33: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.2636 - val_loss: 3.0188\n",
      "Epoch 34/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 1.2521\n",
      "Epoch 34: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 16ms/step - loss: 1.2520 - val_loss: 3.0231\n",
      "Epoch 35/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 1.2430\n",
      "Epoch 35: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.2430 - val_loss: 3.0274\n",
      "Epoch 36/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 1.2306\n",
      "Epoch 36: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 16ms/step - loss: 1.2307 - val_loss: 3.0311\n",
      "Epoch 37/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 1.2193\n",
      "Epoch 37: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.2193 - val_loss: 3.0249\n",
      "Epoch 38/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 1.2124\n",
      "Epoch 38: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 15ms/step - loss: 1.2126 - val_loss: 3.0227\n",
      "Epoch 39/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 1.2027\n",
      "Epoch 39: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 16ms/step - loss: 1.2027 - val_loss: 3.0416\n",
      "Epoch 40/50\n",
      "917/919 [============================>.] - ETA: 0s - loss: 1.1966\n",
      "Epoch 40: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.1965 - val_loss: 3.0357\n",
      "Epoch 41/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 1.1881\n",
      "Epoch 41: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 16ms/step - loss: 1.1882 - val_loss: 3.0447\n",
      "Epoch 42/50\n",
      "916/919 [============================>.] - ETA: 0s - loss: 1.1794\n",
      "Epoch 42: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.1795 - val_loss: 3.0413\n",
      "Epoch 43/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 1.1709\n",
      "Epoch 43: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 16ms/step - loss: 1.1710 - val_loss: 3.0510\n",
      "Epoch 44/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 1.1618\n",
      "Epoch 44: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.1618 - val_loss: 3.0495\n",
      "Epoch 45/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 1.1543\n",
      "Epoch 45: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 16ms/step - loss: 1.1543 - val_loss: 3.0625\n",
      "Epoch 46/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 1.1449\n",
      "Epoch 46: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.1449 - val_loss: 3.0817\n",
      "Epoch 47/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 1.1381\n",
      "Epoch 47: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 16ms/step - loss: 1.1382 - val_loss: 3.0876\n",
      "Epoch 48/50\n",
      "919/919 [==============================] - ETA: 0s - loss: 1.1249\n",
      "Epoch 48: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 15s 16ms/step - loss: 1.1249 - val_loss: 3.1312\n",
      "Epoch 49/50\n",
      "917/919 [============================>.] - ETA: 0s - loss: 1.1135\n",
      "Epoch 49: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 16ms/step - loss: 1.1138 - val_loss: 3.1517\n",
      "Epoch 50/50\n",
      "918/919 [============================>.] - ETA: 0s - loss: 1.1037\n",
      "Epoch 50: val_loss did not improve from 2.61920\n",
      "919/919 [==============================] - 14s 16ms/step - loss: 1.1037 - val_loss: 3.1602\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Se define el optimizador RMSprop con una tasa de aprendizaje de 0.001\n",
    "rms = optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "# Se compila el modelo utilizando el optimizador RMSprop y la función de pérdida 'sparse_categorical_crossentropy'\n",
    "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Se crea un punto de control para guardar el mejor modelo según la pérdida de validación\n",
    "checkpoint = ModelCheckpoint('dut_en', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Se entrena el modelo\n",
    "history2 = model.fit(trainX, \n",
    "                     trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
    "                     epochs=50, \n",
    "                     batch_size=64, \n",
    "                     validation_split=0.2, \n",
    "                     callbacks=[checkpoint],\n",
    "                     verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wW-2Cnuqrbsu"
   },
   "source": [
    "Aplicar el modelo y visualizar los resultados a partir de las prediciones obtenidas con este nuevo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T12:02:13.709596Z",
     "iopub.status.busy": "2023-06-03T12:02:13.709229Z",
     "iopub.status.idle": "2023-06-03T12:02:16.564901Z",
     "shell.execute_reply": "2023-06-03T12:02:16.563913Z",
     "shell.execute_reply.started": "2023-06-03T12:02:13.709564Z"
    },
    "executionInfo": {
     "elapsed": 7369,
     "status": "ok",
     "timestamp": 1684233703273,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "efztL7bg7nbT",
    "outputId": "3b5f1bdc-95d4-49ba-b8a7-f086433e7fa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 7ms/step\n",
      "                                          actual                 predicted\n",
      "0                                   i feel lousy              i feel      \n",
      "1   do you want red or white wine with your meal  you  or white   of white\n",
      "2                             you look satisfied       you look right     \n",
      "3                     is that all right with you       do you think at    \n",
      "4                   you cant live on that island  you cant live on in on  \n",
      "..                                           ...                       ...\n",
      "69                                   was i wrong               i  got     \n",
      "70                                  its been fun            its been      \n",
      "71                      did that happen recently   did it happened the    \n",
      "72                      you were late werent you    you guys didnt get    \n",
      "73                        ive hired an assistant       i need an sorry    \n",
      "\n",
      "[74 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Obtener las predicciones del modelo\n",
    "preds = np.argmax(model.predict(testX.reshape((testX.shape[0],testX.shape[1]))), axis=-1)\n",
    "\n",
    "# Resultados\n",
    "preds_text = []\n",
    "\n",
    "# Buscamos las predicciones\n",
    "for i in preds:\n",
    "    temp = []\n",
    "    for j in range(len(i)):\n",
    "        t = get_word(i[j], eng_tokenizer)\n",
    "        if j > 0:\n",
    "            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
    "                     temp.append('')\n",
    "            else:\n",
    "                     temp.append(t)\n",
    "        else:\n",
    "            if(t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t) \n",
    "\n",
    "    preds_text.append(' '.join(temp))\n",
    "\n",
    "# Crear un DataFrame con las columnas 'actual' y 'predicted'\n",
    "print(pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0n1SFEj7nbT"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>(Opcional) Análisis:</strong> Explica cuales són las principales diferencias entre los dos modelos entrenados. ¿Como podríamos mejorar los resultados de esta tarea en concreto?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las principales diferencias entre los dos modelos entrenados son las siguientes:\n",
    "\n",
    "1. **Modelo sin preentrenamiento de embeddings:** En este modelo, se utilizó una capa de embeddings que se inicializó aleatoriamente y se entrenó junto con el resto del modelo durante el proceso de entrenamiento. Los pesos de los embeddings se ajustaron para adaptarse a los datos específicos de la tarea de traducción.\n",
    "\n",
    "2. **Modelo con preentrenamiento de embeddings:** En este modelo, se cargaron los pesos de la capa de embeddings a partir de un modelo GloVe preentrenado en inglés. Esto permitió utilizar vectores de palabras preentrenados que capturan información semántica y sintáctica general del idioma inglés. Los pesos de la capa de embeddings se mantuvieron fijos durante el entrenamiento del modelo de traducción.\n",
    "\n",
    "Para mejorar los resultados de esta tarea en concreto, se pueden considerar las siguientes estrategias:\n",
    "\n",
    "1. **Aumentar el tamaño del conjunto de entrenamiento:** Obtener más datos de entrenamiento puede ayudar al modelo a capturar patrones y regularidades lingüísticas de manera más efectiva.\n",
    "\n",
    "2. **Aumentar la complejidad del modelo:** Se puede explorar la posibilidad de utilizar arquitecturas de modelos más complejas, como modelos con más capas o unidades de LSTM más grandes, para capturar relaciones más complejas en los datos de entrada.\n",
    "\n",
    "3. **Ajustar los hiperparámetros:** Se pueden experimentar con diferentes valores de hiperparámetros, como la tasa de aprendizaje, el tamaño del lote y el número de épocas de entrenamiento, para encontrar la configuración óptima que maximice el rendimiento del modelo.\n",
    "\n",
    "4. **Utilizar técnicas de regularización:** Se pueden aplicar técnicas de regularización, como la regularización L1 o L2, la eliminación de dropout o la normalización por lotes, para reducir el sobreajuste y mejorar la generalización del modelo.\n",
    "\n",
    "5. **Explorar modelos de traducción más avanzados:** En lugar de utilizar un modelo encoder-decoder básico, se pueden explorar modelos más avanzados como Transformer, que han demostrado excelentes resultados en tareas de traducción automática."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcJ5JpIXuE9M"
   },
   "source": [
    "# PARTE 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpiJOV3MxHQQ"
   },
   "source": [
    "# 2. Classificación de notícias (4 puntos)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwA-vzTklXP0"
   },
   "source": [
    "En este apartado planteamos el uso de las arquitecturas vistas hasta el momento para crear un clasificador de notícias en inglés. \n",
    "En concreto usaremos este dataset:\n",
    "https://www.kaggle.com/datasets/rmisra/news-category-dataset\n",
    "Es un dataset de clasificación que tiene 42 categorías. Vamos a enfocarnos en 6 de ellas. En concreto las siguientes:\n",
    "+ HEALTHY LIVING\n",
    "+ QUEER VOICES\n",
    "+ FOOD & DRINK\n",
    "+ BUSINESS\n",
    "+ COMEDY\n",
    "+ SPORTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dACMx1zAN4qc"
   },
   "source": [
    "## 2.1 Preparar datos clasificación notícias (1 punto)\n",
    "\n",
    "Lo primero que haremos será obtener el dataset, coger el archivo , leerlo desde un dataframe pandas, coger las columnas \"headline\" y \"category\".\n",
    "\n",
    "Samplear el corpus y coger sólo las **1000** primeras entradas para que el script vaya más rápido. Más adelante podremos realizar pruebas para ver si añadiendo más entradas tenemos mejores resultados.Obviamente las entradas, luego se repartiran por las categorías que hemos escogido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T12:03:43.745007Z",
     "iopub.status.busy": "2023-06-03T12:03:43.744507Z",
     "iopub.status.idle": "2023-06-03T12:03:45.560636Z",
     "shell.execute_reply": "2023-06-03T12:03:45.559690Z",
     "shell.execute_reply.started": "2023-06-03T12:03:43.744959Z"
    },
    "executionInfo": {
     "elapsed": 3182,
     "status": "ok",
     "timestamp": 1684234085856,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "VRtrYA3nlXP1",
    "outputId": "057d899e-9c31-435b-9b05-abfb2a4f79f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27275</th>\n",
       "      <td>White Supremacists Are Using Genetic Ancestry ...</td>\n",
       "      <td>HEALTHY LIVING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190610</th>\n",
       "      <td>The 5 Most Bafflingly Racist Shows On TV Right...</td>\n",
       "      <td>COMEDY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192879</th>\n",
       "      <td>Do You Know Your Classic Ice Cream Truck Treat...</td>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46770</th>\n",
       "      <td>It's The Horrifying Tale Of The Drunk Girl Who...</td>\n",
       "      <td>COMEDY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20380</th>\n",
       "      <td>7+ Reasons Why Bisexual, Pansexual, Fluid, And...</td>\n",
       "      <td>QUEER VOICES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 headline        category\n",
       "27275   White Supremacists Are Using Genetic Ancestry ...  HEALTHY LIVING\n",
       "190610  The 5 Most Bafflingly Racist Shows On TV Right...          COMEDY\n",
       "192879  Do You Know Your Classic Ice Cream Truck Treat...    FOOD & DRINK\n",
       "46770   It's The Horrifying Tale Of The Drunk Girl Who...          COMEDY\n",
       "20380   7+ Reasons Why Bisexual, Pansexual, Fluid, And...    QUEER VOICES"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                 #\n",
    "#############################################\n",
    "\n",
    "# Lee el archivo JSON de noticias y selecciona las columnas \"headline\" y \"category\"\n",
    "json = pd.read_json('/kaggle/input/data-pra2/class/News_Category_Dataset_v3.json', lines=True)[['headline','category']]\n",
    "\n",
    "# Filtra el dataframe para mantener solo las filas que pertenecen a las categorías de interés\n",
    "json = json[json['category'].isin(['HEALTHY LIVING','QUEER VOICES','FOOD & DRINK','BUSINESS','COMEDY','SPORTS'])]\n",
    "\n",
    "# Realiza un muestreo aleatorio de 1000 filas del dataframe\n",
    "json = json.sample(1000)\n",
    "\n",
    "# Imprime las primeras filas del dataframe resultante\n",
    "json.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eszZbTJRR2p1"
   },
   "source": [
    "Visualizad la distribución de textos por clase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T12:03:50.583157Z",
     "iopub.status.busy": "2023-06-03T12:03:50.582801Z",
     "iopub.status.idle": "2023-06-03T12:03:50.591422Z",
     "shell.execute_reply": "2023-06-03T12:03:50.590497Z",
     "shell.execute_reply.started": "2023-06-03T12:03:50.583126Z"
    },
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1684234116696,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "N6W_dKhLR19N",
    "outputId": "fb73f6fe-45a9-415c-c98b-fd51f4789fd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEALTHY LIVING    185\n",
      "QUEER VOICES      183\n",
      "COMEDY            172\n",
      "FOOD & DRINK      172\n",
      "BUSINESS          156\n",
      "SPORTS            132\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "#Visualizamos la distribución por clase\n",
    "category_counts = json['category'].value_counts()\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mR6vfn3uE9N"
   },
   "source": [
    "Preparad y preprocesad los datos para el entrenamiento. Utilizaremos one-hot encoding por las etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T12:04:07.939368Z",
     "iopub.status.busy": "2023-06-03T12:04:07.939007Z",
     "iopub.status.idle": "2023-06-03T12:04:07.956178Z",
     "shell.execute_reply": "2023-06-03T12:04:07.955114Z",
     "shell.execute_reply.started": "2023-06-03T12:04:07.939339Z"
    },
    "executionInfo": {
     "elapsed": 962,
     "status": "ok",
     "timestamp": 1684234138747,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "ga20obq5uE9N",
    "outputId": "b95fbff3-191a-442b-c223-49cbdb0c28eb"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Preparación de los textos\n",
    "json['headline'] = json['headline'].apply(lambda x: str(x.translate(str.maketrans('','',string.punctuation)).lower()))\n",
    "\n",
    "def encod(x):\n",
    "    # Función para codificar las etiquetas\n",
    "    if x == 'HEALTHY LIVING':\n",
    "        return 0\n",
    "    elif x == 'QUEER VOICES':\n",
    "        return 1\n",
    "    elif x == 'BUSINESS':\n",
    "        return 2\n",
    "    elif x == 'FOOD & DRINK':\n",
    "        return 3\n",
    "    elif x == 'COMEDY':\n",
    "        return 4\n",
    "    elif x == 'SPORTS':\n",
    "        return 5\n",
    "\n",
    "# Codificación de las etiquetas\n",
    "json['category'] = json['category'].apply(lambda x: encod(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGh2TR3huE9N"
   },
   "source": [
    "## 2.2 Preparar datos y embeddings para entrenar (1 punto)\n",
    "La idea del modelo de clasificación que queremos implementar es más simple que la del encoder-decoder usado en el apartado 1.\n",
    "\n",
    "El modelo debe consistir sólo en:\n",
    "\n",
    "- una capa embedding con los pesos del modelo Glove preentrenado para el inglés disponible en el archivo 'glove.42B.300d.txt'\n",
    "- una capa LSTM con un número de units a elegir (por ejemplo, 300)\n",
    "- una capa Dense con una dimensión de salida que tiene el número de categorías con las que queremos clasificar (en este caso, 6).\n",
    "- Además, como loss function `loss` utilizaremos 'categorical_crossentropy' y como `optimizer`, 'adam'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxxuaTTdawqX"
   },
   "source": [
    "Primeramente creamos un tokenizador para las frases del clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T12:04:23.086100Z",
     "iopub.status.busy": "2023-06-03T12:04:23.085532Z",
     "iopub.status.idle": "2023-06-03T12:04:23.115039Z",
     "shell.execute_reply": "2023-06-03T12:04:23.114203Z",
     "shell.execute_reply.started": "2023-06-03T12:04:23.086067Z"
    },
    "executionInfo": {
     "elapsed": 2253,
     "status": "ok",
     "timestamp": 1684234179967,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "kT12lHMJuE9O"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                               #\n",
    "#############################################\n",
    "\n",
    "# Creación del tokenizador para las frases del clasificador\n",
    "eng_tokenizer = tokenization(json['headline'].values)\n",
    "\n",
    "# Tamaño del vocabulario\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "# Longitud máxima de las frases de entrada\n",
    "eng_length = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THxcbzAWuE9J"
   },
   "source": [
    "\n",
    "Cargamos el siguiente modelo GloVe para el inglés. Lo hemos utilizado en la parte 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T12:04:47.779006Z",
     "iopub.status.busy": "2023-06-03T12:04:47.778635Z",
     "iopub.status.idle": "2023-06-03T12:04:47.788827Z",
     "shell.execute_reply": "2023-06-03T12:04:47.787878Z",
     "shell.execute_reply.started": "2023-06-03T12:04:47.778967Z"
    },
    "executionInfo": {
     "elapsed": 124499,
     "status": "ok",
     "timestamp": 1684234321153,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "WN5BNSvPuE9J",
    "outputId": "fcbc1628-f736-496b-acb1-5c9460e12e01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10101, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                #\n",
    "#############################################\n",
    "\n",
    "# Vemos el tamaño para ver si el modelo esta cargado\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg47gNQ2a9mZ"
   },
   "source": [
    "Una vez cargado el modelo de GloVe definimos la capa de Embedding con todos sus pesos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T12:04:58.531170Z",
     "iopub.status.busy": "2023-06-03T12:04:58.530583Z",
     "iopub.status.idle": "2023-06-03T12:04:58.846553Z",
     "shell.execute_reply": "2023-06-03T12:04:58.845664Z",
     "shell.execute_reply.started": "2023-06-03T12:04:58.531136Z"
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1684234443488,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "hXZn_3I9uE9O"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Definimos el modelo\n",
    "model = Sequential()\n",
    "# Capa de Embedding\n",
    "model.add(Embedding(10101, 300, weights=[embedding_matrix], input_length=8, trainable=False))\n",
    "# Capa LSTM\n",
    "model.add(LSTM(300))\n",
    "# Capa Dense\n",
    "model.add(Dense(6))\n",
    "# Capa de activación softmax\n",
    "model.add(Activation('softmax'))\n",
    "# Compilamos el modelo\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htUhNzKzbHcZ"
   },
   "source": [
    "Preparamos el corpus de entrenamiento y test, usando el model_selection de sklearn, y el onehot_encoded para las clases. Usamos 80% para train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T12:04:33.962076Z",
     "iopub.status.busy": "2023-06-03T12:04:33.960511Z",
     "iopub.status.idle": "2023-06-03T12:04:33.968796Z",
     "shell.execute_reply": "2023-06-03T12:04:33.967849Z",
     "shell.execute_reply.started": "2023-06-03T12:04:33.962035Z"
    },
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1684234454507,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "NAba-FJvuE9O"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# División del dataset en conjuntos de entrenamiento y prueba\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(json['headline'].values, json['category'].values, random_state=42, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHwpK4hHgXwd"
   },
   "source": [
    "Codificar los vectores de entrada para el train y para el text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T12:04:40.721413Z",
     "iopub.status.busy": "2023-06-03T12:04:40.720749Z",
     "iopub.status.idle": "2023-06-03T12:04:40.750435Z",
     "shell.execute_reply": "2023-06-03T12:04:40.749559Z",
     "shell.execute_reply.started": "2023-06-03T12:04:40.721378Z"
    },
    "executionInfo": {
     "elapsed": 601,
     "status": "ok",
     "timestamp": 1684234461750,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "0HUX66RmuE9P"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Codificación de las secuencias de entrada\n",
    "trainX = encode_sequences(eng_tokenizer, eng_length, xtrain)\n",
    "testX = encode_sequences(eng_tokenizer, eng_length, xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_eqWNzNgi4e"
   },
   "source": [
    "## 2.3 Definir el modelo y entrenar (1 punto).\n",
    "\n",
    "\n",
    "El modelo debe consistir sólo en:\n",
    "\n",
    "+ una capa embedding con los pesos del modelo GloVe preentrenado para el inglés disponible en el archivo 'glove.42B.300d.txt'\n",
    "+ una capa LSTM con un número de units a elegir (por ejemplo, 300)\n",
    "+ una capa Dense con una dimensión de salida que tiene el número de categorías con las que queremos clasificar (en este caso, 6).\n",
    "    \n",
    "Además, como loss function loss utilizaremos 'categorical_crossentropy' y como optimizer, 'adam'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T12:05:19.770535Z",
     "iopub.status.busy": "2023-06-03T12:05:19.770171Z",
     "iopub.status.idle": "2023-06-03T12:05:19.791122Z",
     "shell.execute_reply": "2023-06-03T12:05:19.790407Z",
     "shell.execute_reply.started": "2023-06-03T12:05:19.770505Z"
    },
    "executionInfo": {
     "elapsed": 1782,
     "status": "ok",
     "timestamp": 1684234500587,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "0f-2I-K4uE9P",
    "outputId": "de5bbc91-8e3c-418e-b6b0-9147d6a6481d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 8, 300)            3030300   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 300)               721200    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 1806      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 6)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,753,306\n",
      "Trainable params: 723,006\n",
      "Non-trainable params: 3,030,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Se muestra un resumen del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpsPz5uDuZzV"
   },
   "source": [
    "Compilar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T12:05:32.372552Z",
     "iopub.status.busy": "2023-06-03T12:05:32.371750Z",
     "iopub.status.idle": "2023-06-03T12:05:32.390922Z",
     "shell.execute_reply": "2023-06-03T12:05:32.389150Z",
     "shell.execute_reply.started": "2023-06-03T12:05:32.372509Z"
    },
    "executionInfo": {
     "elapsed": 884,
     "status": "ok",
     "timestamp": 1684234505552,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "UC9BvysEuE9Q"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "#Compilamos el modelo\n",
    "rms = optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILf77Jy9ub5Z"
   },
   "source": [
    "Entrenar y guardar el modelo. En esta sección aunque sea recomendable usar GPU, con CPU también se puede obtener el resultado sin tener que esperar \"mucho\" tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T12:06:17.089591Z",
     "iopub.status.busy": "2023-06-03T12:06:17.089222Z",
     "iopub.status.idle": "2023-06-03T12:06:38.440530Z",
     "shell.execute_reply": "2023-06-03T12:06:38.439249Z",
     "shell.execute_reply.started": "2023-06-03T12:06:17.089558Z"
    },
    "executionInfo": {
     "elapsed": 75816,
     "status": "ok",
     "timestamp": 1684234587875,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "0a5ez12buE9Q",
    "outputId": "29d29327-36f7-4385-e612-992684f09045",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/20 [==============================] - 1s 8ms/step - loss: 0.4264 - val_loss: 4.3279\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0215 - val_loss: 4.6433\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0021 - val_loss: 5.1058\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.6281e-04 - val_loss: 5.7006\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.8562e-04 - val_loss: 6.2806\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 7.2143e-05 - val_loss: 6.7916\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 0s 11ms/step - loss: 2.7207e-05 - val_loss: 7.4083\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 0.3312 - val_loss: 6.8895\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 0s 11ms/step - loss: 0.2120 - val_loss: 4.7998\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 0.0036 - val_loss: 5.0854\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 8.0889e-04 - val_loss: 5.4166\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 3.0989e-04 - val_loss: 5.9644\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 1.0793e-04 - val_loss: 6.3982\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 3.9639e-05 - val_loss: 6.9706\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.5179e-05 - val_loss: 7.4806\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 6.1945e-06 - val_loss: 7.8920\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 2.3918e-06 - val_loss: 8.4691\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.2839 - val_loss: 5.2658\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0147 - val_loss: 4.9840\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0014 - val_loss: 5.2545\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.8731e-04 - val_loss: 5.5820\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.5187e-04 - val_loss: 5.9238\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.8056e-05 - val_loss: 6.4170\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 2.1138e-05 - val_loss: 6.8896\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 7.6269e-06 - val_loss: 7.4564\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 2.9776e-06 - val_loss: 7.9882\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.2314e-06 - val_loss: 8.4267\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.2992e-07 - val_loss: 8.9050\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 2.9989e-07 - val_loss: 9.5586\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.3426 - val_loss: 5.6900\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0054 - val_loss: 5.4876\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.7856e-04 - val_loss: 5.7070\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.4299e-04 - val_loss: 5.9981\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 6.6047e-05 - val_loss: 6.2973\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 2.6877e-05 - val_loss: 6.7195\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.0925e-05 - val_loss: 7.1347\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 4.0941e-06 - val_loss: 7.6348\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.5302e-06 - val_loss: 8.0668\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 6.1653e-07 - val_loss: 8.5428\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 2.5239e-07 - val_loss: 8.8857\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.2666e-07 - val_loss: 9.2763\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 6.5379e-08 - val_loss: 9.4433\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.1546 - val_loss: 11.1735\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0598 - val_loss: 6.7808\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0143 - val_loss: 5.8821\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 2.3021e-04 - val_loss: 6.0685\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 8.8060e-05 - val_loss: 6.2867\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 4.1770e-05 - val_loss: 6.5824\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.7307e-05 - val_loss: 6.9426\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 6.7544e-06 - val_loss: 7.4128\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 2.6118e-06 - val_loss: 7.8252\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 9.4790e-07 - val_loss: 8.2433\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.8091e-07 - val_loss: 8.6625\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.6596e-07 - val_loss: 9.0229\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 7.8790e-08 - val_loss: 9.2971\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 4.1910e-08 - val_loss: 9.6250\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 2.0675e-08 - val_loss: 9.8230\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.3970e-08 - val_loss: 9.8219\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 8.5682e-09 - val_loss: 9.8993\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 6.3330e-09 - val_loss: 9.8808\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 2.2352e-09 - val_loss: 9.8781\n",
      "Epoch 62/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 2.6077e-09 - val_loss: 9.9644\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 0s 11ms/step - loss: 1.8626e-09 - val_loss: 9.8522\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.6764e-09 - val_loss: 9.8170\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.1176e-09 - val_loss: 9.8102\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.4901e-09 - val_loss: 9.8812\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.8626e-09 - val_loss: 9.8059\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.8626e-10 - val_loss: 9.7847\n",
      "Epoch 69/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 7.4506e-10 - val_loss: 9.7769\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.8626e-10 - val_loss: 9.7545\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 5.5879e-10 - val_loss: 9.7368\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 7.4506e-10 - val_loss: 9.7259\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.1176e-09 - val_loss: 9.7034\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.7372\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 7.4506e-10 - val_loss: 9.7559\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.6967\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.8626e-10 - val_loss: 9.7480\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.6847\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.6947\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.8626e-10 - val_loss: 9.6728\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.6794\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.6937\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.6415\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.6848\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.7253e-10 - val_loss: 9.6717\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.6425\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.6489\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.8626e-10 - val_loss: 9.6399\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.6717\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.6420\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.7253e-10 - val_loss: 9.6505\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.8626e-10 - val_loss: 9.6539\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.6643\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.6519\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.8626e-10 - val_loss: 9.7018\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.6655\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.6793\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.6669\n",
      "Epoch 99/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.6483\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - val_loss: 9.6767\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Entrenamos el modelo\n",
    "history2 = model.fit(trainX, ytrain, epochs=100, batch_size=32, validation_split = 0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxKAcYjTurx2"
   },
   "source": [
    "## 2.4 Evaluar el modelo (1 punto)\n",
    "\n",
    "Se evalua el modelo y se obtienen sus diferentes métricas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T12:06:38.998772Z",
     "iopub.status.busy": "2023-06-03T12:06:38.998064Z",
     "iopub.status.idle": "2023-06-03T12:06:39.087608Z",
     "shell.execute_reply": "2023-06-03T12:06:39.086644Z",
     "shell.execute_reply.started": "2023-06-03T12:06:38.998735Z"
    },
    "executionInfo": {
     "elapsed": 2460,
     "status": "ok",
     "timestamp": 1684234696071,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "EvXKuDAcuE9Q",
    "outputId": "e58de687-1d59-472e-c828-25f611adbeab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "HEALTHY LIVING       0.25      0.24      0.25        37\n",
      "  QUEER VOICES       0.31      0.33      0.32        30\n",
      "      BUSINESS       0.17      0.09      0.11        35\n",
      "  FOOD & DRINK       0.34      0.35      0.35        37\n",
      "        COMEDY       0.26      0.31      0.28        36\n",
      "        SPORTS       0.21      0.28      0.24        25\n",
      "\n",
      "      accuracy                           0.27       200\n",
      "     macro avg       0.26      0.27      0.26       200\n",
      "  weighted avg       0.26      0.27      0.26       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Evaluamos el modelo\n",
    "print(classification_report(ytest, [np.argmax(i) for i in model.predict(testX)], target_names=['HEALTHY LIVING', 'QUEER VOICES', 'BUSINESS', 'FOOD & DRINK', 'COMEDY', 'SPORTS']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6BDFdI_RaaS"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\"> ¿Que pasaría si el modelo de clasificación lo entrenaramos con más datos? Estamos escogiendo solo los 1000 primeros, son muy pocos ejemplos y se entrena super rápido. Que pasaría con 5000? y 10000?\n",
    "Y si no usaramos los embeddings de GloVe? Que nos aportan ambas cosas?\n",
    "¿Si usamos más datos hacen falta usar los embeddings?\n",
    "\n",
    "Expresar vuestra opinión con experimentación acorde para obtener las conclusiones.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si entrenamos el modelo de clasificación con más datos, como 5000 o 10000 ejemplos en lugar de los 1000 actuales, es probable que mejore el rendimiento y la capacidad de generalización del modelo. Al tener más ejemplos de entrenamiento, el modelo tendrá la oportunidad de aprender patrones más complejos y representativos, lo que puede resultar en una mayor precisión en las predicciones.\n",
    "\n",
    "El uso de los embeddings de GloVe aporta beneficios significativos al modelo. Los embeddings preentrenados capturan la información semántica y sintáctica de las palabras, lo que permite al modelo comprender mejor el significado de las frases. Al utilizar estos embeddings, el modelo se beneficia de la transferencia de conocimiento obtenida del modelo preentrenado en un corpus mucho más grande.\n",
    "\n",
    "Sin embargo, es importante destacar que el impacto de los embeddings y la cantidad de datos depende del contexto y la tarea específica. En algunos casos, cuando se dispone de suficientes datos de entrenamiento específicos para la tarea, los embeddings preentrenados pueden no ser tan necesarios y el modelo puede aprender representaciones adecuadas por sí mismo.\n",
    "\n",
    "Para obtener conclusiones más sólidas, sería recomendable realizar experimentos comparativos utilizando diferentes cantidades de datos de entrenamiento (por ejemplo, 1000, 5000 y 10000 ejemplos) y evaluar el rendimiento del modelo con y sin el uso de los embeddings de GloVe. Esto permitiría determinar cómo afecta la cantidad de datos y la presencia de embeddings en el rendimiento del modelo en la tarea específica de clasificación de noticias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpRSYwVIwqZ1"
   },
   "source": [
    "# PARTE 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "je-AFXAsuE9R"
   },
   "source": [
    "\n",
    "# 3. Detección de NER and NEL (2 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wc5mcU77gWpe"
   },
   "source": [
    "En esta parte detectarermos entidades numeradas utilizando tanto SpaCy con transformers como transformers simplemente. En el segundo caso usando una librería llamada simple transformers.\n",
    "\n",
    "Por otro lado, también haremos Named Entity Linking (NEL) donde buscaremos entidades linkadas a una base de conocimiento (KB), en este caso DBpedia. Encontraremos los enlaces a Wikpedia de ciertas entidades del texto, utilizando DBPedia Spotlight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGB1-kYNxW5V"
   },
   "source": [
    "## 3.1 Detección de NER en Spacy (1 punto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIAzOTIEiH5I"
   },
   "source": [
    "Detección de entidades nombradas (NER) usando spaCy. En esta sección usaremos spaCy para detectar NER. A partir de un corpus de CONLL 2003, lo reeentrenaremos y de esta manera afinaremos su cobertura para estas clases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qn9KhNzWlgjx"
   },
   "source": [
    "Instalamos spacy y mos modelos de lenguaje que necesitemos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T19:53:34.231550Z",
     "iopub.status.busy": "2023-06-03T19:53:34.231133Z",
     "iopub.status.idle": "2023-06-03T19:54:54.291252Z",
     "shell.execute_reply": "2023-06-03T19:54:54.289983Z",
     "shell.execute_reply.started": "2023-06-03T19:53:34.231517Z"
    },
    "executionInfo": {
     "elapsed": 17128,
     "status": "ok",
     "timestamp": 1684240767251,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "6j5WkL16iH5Q",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7f42bf22-a5ba-4014-f97b-b705de55f305",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy==3.2.0\n",
      "  Downloading spacy-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy==3.2.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy==3.2.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy==3.2.0) (1.0.9)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from spacy==3.2.0) (2.4.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy==3.2.0) (21.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy==3.2.0) (59.8.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy==3.2.0) (3.1.2)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy==3.2.0) (4.64.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.10/site-packages (from spacy==3.2.0) (3.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from spacy==3.2.0) (0.7.9)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.10/site-packages (from spacy==3.2.0) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy==3.2.0) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy==3.2.0) (2.28.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from pathy>=0.3.5->spacy==3.2.0) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy==3.2.0) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.0) (3.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy==3.2.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy==3.2.0) (2.1.2)\n",
      "Installing collected packages: wasabi, typer, pydantic, thinc, spacy\n",
      "  Attempting uninstall: wasabi\n",
      "    Found existing installation: wasabi 1.1.1\n",
      "    Uninstalling wasabi-1.1.1:\n",
      "      Successfully uninstalled wasabi-1.1.1\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.7.0\n",
      "    Uninstalling typer-0.7.0:\n",
      "      Successfully uninstalled typer-0.7.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.7\n",
      "    Uninstalling pydantic-1.10.7:\n",
      "      Successfully uninstalled pydantic-1.10.7\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.1.9\n",
      "    Uninstalling thinc-8.1.9:\n",
      "      Successfully uninstalled thinc-8.1.9\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.5.2\n",
      "    Uninstalling spacy-3.5.2:\n",
      "      Successfully uninstalled spacy-3.5.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 1.8.20 requires google-api-python-client<2,>=1.7.8, but you have google-api-python-client 2.86.0 which is incompatible.\n",
      "kfp 1.8.20 requires PyYAML<6,>=5.3, but you have pyyaml 6.0 which is incompatible.\n",
      "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.2.0 which is incompatible.\n",
      "en-core-web-lg 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pydantic-1.8.2 spacy-3.2.0 thinc-8.0.17 typer-0.4.2 wasabi-0.10.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting spacy-transformers==1.1.7\n",
      "  Downloading spacy_transformers-1.1.7-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<4.0.0,>=3.1.3 in /opt/conda/lib/python3.10/site-packages (from spacy-transformers==1.1.7) (3.2.0)\n",
      "Collecting transformers<4.21.0,>=3.4.0\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: srsly<3.0.0,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from spacy-transformers==1.1.7) (2.4.6)\n",
      "Collecting spacy-alignments<1.0.0,>=0.7.2\n",
      "  Downloading spacy_alignments-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from spacy-transformers==1.1.7) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (3.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (1.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (2.28.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (8.0.17)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (3.0.12)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (0.7.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (1.23.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (59.8.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (0.4.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->spacy-transformers==1.1.7) (3.11.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->spacy-transformers==1.1.7) (3.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->spacy-transformers==1.1.7) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->spacy-transformers==1.1.7) (1.11.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers==1.1.7) (2023.3.23)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers==1.1.7) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers==1.1.7) (0.13.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (3.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (6.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (3.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<4.0.0,>=3.1.3->spacy-transformers==1.1.7) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->spacy-transformers==1.1.7) (1.3.0)\n",
      "Installing collected packages: tokenizers, spacy-alignments, transformers, spacy-transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.28.1\n",
      "    Uninstalling transformers-4.28.1:\n",
      "      Successfully uninstalled transformers-4.28.1\n",
      "Successfully installed spacy-alignments-0.9.0 spacy-transformers-1.1.7 tokenizers-0.12.1 transformers-4.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting thinc==8.0.12\n",
      "  Downloading thinc-8.0.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (637 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m637.6/637.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: catalogue<2.1.0,>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from thinc==8.0.12) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from thinc==8.0.12) (2.4.6)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from thinc==8.0.12) (59.8.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from thinc==8.0.12) (1.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from thinc==8.0.12) (1.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from thinc==8.0.12) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from thinc==8.0.12) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from thinc==8.0.12) (1.23.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from thinc==8.0.12) (0.7.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from thinc==8.0.12) (2.0.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->thinc==8.0.12) (4.5.0)\n",
      "Installing collected packages: thinc\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.0.17\n",
      "    Uninstalling thinc-8.0.17:\n",
      "      Successfully uninstalled thinc-8.0.17\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.2.0 which is incompatible.\n",
      "en-core-web-lg 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed thinc-8.0.12\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[33mDEPRECATION: https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl#egg=en_core_web_sm==3.2.0 contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.23.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.28.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (59.8.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.12)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.15)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 3.5.0\n",
      "    Uninstalling en-core-web-sm-3.5.0:\n",
      "      Successfully uninstalled en-core-web-sm-3.5.0\n",
      "Successfully installed en-core-web-sm-3.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy==3.2.0\n",
    "!pip install -U spacy-transformers==1.1.7\n",
    "!pip install -U thinc==8.0.12\n",
    "!python -m spacy download en_core_web_sm-3.2.0 --direct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttkg0CqW4Ym9"
   },
   "source": [
    "Definimos un par de funciones que nos va a permitir imprimir los resultados de la detección de NER de forma muy interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T19:55:01.524171Z",
     "iopub.status.busy": "2023-06-03T19:55:01.523741Z",
     "iopub.status.idle": "2023-06-03T19:55:01.539516Z",
     "shell.execute_reply": "2023-06-03T19:55:01.538316Z",
     "shell.execute_reply.started": "2023-06-03T19:55:01.524139Z"
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1684240777984,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "O8Ot3tH3lO48"
   },
   "outputs": [],
   "source": [
    "def get_tokens_to_print(model, text):\n",
    "  \"\"\"Print tokens of the text and its relevant attributes.\n",
    "\n",
    "    Parameters:\n",
    "      model (spaCy model): spaCy model used for tokenization\n",
    "      text (str):  text to transform in a spaCy doc class.\n",
    "\n",
    "    Returns: ---\n",
    "  \"\"\"\n",
    "  doc = model(text)\n",
    "  print (f\"The text:\\n\\n{get_text_to_print(text)}\\n\\nwas converted in a spaCy object: {type(doc)}\\n\")\n",
    "  print (f\"Token-based analysis. Each token is a spaCy object: {type(doc[0])}\\n\")\n",
    "  \n",
    "  # We obtain rows to print: headers and content\n",
    "  rows  = []\n",
    "  # head_align: List of tuples. Each tuple: heather and its alignment when printing\n",
    "  head_align  = [('Token', '<'), ('Lemma', '<'), ('Syntactic parent', '<'), ('#Tok', '>'), ('Chr_Start', '>'), ('Chr_End', '>'), ('POS', '<'), \n",
    "                 ('TAG', '<'), ('TAG meaning:', '<'), ('ENT', '<'), ('DEP', '<'), ('DEP meaning:', '<')]   \n",
    "  head, align = list(zip(*head_align))  \n",
    "  rows.append(head)                           # Header\n",
    "  rows.append(['='*len(i) for i in head])     # Underline headers\n",
    "  for tok in doc:\n",
    "    rows.append([tok.text, tok.lemma_, tok.head.text, str(tok.i), str(tok.idx), str(tok.idx+len(tok)-1), tok.pos_, \n",
    "                 tok.tag_, str(spacy.explain(tok.tag_))[:20], tok.ent_type_, tok.dep_, str(spacy.explain(tok.dep_))[:20]])\n",
    "  \n",
    "  # Width of each column: the witdh of the longest element\n",
    "  columns       = zip(*rows)     \n",
    "  column_widths = [max(len(i) for i in col) for col in columns]\n",
    "\n",
    "  # Print the files with alignment\n",
    "  for row in rows:\n",
    "    print(*[f\"{row[i]:{align[i]}{column_widths[i]}}  \" for i in range(0, len(row))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T19:55:07.281876Z",
     "iopub.status.busy": "2023-06-03T19:55:07.281480Z",
     "iopub.status.idle": "2023-06-03T19:55:07.288739Z",
     "shell.execute_reply": "2023-06-03T19:55:07.287754Z",
     "shell.execute_reply.started": "2023-06-03T19:55:07.281845Z"
    },
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1684240781257,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "u9N8mQA3s56Z"
   },
   "outputs": [],
   "source": [
    "def get_text_to_print(text):\n",
    "  \"\"\"Format given text.\n",
    "\n",
    "    Parameters:\n",
    "      text (str): text to print\n",
    "\n",
    "    Returns:\n",
    "      str: text formatted in 100 character lines with an initial line numbering the characters\n",
    "  \"\"\"\n",
    "  line_length = 100\n",
    "  line_poss   = \"     1-------10--------20--------30--------40--------50--------60--------70--------80--------90-------100\"\n",
    "  text        = text.replace(\"\\n\", \" \")     # In order to avoid that the \\n character produces a line change.\n",
    "  text        = text.replace(\"\\r\", \" \")     # In wikipedia texts we have detected the character '\\r' that, if interpreted, may induce some printing problems.\n",
    "  text_format = \"\\n\".join([ f\"{i//line_length:<5}{text[i:i+line_length]}\"  for i in range(0, len(text), line_length) ])\n",
    "  return line_poss + \"\\n\" + text_format + \"\\n\" + line_poss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2xbabYm4uIe"
   },
   "source": [
    "Cargamos el modelo \"en_core_web_sm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T19:55:24.175396Z",
     "iopub.status.busy": "2023-06-03T19:55:24.174995Z",
     "iopub.status.idle": "2023-06-03T19:55:28.637436Z",
     "shell.execute_reply": "2023-06-03T19:55:28.636318Z",
     "shell.execute_reply.started": "2023-06-03T19:55:24.175362Z"
    },
    "executionInfo": {
     "elapsed": 2307,
     "status": "ok",
     "timestamp": 1684240789154,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "WNY5YFsUlYBN",
    "outputId": "e18662ec-fd85-42ca-9d36-cda856497767"
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Cargamos el modelo\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GCpZoQd4oqD"
   },
   "source": [
    "Convertimos un texto en objecto 'Doc' de spaCy y visualizamos los resultados de analizar este texto a nivel de POS, NER, ENT, DEP... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T19:55:31.297674Z",
     "iopub.status.busy": "2023-06-03T19:55:31.296976Z",
     "iopub.status.idle": "2023-06-03T19:55:31.325280Z",
     "shell.execute_reply": "2023-06-03T19:55:31.324227Z",
     "shell.execute_reply.started": "2023-06-03T19:55:31.297638Z"
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1684240795832,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "iIDs4LbelUjj",
    "outputId": "06bc6a62-ffde-462f-8396-20ff7c49b07c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Apple\tLemma: Apple\tPOS: PROPN\tNER: ORG\tDependency: compound\n",
      "Token: Inc.\tLemma: Inc.\tPOS: PROPN\tNER: ORG\tDependency: nsubj\n",
      "Token: is\tLemma: be\tPOS: AUX\tNER: \tDependency: aux\n",
      "Token: looking\tLemma: look\tPOS: VERB\tNER: \tDependency: ROOT\n",
      "Token: to\tLemma: to\tPOS: PART\tNER: \tDependency: aux\n",
      "Token: buy\tLemma: buy\tPOS: VERB\tNER: \tDependency: xcomp\n",
      "Token: a\tLemma: a\tPOS: DET\tNER: \tDependency: det\n",
      "Token: startup\tLemma: startup\tPOS: NOUN\tNER: \tDependency: dobj\n",
      "Token: in\tLemma: in\tPOS: ADP\tNER: \tDependency: prep\n",
      "Token: the\tLemma: the\tPOS: DET\tNER: \tDependency: det\n",
      "Token: autonomous\tLemma: autonomous\tPOS: ADJ\tNER: \tDependency: amod\n",
      "Token: vehicle\tLemma: vehicle\tPOS: NOUN\tNER: \tDependency: compound\n",
      "Token: industry\tLemma: industry\tPOS: NOUN\tNER: \tDependency: pobj\n",
      "Token: .\tLemma: .\tPOS: PUNCT\tNER: \tDependency: punct\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Función para analizar un texto\n",
    "def analyze_text(text):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        print(f\"Token: {token.text}\\tLemma: {token.lemma_}\\tPOS: {token.pos_}\\tNER: {token.ent_type_}\\tDependency: {token.dep_}\")\n",
    "\n",
    "# Uso de la función anterior\n",
    "text = \"Apple Inc. is looking to buy a startup in the autonomous vehicle industry.\"\n",
    "analyze_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHr-Nz6czi3P"
   },
   "source": [
    "Entrenar un nuevo modelo de NER con los datos de CONLL2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BHEexwQ2K5f"
   },
   "source": [
    "Convertimos los ficheros conll03 (train y valid) a formato spaCy. \n",
    "El corpus lo hemos obtenido de aquí:\n",
    "https://github.com/Hironsan/anago\n",
    "\n",
    "Tip: spacy contiene funciones que permiten convertir de formato conll al formato compilado que necesitan el módulo de train de spaCy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T20:37:42.217574Z",
     "iopub.status.busy": "2023-06-03T20:37:42.217113Z",
     "iopub.status.idle": "2023-06-03T20:39:09.759258Z",
     "shell.execute_reply": "2023-06-03T20:39:09.758063Z",
     "shell.execute_reply.started": "2023-06-03T20:37:42.217539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
      "into documents with `-n 10`.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (14041 documents):\n",
      "/kaggle/working/train.spacy\u001b[0m\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
      "into documents with `-n 10`.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (3250 documents):\n",
      "/kaggle/working/valid.spacy\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# Ruta a los archivos CONLL\n",
    "train_file = \"/kaggle/input/data-pra2/ner/conll03/train.txt\"\n",
    "valid_file = \"/kaggle/input/data-pra2/ner/conll03/valid.txt\"\n",
    "\n",
    "# Rutas de salida para los archivos en formato spaCy\n",
    "train_output_dir = \"/kaggle/working/\"\n",
    "valid_output_dir = \"/kaggle/working/\"\n",
    "\n",
    "# Convertir archivo de entrenamiento a formato spaCy\n",
    "!python -m spacy convert {train_file} {train_output_dir} --converter conll --file-type \"spacy\"\n",
    "\n",
    "# Convertir archivo de validación a formato spaCy\n",
    "!python -m spacy convert {valid_file} {valid_output_dir} --converter conll --file-type \"spacy\"\n",
    "\n",
    "# Cargar los datos de entrenamiento y validación en variables\n",
    "train_data = DocBin().from_disk(\"/kaggle/working/train.spacy\")\n",
    "valid_data = DocBin().from_disk(\"/kaggle/working/valid.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdXR3Jvg5kQV"
   },
   "source": [
    "Descargar el modelo 'en_core_web_trf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T19:56:46.420965Z",
     "iopub.status.busy": "2023-06-03T19:56:46.420529Z",
     "iopub.status.idle": "2023-06-03T19:57:21.668066Z",
     "shell.execute_reply": "2023-06-03T19:57:21.666501Z",
     "shell.execute_reply.started": "2023-06-03T19:56:46.420929Z"
    },
    "executionInfo": {
     "elapsed": 35953,
     "status": "ok",
     "timestamp": 1684236855657,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "CArCJmMSKm0H",
    "outputId": "658e786f-6039-4ba3-d197-c69b587d7de9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[33mDEPRECATION: https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.2.0/en_core_web_trf-3.2.0-py3-none-any.whl#egg=en_core_web_trf==3.2.0 contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting en-core-web-trf==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.2.0/en_core_web_trf-3.2.0-py3-none-any.whl (460.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.2/460.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy-transformers<1.2.0,>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from en-core-web-trf==3.2.0) (1.1.7)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from en-core-web-trf==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (59.8.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.4.6)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (1.23.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (1.0.9)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (0.4.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (8.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.28.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (4.64.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (0.7.9)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (2.0.0)\n",
      "Requirement already satisfied: transformers<4.21.0,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (4.20.1)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /opt/conda/lib/python3.10/site-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.4)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (3.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (3.11.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (2023.3.23)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from transformers<4.21.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (0.13.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (1.3.0)\n",
      "Installing collected packages: en-core-web-trf\n",
      "Successfully installed en-core-web-trf-3.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_trf')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T19:57:33.000575Z",
     "iopub.status.busy": "2023-06-03T19:57:33.000163Z",
     "iopub.status.idle": "2023-06-03T19:57:42.236718Z",
     "shell.execute_reply": "2023-06-03T19:57:42.235427Z",
     "shell.execute_reply.started": "2023-06-03T19:57:33.000540Z"
    },
    "executionInfo": {
     "elapsed": 7661,
     "status": "ok",
     "timestamp": 1684236894813,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "AKBnk2lcKlWZ",
    "outputId": "86a3e2ca-1f02-4f2c-92d2-88dd85f140a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "================= Installed pipeline packages (spaCy v3.2.0) =================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation: /opt/conda/lib/python3.10/site-packages/spacy\u001b[0m\n",
      "\n",
      "NAME              SPACY            VERSION                            \n",
      "en_core_web_trf   >=3.2.0,<3.3.0   \u001b[38;5;2m3.2.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "en_core_web_sm    >=3.2.0,<3.3.0   \u001b[38;5;2m3.2.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "en_core_web_lg    >=3.5.0,<3.6.0   \u001b[38;5;3m3.5.0\u001b[0m   --> 3.2.0     \n",
      "\n",
      "\u001b[1m\n",
      "============================== Install updates ==============================\u001b[0m\n",
      "Use the following commands to update the packages:\n",
      "python -m spacy download en_core_web_lg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T19:57:47.153769Z",
     "iopub.status.busy": "2023-06-03T19:57:47.153293Z",
     "iopub.status.idle": "2023-06-03T19:57:47.162046Z",
     "shell.execute_reply": "2023-06-03T19:57:47.160389Z",
     "shell.execute_reply.started": "2023-06-03T19:57:47.153724Z"
    },
    "executionInfo": {
     "elapsed": 201,
     "status": "ok",
     "timestamp": 1684240727492,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "NAA9QpGWKosp",
    "outputId": "16cd7046-a190-48c2-b3cd-845d3d79207d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy version installed: 3.2.0\n"
     ]
    }
   ],
   "source": [
    "print (f\"Spacy version installed: {spacy.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L35Q-Df_5yfC"
   },
   "source": [
    "Entrenar usando la función train de spaCy a partir del modelo 'en_core_web_trf'. \n",
    "Usar el fichero de configuración adjunto y modificar las cosas que consideréis oportunas. La versión entregada ya funciona pero se puede customizar si hay interés. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T21:07:46.647408Z",
     "iopub.status.busy": "2023-06-03T21:07:46.646999Z",
     "iopub.status.idle": "2023-06-03T21:33:24.352263Z",
     "shell.execute_reply": "2023-06-03T21:33:24.350957Z",
     "shell.execute_reply.started": "2023-06-03T21:07:46.647375Z"
    },
    "id": "iXl4T4595d1x",
    "outputId": "7f619114-1ad3-41f3-dea0-1696e38049be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: /kaggle/working\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-03 21:07:49,582] [INFO] Set up nlp object from config\n",
      "[2023-06-03 21:07:49,619] [INFO] Pipeline: ['transformer', 'ner']\n",
      "[2023-06-03 21:07:49,621] [INFO] Resuming training for: ['ner', 'transformer']\n",
      "[2023-06-03 21:07:49,630] [INFO] Created vocabulary\n",
      "[2023-06-03 21:07:52,010] [WARNING] [W112] The model specified to use for initial vectors (en_core_web_trf) has no vectors. This is almost certainly a mistake.\n",
      "[2023-06-03 21:07:52,015] [INFO] Added vectors: en_core_web_trf\n",
      "[2023-06-03 21:07:52,056] [INFO] Finished initializing nlp object\n",
      "[2023-06-03 21:07:52,057] [INFO] Initialized pipeline components: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0         480.74    345.40    7.88    6.40   10.27    0.08\n",
      "  1     200       38910.36  25996.48   60.22   67.55   54.33    0.60\n",
      "  2     400        4289.68  12146.00   70.58   70.69   70.46    0.71\n",
      "  3     600        2295.85   5294.50   93.28   92.85   93.72    0.93\n",
      "  4     800        1526.68   1930.17   94.69   94.62   94.75    0.95\n",
      "  5    1000         902.21   1114.27   95.62   95.33   95.91    0.96\n",
      "  6    1200         843.79    952.09   95.68   95.54   95.83    0.96\n",
      "  7    1400         685.02    797.04   94.80   94.59   95.02    0.95\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "/kaggle/working/model-last\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "from spacy.cli.train import train\n",
    "\n",
    "#Entrenar el modelo con la configuración nueva\n",
    "train(\"/kaggle/input/configuracionmodificada/config_Modificado.cfg\", overrides={\"paths.train\": \"/kaggle/working/train.spacy\", \"paths.dev\": \"/kaggle/working/valid.spacy\"}, use_gpu= 0, output_path =\"/kaggle/working/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOlpHvLn6Qfg"
   },
   "source": [
    "Predecir una frase de ejemplo con el nuevo modelo y visualizar los resultados. \n",
    "Ojo que colab, no le gusta cargar modelos desde paths, solo si estan en local, con lo que recomendamos, generar el modelo en drive, guardarlo y luego subir la mejor versión para cargarlo desde de aquí. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T21:38:38.207447Z",
     "iopub.status.busy": "2023-06-03T21:38:38.207024Z",
     "iopub.status.idle": "2023-06-03T21:38:40.475010Z",
     "shell.execute_reply": "2023-06-03T21:38:40.474033Z",
     "shell.execute_reply.started": "2023-06-03T21:38:38.207417Z"
    },
    "executionInfo": {
     "elapsed": 9863,
     "status": "ok",
     "timestamp": 1684240876100,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "rmr1vYmY_MUC",
    "outputId": "78b740de-923a-408d-c65e-deb99ca8deb6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    John\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " works at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple Inc.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Cupertino\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Obtener el modelo\n",
    "nlp_predict = spacy.load(\"/kaggle/working/model-best\")\n",
    "\n",
    "# Frase a predecir\n",
    "text = \"John works at Apple Inc. in Cupertino.\"\n",
    "\n",
    "# Realizar al predicción\n",
    "doc = nlp_predict(text)\n",
    "\n",
    "# Mostrar el resultado\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TR08_0r7dIT"
   },
   "source": [
    "Evaluar los resultados obtenidos y calcular las métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-06-03T21:42:04.253683Z",
     "iopub.status.busy": "2023-06-03T21:42:04.252661Z",
     "iopub.status.idle": "2023-06-03T21:42:28.303061Z",
     "shell.execute_reply": "2023-06-03T21:42:28.301747Z",
     "shell.execute_reply.started": "2023-06-03T21:42:04.253636Z"
    },
    "executionInfo": {
     "elapsed": 52664,
     "status": "ok",
     "timestamp": 1684240971784,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "cJqZUm9k7gNb",
    "outputId": "d6d6ff65-5310-49ba-e59e-b1308a1578a1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
      "into documents with `-n 10`.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (3453 documents):\n",
      "/kaggle/working/test.spacy\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Ruta a los archivos CONLL\n",
    "test_file = \"/kaggle/input/data-pra2/ner/conll03/test.txt\"\n",
    "\n",
    "# Rutas de salida para los archivos en formato spaCy\n",
    "test_output_dir = \"/kaggle/working/\"\n",
    "\n",
    "# Convertir archivo de entrenamiento a formato spaCy\n",
    "!python -m spacy convert {test_file} {test_output_dir} --converter conll --file-type \"spacy\"\n",
    "\n",
    "test_data = DocBin().from_disk(\"/kaggle/working/test.spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T22:03:26.708965Z",
     "iopub.status.busy": "2023-06-03T22:03:26.708392Z",
     "iopub.status.idle": "2023-06-03T22:03:26.732589Z",
     "shell.execute_reply": "2023-06-03T22:03:26.731241Z",
     "shell.execute_reply.started": "2023-06-03T22:03:26.708913Z"
    }
   },
   "outputs": [],
   "source": [
    "####### CÓDIGO OBTENIDO EN PARTE DE LOS EJEMPLOS DEL TEMARIO ###########\n",
    "def score_ner(examples):\n",
    "    \"\"\"Evaluate spaCy model with the spaCy Scorer class.\n",
    "\n",
    "    Parameters:\n",
    "      examples (list): list of 'example' objects spaCy: documents spaCy annotated (predicted and gold reference).\n",
    "\n",
    "    Returns: \n",
    "      dict: result of the evaluation. See dictionary format at: https://spacy.io/api/scorer#score_spans   \n",
    "    \"\"\"\n",
    "    scorer = Scorer()\n",
    "    # Evaluate 'examples' and return the evaluation result\n",
    "    return scorer.score_spans(examples, \"ents\")\n",
    "\n",
    "\n",
    "def print_metrics(metrics, model):\n",
    "    \"\"\"Print results of the spaCy entity evaluation.\n",
    "\n",
    "    Parameters:\n",
    "      metrics (dict): results of the spaCy entity evaluation; dictionary with spaCy format (see 'returns' in https://spacy.io/api/scorer#score_spans)\n",
    "      model (spaCy model): spaCy model for NER\n",
    "\n",
    "    Returns: --- \n",
    "    \"\"\"\n",
    "    df = pd.DataFrame([metrics[\"ents_p\"], metrics[\"ents_r\"], metrics[\"ents_f\"]], columns=[\"Total\"], index=[\"Precision\", \"Recall\", \"F-score\"])\n",
    "    for entity, scores in metrics[\"ents_per_type\"].items():\n",
    "        df[entity] = [scores[\"p\"], scores[\"r\"], scores[\"f\"]]\n",
    "    \n",
    "    df = (df * 100).round(decimals=2)\n",
    "\n",
    "    print(f\"\\nEvaluation of the model: '{model.meta['lang']+'_'+model.meta['name']}'\")\n",
    "    print(\"===============================================\\n\")\n",
    "    print(df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "def ner_evaluation(model, examples):\n",
    "    \"\"\"Evaluate NER with spaCy. Print the evaluation.\n",
    "\n",
    "    Parameters:\n",
    "      model (spaCy model): spaCy model for NER\n",
    "      examples (list): list of spaCy 'example' objects (each 'example' contains text and predicted/gold reference entities)\n",
    "\n",
    "    Returns: \n",
    "      None\n",
    "    \"\"\"\n",
    "    if model is None or examples is None:\n",
    "        print(\"ner_evaluation must have valid arguments\")\n",
    "        return\n",
    "  \n",
    "    metrics = score_ner(examples)\n",
    "    print_metrics(metrics, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "# Cargar el modelo de spaCy\n",
    "nlp_predict = spacy.load(\"/kaggle/working/model-best\")\n",
    "\n",
    "# Ruta al archivo de prueba en formato .spacy\n",
    "test_data_path = \"/kaggle/working/test.spacy\"\n",
    "\n",
    "# Cargar los ejemplos de prueba desde el archivo .spacy\n",
    "test_examples = list(spacy.load(test_data_path).get_pipe(\"ner\").pipe)\n",
    "\n",
    "# Realizar la evaluación del NER\n",
    "ner_evaluation(model=nlp_predict, examples=test_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPUPNSqbXT-T"
   },
   "source": [
    "## 3.3 NEL (1 punto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itchQVx3XcU6"
   },
   "source": [
    "En esta sección, la idea es obtener los enlaces en la DBpedia spotlight relacionados con las entidades que se han obtenido de NER usando spaCy. \n",
    "\n",
    "\n",
    "\n",
    "Desarrolla una función que dado un texto, obtenga automáticamente las entidades relacionadas en el DBpedia Spotlight.\n",
    "URL de acceso a la API, DBPedia inglés: https://www.dbpedia-spotlight.org/api o https://www.dbpedia-spotlight.org/api/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-03T19:24:08.996886Z",
     "iopub.status.busy": "2023-06-03T19:24:08.996305Z",
     "iopub.status.idle": "2023-06-03T19:24:09.488709Z",
     "shell.execute_reply": "2023-06-03T19:24:09.487727Z",
     "shell.execute_reply.started": "2023-06-03T19:24:08.996850Z"
    },
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1684248332555,
     "user": {
      "displayName": "Jordi Conesa Caralt",
      "userId": "06942367536565960417"
     },
     "user_tz": -120
    },
    "id": "PKZ_bvCVXjI2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: Elizabeth II has been the Queen of England.\n",
      "DBpedia links: ['http://dbpedia.org/resource/Elizabeth_II', 'http://dbpedia.org/resource/England']\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "# Función para obtener DPEDIA-LINKS\n",
    "def get_dbpedia_links(text):\n",
    "    \n",
    "    # URL\n",
    "    base_url = 'https://api.dbpedia-spotlight.org/en/annotate'\n",
    "    \n",
    "    # Parametros\n",
    "    query_params = {\n",
    "        'text': text,\n",
    "        'confidence': 0.5\n",
    "    }\n",
    "    \n",
    "    # Codificar el texto para incluirlo en la URL\n",
    "    encoded_text = urllib.parse.quote(text)\n",
    "    \n",
    "    # Codificar los parámetros de la consulta\n",
    "    encoded_params = urllib.parse.urlencode(query_params)\n",
    "    \n",
    "    # Construir la URL completa con el texto codificado y los parámetros\n",
    "    url = f\"{base_url}?text={encoded_text}&{encoded_params}\" \n",
    "    headers = {'accept': 'application/json'}\n",
    "    \n",
    "    try:\n",
    "        # Realizar la solicitud GET a la API de DBpedia Spotlight\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "         # Verificar si la solicitud fue exitosa\n",
    "        if response.status_code == 200:\n",
    "            \n",
    "            # Obtener las anotaciones (recursos) de la respuesta JSON\n",
    "            annotations = response.json().get('Resources', [])\n",
    "            \n",
    "             # Extraer los enlaces (URIs) de las anotaciones\n",
    "            links = [ann['@URI'] for ann in annotations] \n",
    "            return links\n",
    "        else:\n",
    "            \n",
    "             # Mostrar el código de estado de la respuesta en caso de error\n",
    "            print(f'Error accessing DBpedia Spotlight API: {response.status_code}')\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        \n",
    "        # Mostrar el error de solicitud en caso de excepción\n",
    "        print(f'Error accessing DBpedia Spotlight API: {e}')  \n",
    "    return []\n",
    "\n",
    "# Texo a analizar\n",
    "text = 'Elizabeth II has been the Queen of England.'\n",
    "print(\"Texto:\", text)\n",
    "\n",
    "\n",
    "try:\n",
    "    # Obtener los enlaces de DBpedia Spotlight para el texto dado\n",
    "    dbpedia_links = get_dbpedia_links(text)\n",
    "    \n",
    "    # Imprimir los enlaces obtenidos\n",
    "    print('DBpedia links:', dbpedia_links)  \n",
    "except Exception as e:\n",
    "    \n",
    "    # Mostrar el error en caso de excepción\n",
    "    print(f'Error getting DBpedia links: {e}')  \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "JuKkXmmS7nbD",
    "Re2z6jLm7nbP"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
